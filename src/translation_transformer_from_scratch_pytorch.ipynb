{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIvoBMMmuwyn/0MFLgs8k5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tonystark11/transformer-from-scratch/blob/main/src/translation_transformer_from_scratch_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "3Ey--O2jcRnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U 'spacy[cuda-autodetect]' -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgR32j24cTR9",
        "outputId": "def46b80-a383-418c-f537-49f811101d1d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for cupy-wheel (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWJoajxGHupW",
        "outputId": "cbcbcc2f-f0ad-41d5-9c1f-419839265eee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-20 06:18:08.219983: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 06:18:09.155391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-20 06:18:12.776350: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-20 06:18:12.776932: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-20 06:18:12.778508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2023-08-20 06:18:25.809549: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-20 06:18:26.813174: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-08-20 06:18:28.322484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-20 06:18:28.322975: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-08-20 06:18:28.323174: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "96u-2MGiXs-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "giaE9jhGXmY-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy\n",
        "from functools import partial\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ydNngtERXu4H",
        "outputId": "ca1d569d-1cf6-4ce2-e665-99a966cb8d7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42"
      ],
      "metadata": {
        "id": "SieEu-aCLUUn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MultiHeadAttention"
      ],
      "metadata": {
        "id": "mJawqQ9GX8rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_probs, V)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, d_model = x.size()\n",
        "        return x.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_len, d_k = x.size()\n",
        "        return x.transpose(1, 2).reshape(batch_size, seq_len, self.d_model)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        Q = self.split_heads(self.W_q(Q))\n",
        "        K = self.split_heads(self.W_k(K))\n",
        "        V = self.split_heads(self.W_v(V))\n",
        "\n",
        "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "CnGEAqsBXzkU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Position wise Feed Forward Network"
      ],
      "metadata": {
        "id": "L7yfVhkMYEhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "nhLEdRqBYAyN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "nv9loWVOYKQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        pe = torch.zeros(max_seq_length, d_model, device=device)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float, device=device).unsqueeze(1)\n",
        "        div_term = torch.pow(10_000, (-torch.arange(0, d_model, 2, device=device).float() / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        return self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "_SqsjrSaYIlt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Layer"
      ],
      "metadata": {
        "id": "ZKUrDvlYYRWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "i-Suvxv0YOqh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder Layer"
      ],
      "metadata": {
        "id": "Z8ySRXYzYYH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "HRTalpKJYU6P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "YubbLshNYekk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "x6RA6zpgYbHF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Data"
      ],
      "metadata": {
        "id": "5Fu_f8PgYnXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/spa-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi6nauuSYhQR",
        "outputId": "1002321f-5af3-43fc-af7f-f4d350b923f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-20 06:18:38--  https://www.manythings.org/anki/spa-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5394805 (5.1M) [application/zip]\n",
            "Saving to: ‘spa-eng.zip’\n",
            "\n",
            "spa-eng.zip         100%[===================>]   5.14M  27.2MB/s    in 0.2s    \n",
            "\n",
            "2023-08-20 06:18:38 (27.2 MB/s) - ‘spa-eng.zip’ saved [5394805/5394805]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip spa-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h14iqrxHZOXI",
        "outputId": "c7cb0ca8-04cd-49f1-a90f-777679df96f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  spa-eng.zip\n",
            "  inflating: _about.txt              \n",
            "  inflating: spa.txt                 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('spa.txt', 'r') as f:\n",
        "    lines = f.readlines()"
      ],
      "metadata": {
        "id": "uptfO7IsZ2TO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HECjSS1lZ-x3",
        "outputId": "8c9fb2ee-0799-4ec8-b595-cdc3a3b797a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140868"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines[10000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bsswMXSfaAy6",
        "outputId": "416cfe6c-01c4-4e06-8421-ff933f5603fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I hate tomatoes.\\tOdio los tomates.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549559 (CK) & #2817690 (marcelostockle)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove everything after the 2nd tab character.\n",
        "# As we can see above, we only need the first two columns of the data\n",
        "lines = [line.split('\\t') for line in lines]\n",
        "lines = ['\\t'.join(line[:2]) for line in lines]"
      ],
      "metadata": {
        "id": "OUCSX2QzaEJM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines[10000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mYLNfLd9aMYw",
        "outputId": "d5737dd7-089c-4f4b-8185-f79b948cd945"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I hate tomatoes.\\tOdio los tomates.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train, val, test split\n",
        "train_lines, val_test_lines = train_test_split(lines, test_size=0.2, random_state=random_seed, shuffle=True)\n",
        "val_lines, test_lines = train_test_split(val_test_lines, test_size=0.5, random_state=random_seed, shuffle=True)"
      ],
      "metadata": {
        "id": "0jrrLdoMKRs-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_lines))\n",
        "print(len(val_lines))\n",
        "print(len(test_lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXX2WcrTLlPO",
        "outputId": "0a35df38-3cb0-48fd-8563-f58b7fe42b1e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "112694\n",
            "14087\n",
            "14087\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NJwO1OCELxwM",
        "outputId": "d1ec50ee-e52a-4970-a7bb-66e1ec24bf9f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tom couldn't rule out the possibility that he was mistaken.\\tTom no pudo descartar la posibilidad de que él estuviera equivocado.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wuOmjimML1GB",
        "outputId": "9078578e-726d-4915-b87a-8a465ab2a582"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Don't you want me to keep it?\\t¿Quieres que me lo quede?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_lines[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5CIt4obBL3fK",
        "outputId": "b482b4d6-57f4-48ae-be0c-524548b50880"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Do you think we should import rice from the U.S.?\\t¿Crees que deberíamos importar arroz de los Estados Unidos?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Data"
      ],
      "metadata": {
        "id": "EA5cl9qOIukd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SRC_LANGUAGE = \"en\"\n",
        "TGT_LANGUAGE = \"es\""
      ],
      "metadata": {
        "id": "XWO6oUP9aQrE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = {}\n",
        "tokenizer[SRC_LANGUAGE] = get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
        "tokenizer[TGT_LANGUAGE] = get_tokenizer(\"spacy\", \"es_core_news_sm\")"
      ],
      "metadata": {
        "id": "K57FvxR9HjHS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Dataset"
      ],
      "metadata": {
        "id": "av5y5eatMm3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentencePairDataset(Dataset):\n",
        "    def __init__(self, lines, src_tokenizer, tgt_tokenizer):\n",
        "        super(SentencePairDataset, self).__init__()\n",
        "\n",
        "        self.lines = lines\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.lines)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        line = self.lines[idx]\n",
        "\n",
        "        src, tgt = line.split('\\t')\n",
        "        src_tokens = self.src_tokenizer(src)\n",
        "        tgt_tokens = self.tgt_tokenizer(tgt)\n",
        "\n",
        "        return src_tokens, tgt_tokens"
      ],
      "metadata": {
        "id": "9pQb9TXQIQHU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = SentencePairDataset(train_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
        "val_ds = SentencePairDataset(val_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])\n",
        "test_ds = SentencePairDataset(test_lines, tokenizer[SRC_LANGUAGE], tokenizer[TGT_LANGUAGE])"
      ],
      "metadata": {
        "id": "_VhZbqdSKIn5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of longest src sequence\n",
        "print(max(len(x[0]) for x in train_ds))\n",
        "print(max(len(x[0]) for x in val_ds))\n",
        "print(max(len(x[0]) for x in test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpBOyY4vqwfi",
        "outputId": "019210aa-3cfb-4ad4-f8fa-fe3709d73263"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "53\n",
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of longest tgt sequence\n",
        "print(max(len(x[1]) for x in train_ds))\n",
        "print(max(len(x[1]) for x in val_ds))\n",
        "print(max(len(x[1]) for x in test_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFxXfbeSrB4h",
        "outputId": "1fb399f5-266b-434b-c207-1725d5c56502"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "57\n",
            "78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_ds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsIgzRtAMNQ-",
        "outputId": "6452a00e-aaa1-4fc4-f5b3-d545c309fadb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['Tom',\n",
              "  'could',\n",
              "  \"n't\",\n",
              "  'rule',\n",
              "  'out',\n",
              "  'the',\n",
              "  'possibility',\n",
              "  'that',\n",
              "  'he',\n",
              "  'was',\n",
              "  'mistaken',\n",
              "  '.'],\n",
              " ['Tom',\n",
              "  'no',\n",
              "  'pudo',\n",
              "  'descartar',\n",
              "  'la',\n",
              "  'posibilidad',\n",
              "  'de',\n",
              "  'que',\n",
              "  'él',\n",
              "  'estuviera',\n",
              "  'equivocado',\n",
              "  '.'])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Vocabulary"
      ],
      "metadata": {
        "id": "Yqo3q9RkMkJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {}"
      ],
      "metadata": {
        "id": "rvyraKQ4r9gG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "max_seq_len = 100\n",
        "\n",
        "PAD_IDX = 0\n",
        "UNK_IDX = 1\n",
        "BOS_IDX = 2\n",
        "EOS_IDX = 3\n",
        "\n",
        "special_symbols = ['<PAD>', '<UNK>', '<BOS>', '<EOS>']"
      ],
      "metadata": {
        "id": "f4FMeg4yMP8x"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(dataset, lang_idx=0):\n",
        "    n = len(dataset)\n",
        "    i = 0\n",
        "\n",
        "    while i < n:\n",
        "        yield dataset[i][lang_idx]\n",
        "        i += 1"
      ],
      "metadata": {
        "id": "ujR7SLRvM1h7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_iterator = yield_tokens(train_ds, lang_idx=0)\n",
        "tgt_iterator = yield_tokens(train_ds, lang_idx=1)"
      ],
      "metadata": {
        "id": "knnnCTpMNGMA"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[SRC_LANGUAGE] = build_vocab_from_iterator(\n",
        "    src_iterator,\n",
        "    min_freq=1,\n",
        "    specials=special_symbols,\n",
        "    special_first=True,\n",
        "    max_tokens=src_vocab_size,\n",
        ")"
      ],
      "metadata": {
        "id": "igmo37DaNTox"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[TGT_LANGUAGE] = build_vocab_from_iterator(\n",
        "    tgt_iterator,\n",
        "    min_freq=1,\n",
        "    specials=special_symbols,\n",
        "    special_first=True,\n",
        "    max_tokens=tgt_vocab_size,\n",
        ")"
      ],
      "metadata": {
        "id": "YZ0aiHA-OAEf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[SRC_LANGUAGE].set_default_index(UNK_IDX)\n",
        "vocab[TGT_LANGUAGE].set_default_index(UNK_IDX)"
      ],
      "metadata": {
        "id": "J5I7rHfFOMTP"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[SRC_LANGUAGE]['hello']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6dKNNCzOfoy",
        "outputId": "ea75d92f-e2c4-477c-ed1b-47e7c2bcef1f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2248"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[TGT_LANGUAGE]['Hola']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJCNg2MfORYQ",
        "outputId": "a3506664-f70b-4a62-da85-b4c2e06b240f"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2142"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, vocab):\n",
        "    batch_size = len(batch)\n",
        "    srcs, tgts = zip(*batch)\n",
        "    src_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "    tgt_vectors = torch.zeros((batch_size, max_seq_len), dtype=torch.long, device=device)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        src_vectors[i] = torch.tensor(([BOS_IDX] + vocab[SRC_LANGUAGE](srcs[i]) + [EOS_IDX] + [0] * (max_seq_len - len(srcs[i])))[:max_seq_len], dtype=torch.long, device=device)\n",
        "        tgt_vectors[i] = torch.tensor(([BOS_IDX] + vocab[TGT_LANGUAGE](tgts[i]) + [EOS_IDX] + [0] * (max_seq_len - len(tgts[i])))[:max_seq_len], dtype=torch.long, device=device)\n",
        "\n",
        "    return src_vectors, tgt_vectors"
      ],
      "metadata": {
        "id": "hfK2UQZFmxQD"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
        "val_dataloader = DataLoader(val_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))\n",
        "test_dataloader = DataLoader(test_ds, batch_size=64, shuffle=True, collate_fn=partial(collate_fn, vocab=vocab))"
      ],
      "metadata": {
        "id": "Uqxlzw-Hu_YJ"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "d_model = 512\n",
        "num_heads = 4\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "num_epochs = 3\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "transformer.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------------------\")\n",
        "    transformer.train()\n",
        "    for data in train_dataloader:\n",
        "        src_data, tgt_data = data\n",
        "        optimizer.zero_grad()\n",
        "        output = transformer(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        print(f\"Epoch: {epoch+1}, Training Loss: {loss.item()}\")\n",
        "\n",
        "    transformer.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in val_dataloader:\n",
        "            src_data, tgt_data = data\n",
        "            output = transformer(src_data, tgt_data[:, :-1])\n",
        "            loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "            print(f\"Epoch: {epoch+1}, Validation Loss: {loss.item()}\")\n",
        "\n",
        "    torch.save(transformer.state_dict(), f'./transformer_state_dict_epoch_{epoch+1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxUsDY5JzNj7",
        "outputId": "8193140e-cbc0-4d38-ea8d-e3f9986e5726"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 1, Training Loss: 2.7225699424743652\n",
            "Epoch: 1, Training Loss: 2.866717576980591\n",
            "Epoch: 1, Training Loss: 2.7739784717559814\n",
            "Epoch: 1, Training Loss: 3.022616147994995\n",
            "Epoch: 1, Training Loss: 2.658881187438965\n",
            "Epoch: 1, Training Loss: 2.632040023803711\n",
            "Epoch: 1, Training Loss: 2.7406771183013916\n",
            "Epoch: 1, Training Loss: 2.696399211883545\n",
            "Epoch: 1, Training Loss: 2.5802083015441895\n",
            "Epoch: 1, Training Loss: 2.7932116985321045\n",
            "Epoch: 1, Training Loss: 2.635684013366699\n",
            "Epoch: 1, Training Loss: 2.5481836795806885\n",
            "Epoch: 1, Training Loss: 2.6114938259124756\n",
            "Epoch: 1, Training Loss: 2.736689805984497\n",
            "Epoch: 1, Training Loss: 2.8214921951293945\n",
            "Epoch: 1, Training Loss: 2.7694509029388428\n",
            "Epoch: 1, Training Loss: 2.569808006286621\n",
            "Epoch: 1, Training Loss: 2.682981252670288\n",
            "Epoch: 1, Training Loss: 2.7397444248199463\n",
            "Epoch: 1, Training Loss: 2.86733078956604\n",
            "Epoch: 1, Training Loss: 2.6897354125976562\n",
            "Epoch: 1, Training Loss: 2.65122652053833\n",
            "Epoch: 1, Training Loss: 2.7977423667907715\n",
            "Epoch: 1, Training Loss: 2.7893354892730713\n",
            "Epoch: 1, Training Loss: 2.5583724975585938\n",
            "Epoch: 1, Training Loss: 2.8401856422424316\n",
            "Epoch: 1, Training Loss: 2.6307809352874756\n",
            "Epoch: 1, Training Loss: 2.6069960594177246\n",
            "Epoch: 1, Training Loss: 2.576953887939453\n",
            "Epoch: 1, Training Loss: 3.0721113681793213\n",
            "Epoch: 1, Training Loss: 2.6977310180664062\n",
            "Epoch: 1, Training Loss: 2.6316301822662354\n",
            "Epoch: 1, Training Loss: 2.65282940864563\n",
            "Epoch: 1, Training Loss: 2.6612672805786133\n",
            "Epoch: 1, Training Loss: 2.5377442836761475\n",
            "Epoch: 1, Training Loss: 2.3829424381256104\n",
            "Epoch: 1, Training Loss: 2.6282665729522705\n",
            "Epoch: 1, Training Loss: 2.8046932220458984\n",
            "Epoch: 1, Training Loss: 2.897775888442993\n",
            "Epoch: 1, Training Loss: 2.7450613975524902\n",
            "Epoch: 1, Training Loss: 2.788145065307617\n",
            "Epoch: 1, Training Loss: 2.8582682609558105\n",
            "Epoch: 1, Training Loss: 2.5992417335510254\n",
            "Epoch: 1, Training Loss: 2.748502254486084\n",
            "Epoch: 1, Training Loss: 2.6089630126953125\n",
            "Epoch: 1, Training Loss: 2.5870907306671143\n",
            "Epoch: 1, Training Loss: 2.518566370010376\n",
            "Epoch: 1, Training Loss: 2.568405866622925\n",
            "Epoch: 1, Training Loss: 2.640759229660034\n",
            "Epoch: 1, Training Loss: 2.5434842109680176\n",
            "Epoch: 1, Training Loss: 2.8427751064300537\n",
            "Epoch: 1, Training Loss: 2.6945583820343018\n",
            "Epoch: 1, Training Loss: 2.5252952575683594\n",
            "Epoch: 1, Training Loss: 2.7337241172790527\n",
            "Epoch: 1, Training Loss: 2.6258349418640137\n",
            "Epoch: 1, Training Loss: 2.733612060546875\n",
            "Epoch: 1, Training Loss: 2.5828545093536377\n",
            "Epoch: 1, Training Loss: 2.580364465713501\n",
            "Epoch: 1, Training Loss: 2.613492965698242\n",
            "Epoch: 1, Training Loss: 2.6151931285858154\n",
            "Epoch: 1, Training Loss: 2.7424890995025635\n",
            "Epoch: 1, Training Loss: 2.713207960128784\n",
            "Epoch: 1, Training Loss: 2.751967430114746\n",
            "Epoch: 1, Training Loss: 2.7427477836608887\n",
            "Epoch: 1, Training Loss: 2.7176997661590576\n",
            "Epoch: 1, Training Loss: 2.840611696243286\n",
            "Epoch: 1, Training Loss: 2.67940092086792\n",
            "Epoch: 1, Training Loss: 2.6925230026245117\n",
            "Epoch: 1, Training Loss: 2.6776857376098633\n",
            "Epoch: 1, Training Loss: 2.641371011734009\n",
            "Epoch: 1, Training Loss: 2.6162233352661133\n",
            "Epoch: 1, Training Loss: 2.4764034748077393\n",
            "Epoch: 1, Training Loss: 2.701032876968384\n",
            "Epoch: 1, Training Loss: 2.696509599685669\n",
            "Epoch: 1, Training Loss: 2.5719916820526123\n",
            "Epoch: 1, Training Loss: 2.4297266006469727\n",
            "Epoch: 1, Training Loss: 2.8548314571380615\n",
            "Epoch: 1, Training Loss: 2.8114895820617676\n",
            "Epoch: 1, Training Loss: 2.725679397583008\n",
            "Epoch: 1, Training Loss: 2.6575706005096436\n",
            "Epoch: 1, Training Loss: 2.8154642581939697\n",
            "Epoch: 1, Training Loss: 3.005295753479004\n",
            "Epoch: 1, Training Loss: 2.4485294818878174\n",
            "Epoch: 1, Training Loss: 2.4468863010406494\n",
            "Epoch: 1, Training Loss: 2.8119168281555176\n",
            "Epoch: 1, Training Loss: 2.625521659851074\n",
            "Epoch: 1, Training Loss: 2.51678729057312\n",
            "Epoch: 1, Training Loss: 2.636369466781616\n",
            "Epoch: 1, Training Loss: 3.0065484046936035\n",
            "Epoch: 1, Training Loss: 2.7008423805236816\n",
            "Epoch: 1, Training Loss: 2.8701040744781494\n",
            "Epoch: 1, Training Loss: 2.7226805686950684\n",
            "Epoch: 1, Training Loss: 2.6271560192108154\n",
            "Epoch: 1, Training Loss: 2.3967502117156982\n",
            "Epoch: 1, Training Loss: 2.6337716579437256\n",
            "Epoch: 1, Training Loss: 2.547376871109009\n",
            "Epoch: 1, Training Loss: 3.0982141494750977\n",
            "Epoch: 1, Training Loss: 2.9541423320770264\n",
            "Epoch: 1, Training Loss: 2.750394105911255\n",
            "Epoch: 1, Training Loss: 2.733541965484619\n",
            "Epoch: 1, Training Loss: 2.398472547531128\n",
            "Epoch: 1, Training Loss: 2.7861785888671875\n",
            "Epoch: 1, Training Loss: 2.683427572250366\n",
            "Epoch: 1, Training Loss: 2.745898723602295\n",
            "Epoch: 1, Training Loss: 2.507608652114868\n",
            "Epoch: 1, Training Loss: 2.4130148887634277\n",
            "Epoch: 1, Training Loss: 2.388540744781494\n",
            "Epoch: 1, Training Loss: 2.4365925788879395\n",
            "Epoch: 1, Training Loss: 2.6958327293395996\n",
            "Epoch: 1, Training Loss: 2.7237820625305176\n",
            "Epoch: 1, Training Loss: 2.664046049118042\n",
            "Epoch: 1, Training Loss: 2.4042763710021973\n",
            "Epoch: 1, Training Loss: 2.6575164794921875\n",
            "Epoch: 1, Training Loss: 2.641300678253174\n",
            "Epoch: 1, Training Loss: 2.828805446624756\n",
            "Epoch: 1, Training Loss: 2.610391616821289\n",
            "Epoch: 1, Training Loss: 2.729813814163208\n",
            "Epoch: 1, Training Loss: 2.5862622261047363\n",
            "Epoch: 1, Training Loss: 2.481849431991577\n",
            "Epoch: 1, Training Loss: 2.7279248237609863\n",
            "Epoch: 1, Training Loss: 2.951463222503662\n",
            "Epoch: 1, Training Loss: 2.549226760864258\n",
            "Epoch: 1, Training Loss: 2.6029672622680664\n",
            "Epoch: 1, Training Loss: 2.9447784423828125\n",
            "Epoch: 1, Training Loss: 2.468404769897461\n",
            "Epoch: 1, Training Loss: 2.8015809059143066\n",
            "Epoch: 1, Training Loss: 2.9476377964019775\n",
            "Epoch: 1, Training Loss: 2.765058755874634\n",
            "Epoch: 1, Training Loss: 2.6879820823669434\n",
            "Epoch: 1, Training Loss: 2.600350856781006\n",
            "Epoch: 1, Training Loss: 2.747708320617676\n",
            "Epoch: 1, Training Loss: 2.4182801246643066\n",
            "Epoch: 1, Training Loss: 2.9230027198791504\n",
            "Epoch: 1, Training Loss: 2.7908551692962646\n",
            "Epoch: 1, Training Loss: 2.6816959381103516\n",
            "Epoch: 1, Training Loss: 2.864927053451538\n",
            "Epoch: 1, Training Loss: 2.284885883331299\n",
            "Epoch: 1, Training Loss: 2.635014772415161\n",
            "Epoch: 1, Training Loss: 2.5792229175567627\n",
            "Epoch: 1, Training Loss: 2.5033717155456543\n",
            "Epoch: 1, Training Loss: 2.566511631011963\n",
            "Epoch: 1, Training Loss: 2.4335825443267822\n",
            "Epoch: 1, Training Loss: 2.5121500492095947\n",
            "Epoch: 1, Training Loss: 2.6685240268707275\n",
            "Epoch: 1, Training Loss: 2.81777286529541\n",
            "Epoch: 1, Training Loss: 2.694228410720825\n",
            "Epoch: 1, Training Loss: 2.6113064289093018\n",
            "Epoch: 1, Training Loss: 2.8382112979888916\n",
            "Epoch: 1, Training Loss: 2.472912549972534\n",
            "Epoch: 1, Training Loss: 2.777570962905884\n",
            "Epoch: 1, Training Loss: 2.9129059314727783\n",
            "Epoch: 1, Training Loss: 2.7153093814849854\n",
            "Epoch: 1, Training Loss: 2.617060899734497\n",
            "Epoch: 1, Training Loss: 2.6930625438690186\n",
            "Epoch: 1, Training Loss: 2.720856189727783\n",
            "Epoch: 1, Training Loss: 2.3516764640808105\n",
            "Epoch: 1, Training Loss: 2.708592653274536\n",
            "Epoch: 1, Training Loss: 2.684680461883545\n",
            "Epoch: 1, Training Loss: 2.469459295272827\n",
            "Epoch: 1, Training Loss: 2.4526162147521973\n",
            "Epoch: 1, Training Loss: 2.7464046478271484\n",
            "Epoch: 1, Training Loss: 2.475830316543579\n",
            "Epoch: 1, Training Loss: 2.7393860816955566\n",
            "Epoch: 1, Training Loss: 2.7931482791900635\n",
            "Epoch: 1, Training Loss: 2.775308609008789\n",
            "Epoch: 1, Training Loss: 2.5040557384490967\n",
            "Epoch: 1, Training Loss: 2.8646492958068848\n",
            "Epoch: 1, Training Loss: 2.6889374256134033\n",
            "Epoch: 1, Training Loss: 2.5416579246520996\n",
            "Epoch: 1, Training Loss: 2.851836681365967\n",
            "Epoch: 1, Training Loss: 2.836062431335449\n",
            "Epoch: 1, Training Loss: 2.403128147125244\n",
            "Epoch: 1, Training Loss: 2.6453230381011963\n",
            "Epoch: 1, Training Loss: 2.598247528076172\n",
            "Epoch: 1, Training Loss: 2.6646063327789307\n",
            "Epoch: 1, Training Loss: 2.5744316577911377\n",
            "Epoch: 1, Training Loss: 2.514820098876953\n",
            "Epoch: 1, Training Loss: 2.6443636417388916\n",
            "Epoch: 1, Training Loss: 2.356069326400757\n",
            "Epoch: 1, Training Loss: 2.752751350402832\n",
            "Epoch: 1, Training Loss: 2.756026268005371\n",
            "Epoch: 1, Training Loss: 2.715998649597168\n",
            "Epoch: 1, Training Loss: 2.738024950027466\n",
            "Epoch: 1, Training Loss: 2.823519706726074\n",
            "Epoch: 1, Training Loss: 2.708951234817505\n",
            "Epoch: 1, Training Loss: 2.529585123062134\n",
            "Epoch: 1, Training Loss: 2.7482354640960693\n",
            "Epoch: 1, Training Loss: 2.609687089920044\n",
            "Epoch: 1, Training Loss: 2.8002004623413086\n",
            "Epoch: 1, Training Loss: 2.6144654750823975\n",
            "Epoch: 1, Training Loss: 2.5572152137756348\n",
            "Epoch: 1, Training Loss: 3.0129830837249756\n",
            "Epoch: 1, Training Loss: 2.715118646621704\n",
            "Epoch: 1, Training Loss: 2.57432222366333\n",
            "Epoch: 1, Training Loss: 2.677077054977417\n",
            "Epoch: 1, Training Loss: 2.5109081268310547\n",
            "Epoch: 1, Training Loss: 2.5743706226348877\n",
            "Epoch: 1, Training Loss: 2.7059812545776367\n",
            "Epoch: 1, Training Loss: 2.4781839847564697\n",
            "Epoch: 1, Training Loss: 2.67545747756958\n",
            "Epoch: 1, Training Loss: 2.5801844596862793\n",
            "Epoch: 1, Training Loss: 2.376589298248291\n",
            "Epoch: 1, Training Loss: 2.9563376903533936\n",
            "Epoch: 1, Training Loss: 2.412506103515625\n",
            "Epoch: 1, Training Loss: 2.4541454315185547\n",
            "Epoch: 1, Training Loss: 2.796989917755127\n",
            "Epoch: 1, Training Loss: 2.885814666748047\n",
            "Epoch: 1, Training Loss: 2.7224254608154297\n",
            "Epoch: 1, Training Loss: 2.582044839859009\n",
            "Epoch: 1, Training Loss: 2.6323318481445312\n",
            "Epoch: 1, Training Loss: 2.553692102432251\n",
            "Epoch: 1, Training Loss: 2.6778738498687744\n",
            "Epoch: 1, Training Loss: 2.4939780235290527\n",
            "Epoch: 1, Training Loss: 2.42903470993042\n",
            "Epoch: 1, Training Loss: 2.295013666152954\n",
            "Epoch: 1, Training Loss: 2.6624183654785156\n",
            "Epoch: 1, Training Loss: 2.6333694458007812\n",
            "Epoch: 1, Training Loss: 2.7883026599884033\n",
            "Epoch: 1, Training Loss: 2.4922289848327637\n",
            "Epoch: 1, Training Loss: 2.696085214614868\n",
            "Epoch: 1, Training Loss: 2.9492554664611816\n",
            "Epoch: 1, Training Loss: 2.4158265590667725\n",
            "Epoch: 1, Training Loss: 2.8206002712249756\n",
            "Epoch: 1, Training Loss: 2.425349712371826\n",
            "Epoch: 1, Training Loss: 2.5491459369659424\n",
            "Epoch: 1, Training Loss: 2.39620041847229\n",
            "Epoch: 1, Training Loss: 2.59403657913208\n",
            "Epoch: 1, Training Loss: 2.6866416931152344\n",
            "Epoch: 1, Training Loss: 2.4811558723449707\n",
            "Epoch: 1, Training Loss: 2.6674370765686035\n",
            "Epoch: 1, Training Loss: 2.6515774726867676\n",
            "Epoch: 1, Training Loss: 2.7454259395599365\n",
            "Epoch: 1, Training Loss: 2.5672495365142822\n",
            "Epoch: 1, Training Loss: 2.7453513145446777\n",
            "Epoch: 1, Training Loss: 2.5632119178771973\n",
            "Epoch: 1, Training Loss: 2.4827094078063965\n",
            "Epoch: 1, Training Loss: 2.503403902053833\n",
            "Epoch: 1, Training Loss: 2.6551973819732666\n",
            "Epoch: 1, Training Loss: 2.386746644973755\n",
            "Epoch: 1, Training Loss: 2.597534418106079\n",
            "Epoch: 1, Training Loss: 2.855133056640625\n",
            "Epoch: 1, Training Loss: 2.620323896408081\n",
            "Epoch: 1, Training Loss: 2.3440377712249756\n",
            "Epoch: 1, Training Loss: 2.603858709335327\n",
            "Epoch: 1, Training Loss: 2.717763900756836\n",
            "Epoch: 1, Training Loss: 2.6733577251434326\n",
            "Epoch: 1, Training Loss: 2.2504005432128906\n",
            "Epoch: 1, Training Loss: 2.4915771484375\n",
            "Epoch: 1, Training Loss: 2.450977325439453\n",
            "Epoch: 1, Training Loss: 2.7129220962524414\n",
            "Epoch: 1, Training Loss: 2.359006643295288\n",
            "Epoch: 1, Training Loss: 2.4907610416412354\n",
            "Epoch: 1, Training Loss: 2.468489170074463\n",
            "Epoch: 1, Training Loss: 2.6905505657196045\n",
            "Epoch: 1, Training Loss: 2.319209575653076\n",
            "Epoch: 1, Training Loss: 2.6012163162231445\n",
            "Epoch: 1, Training Loss: 2.315168857574463\n",
            "Epoch: 1, Training Loss: 2.621253728866577\n",
            "Epoch: 1, Training Loss: 2.680001735687256\n",
            "Epoch: 1, Training Loss: 2.563535213470459\n",
            "Epoch: 1, Training Loss: 2.5420243740081787\n",
            "Epoch: 1, Training Loss: 2.7534031867980957\n",
            "Epoch: 1, Training Loss: 2.351309299468994\n",
            "Epoch: 1, Training Loss: 2.7361316680908203\n",
            "Epoch: 1, Training Loss: 2.7108476161956787\n",
            "Epoch: 1, Training Loss: 2.637866735458374\n",
            "Epoch: 1, Training Loss: 2.609806537628174\n",
            "Epoch: 1, Training Loss: 2.4395313262939453\n",
            "Epoch: 1, Training Loss: 2.850404739379883\n",
            "Epoch: 1, Training Loss: 2.5841517448425293\n",
            "Epoch: 1, Training Loss: 2.6832706928253174\n",
            "Epoch: 1, Training Loss: 2.6348788738250732\n",
            "Epoch: 1, Training Loss: 2.4566092491149902\n",
            "Epoch: 1, Training Loss: 2.7157623767852783\n",
            "Epoch: 1, Training Loss: 2.705050230026245\n",
            "Epoch: 1, Training Loss: 2.366673469543457\n",
            "Epoch: 1, Training Loss: 2.4348764419555664\n",
            "Epoch: 1, Training Loss: 2.808803081512451\n",
            "Epoch: 1, Training Loss: 2.570345401763916\n",
            "Epoch: 1, Training Loss: 2.5852997303009033\n",
            "Epoch: 1, Training Loss: 2.5438263416290283\n",
            "Epoch: 1, Training Loss: 2.599781036376953\n",
            "Epoch: 1, Training Loss: 2.5321340560913086\n",
            "Epoch: 1, Training Loss: 2.4117023944854736\n",
            "Epoch: 1, Training Loss: 2.5947623252868652\n",
            "Epoch: 1, Training Loss: 2.63653564453125\n",
            "Epoch: 1, Training Loss: 2.238069534301758\n",
            "Epoch: 1, Training Loss: 2.3064093589782715\n",
            "Epoch: 1, Training Loss: 2.503412961959839\n",
            "Epoch: 1, Training Loss: 2.791288375854492\n",
            "Epoch: 1, Training Loss: 2.5959017276763916\n",
            "Epoch: 1, Training Loss: 2.325315475463867\n",
            "Epoch: 1, Training Loss: 2.5260515213012695\n",
            "Epoch: 1, Training Loss: 2.6443912982940674\n",
            "Epoch: 1, Training Loss: 2.8741421699523926\n",
            "Epoch: 1, Training Loss: 2.5354714393615723\n",
            "Epoch: 1, Training Loss: 2.372105360031128\n",
            "Epoch: 1, Training Loss: 2.4622292518615723\n",
            "Epoch: 1, Training Loss: 2.6362955570220947\n",
            "Epoch: 1, Training Loss: 2.5911307334899902\n",
            "Epoch: 1, Training Loss: 2.7871897220611572\n",
            "Epoch: 1, Training Loss: 2.304948329925537\n",
            "Epoch: 1, Training Loss: 2.5241847038269043\n",
            "Epoch: 1, Training Loss: 2.708888530731201\n",
            "Epoch: 1, Training Loss: 2.574392557144165\n",
            "Epoch: 1, Training Loss: 2.6020936965942383\n",
            "Epoch: 1, Training Loss: 2.497445583343506\n",
            "Epoch: 1, Training Loss: 2.7270123958587646\n",
            "Epoch: 1, Training Loss: 2.3975589275360107\n",
            "Epoch: 1, Training Loss: 2.7292139530181885\n",
            "Epoch: 1, Training Loss: 2.4635839462280273\n",
            "Epoch: 1, Training Loss: 2.617539644241333\n",
            "Epoch: 1, Training Loss: 2.6599433422088623\n",
            "Epoch: 1, Training Loss: 2.565152406692505\n",
            "Epoch: 1, Training Loss: 2.41279673576355\n",
            "Epoch: 1, Training Loss: 2.416593313217163\n",
            "Epoch: 1, Training Loss: 2.524139404296875\n",
            "Epoch: 1, Training Loss: 2.7125799655914307\n",
            "Epoch: 1, Training Loss: 2.253235340118408\n",
            "Epoch: 1, Training Loss: 2.369034767150879\n",
            "Epoch: 1, Training Loss: 2.686885118484497\n",
            "Epoch: 1, Training Loss: 2.6422057151794434\n",
            "Epoch: 1, Training Loss: 2.424569606781006\n",
            "Epoch: 1, Training Loss: 2.282572031021118\n",
            "Epoch: 1, Training Loss: 2.189995050430298\n",
            "Epoch: 1, Training Loss: 2.5448596477508545\n",
            "Epoch: 1, Training Loss: 2.6057825088500977\n",
            "Epoch: 1, Training Loss: 2.559521198272705\n",
            "Epoch: 1, Training Loss: 2.2340829372406006\n",
            "Epoch: 1, Training Loss: 2.5576248168945312\n",
            "Epoch: 1, Training Loss: 2.3496599197387695\n",
            "Epoch: 1, Training Loss: 2.4084949493408203\n",
            "Epoch: 1, Training Loss: 2.5167157649993896\n",
            "Epoch: 1, Training Loss: 2.676664352416992\n",
            "Epoch: 1, Training Loss: 2.4410364627838135\n",
            "Epoch: 1, Training Loss: 2.335864782333374\n",
            "Epoch: 1, Training Loss: 2.350945472717285\n",
            "Epoch: 1, Training Loss: 2.537123203277588\n",
            "Epoch: 1, Training Loss: 2.728323459625244\n",
            "Epoch: 1, Training Loss: 2.520308494567871\n",
            "Epoch: 1, Training Loss: 2.492201566696167\n",
            "Epoch: 1, Training Loss: 2.315746784210205\n",
            "Epoch: 1, Training Loss: 2.3523852825164795\n",
            "Epoch: 1, Training Loss: 2.509476661682129\n",
            "Epoch: 1, Training Loss: 2.4341635704040527\n",
            "Epoch: 1, Training Loss: 2.2851390838623047\n",
            "Epoch: 1, Training Loss: 2.57136607170105\n",
            "Epoch: 1, Training Loss: 2.5327060222625732\n",
            "Epoch: 1, Training Loss: 2.4087226390838623\n",
            "Epoch: 1, Training Loss: 2.6402885913848877\n",
            "Epoch: 1, Training Loss: 2.237616539001465\n",
            "Epoch: 1, Training Loss: 2.3284175395965576\n",
            "Epoch: 1, Training Loss: 2.550379514694214\n",
            "Epoch: 1, Training Loss: 2.336371898651123\n",
            "Epoch: 1, Training Loss: 2.571269989013672\n",
            "Epoch: 1, Training Loss: 2.994518756866455\n",
            "Epoch: 1, Training Loss: 2.485687494277954\n",
            "Epoch: 1, Training Loss: 2.5165719985961914\n",
            "Epoch: 1, Training Loss: 2.5318143367767334\n",
            "Epoch: 1, Training Loss: 2.743074893951416\n",
            "Epoch: 1, Training Loss: 2.4328818321228027\n",
            "Epoch: 1, Training Loss: 2.351318120956421\n",
            "Epoch: 1, Training Loss: 2.270571231842041\n",
            "Epoch: 1, Training Loss: 2.3280351161956787\n",
            "Epoch: 1, Training Loss: 2.6640939712524414\n",
            "Epoch: 1, Training Loss: 2.4201598167419434\n",
            "Epoch: 1, Training Loss: 2.5307457447052\n",
            "Epoch: 1, Training Loss: 2.306550979614258\n",
            "Epoch: 1, Training Loss: 2.5056183338165283\n",
            "Epoch: 1, Training Loss: 2.3748414516448975\n",
            "Epoch: 1, Training Loss: 2.2946066856384277\n",
            "Epoch: 1, Training Loss: 2.4841291904449463\n",
            "Epoch: 1, Training Loss: 2.5106568336486816\n",
            "Epoch: 1, Training Loss: 2.4594926834106445\n",
            "Epoch: 1, Training Loss: 2.4241654872894287\n",
            "Epoch: 1, Training Loss: 2.7507166862487793\n",
            "Epoch: 1, Training Loss: 2.4892303943634033\n",
            "Epoch: 1, Training Loss: 2.3983142375946045\n",
            "Epoch: 1, Training Loss: 2.5912344455718994\n",
            "Epoch: 1, Training Loss: 2.5193145275115967\n",
            "Epoch: 1, Training Loss: 2.406334161758423\n",
            "Epoch: 1, Training Loss: 2.355642318725586\n",
            "Epoch: 1, Training Loss: 2.8369362354278564\n",
            "Epoch: 1, Training Loss: 2.3696563243865967\n",
            "Epoch: 1, Training Loss: 2.4577629566192627\n",
            "Epoch: 1, Training Loss: 2.562154769897461\n",
            "Epoch: 1, Training Loss: 2.3135390281677246\n",
            "Epoch: 1, Training Loss: 2.336888551712036\n",
            "Epoch: 1, Training Loss: 2.711754083633423\n",
            "Epoch: 1, Training Loss: 2.12141489982605\n",
            "Epoch: 1, Training Loss: 2.3910093307495117\n",
            "Epoch: 1, Training Loss: 2.639852285385132\n",
            "Epoch: 1, Training Loss: 2.60483717918396\n",
            "Epoch: 1, Training Loss: 2.319974184036255\n",
            "Epoch: 1, Training Loss: 2.896833658218384\n",
            "Epoch: 1, Training Loss: 2.482935667037964\n",
            "Epoch: 1, Training Loss: 2.326768636703491\n",
            "Epoch: 1, Training Loss: 2.445269823074341\n",
            "Epoch: 1, Training Loss: 2.365023612976074\n",
            "Epoch: 1, Training Loss: 2.6381399631500244\n",
            "Epoch: 1, Training Loss: 2.5936203002929688\n",
            "Epoch: 1, Training Loss: 2.632744073867798\n",
            "Epoch: 1, Training Loss: 2.608713388442993\n",
            "Epoch: 1, Training Loss: 2.27701735496521\n",
            "Epoch: 1, Training Loss: 2.549633741378784\n",
            "Epoch: 1, Training Loss: 2.349559783935547\n",
            "Epoch: 1, Training Loss: 2.5717146396636963\n",
            "Epoch: 1, Training Loss: 2.3565804958343506\n",
            "Epoch: 1, Training Loss: 2.583181619644165\n",
            "Epoch: 1, Training Loss: 2.7295024394989014\n",
            "Epoch: 1, Training Loss: 2.4686450958251953\n",
            "Epoch: 1, Training Loss: 2.4168918132781982\n",
            "Epoch: 1, Training Loss: 2.3244235515594482\n",
            "Epoch: 1, Training Loss: 2.586204767227173\n",
            "Epoch: 1, Training Loss: 2.3924169540405273\n",
            "Epoch: 1, Training Loss: 2.527327060699463\n",
            "Epoch: 1, Training Loss: 2.5282399654388428\n",
            "Epoch: 1, Training Loss: 2.352383613586426\n",
            "Epoch: 1, Training Loss: 2.5274248123168945\n",
            "Epoch: 1, Training Loss: 2.7076327800750732\n",
            "Epoch: 1, Training Loss: 2.6783840656280518\n",
            "Epoch: 1, Training Loss: 2.6260697841644287\n",
            "Epoch: 1, Training Loss: 2.7891407012939453\n",
            "Epoch: 1, Training Loss: 2.4416935443878174\n",
            "Epoch: 1, Training Loss: 2.534492254257202\n",
            "Epoch: 1, Training Loss: 2.520765542984009\n",
            "Epoch: 1, Training Loss: 2.3703126907348633\n",
            "Epoch: 1, Training Loss: 2.4869840145111084\n",
            "Epoch: 1, Training Loss: 2.2943167686462402\n",
            "Epoch: 1, Training Loss: 2.509600877761841\n",
            "Epoch: 1, Training Loss: 2.516814947128296\n",
            "Epoch: 1, Training Loss: 2.375126838684082\n",
            "Epoch: 1, Training Loss: 2.489269971847534\n",
            "Epoch: 1, Training Loss: 2.6239888668060303\n",
            "Epoch: 1, Training Loss: 2.6405045986175537\n",
            "Epoch: 1, Training Loss: 2.460925579071045\n",
            "Epoch: 1, Training Loss: 2.434922456741333\n",
            "Epoch: 1, Training Loss: 2.4143412113189697\n",
            "Epoch: 1, Training Loss: 2.4315178394317627\n",
            "Epoch: 1, Training Loss: 2.4265716075897217\n",
            "Epoch: 1, Training Loss: 2.525458335876465\n",
            "Epoch: 1, Training Loss: 2.3986966609954834\n",
            "Epoch: 1, Training Loss: 2.2891814708709717\n",
            "Epoch: 1, Training Loss: 2.1897144317626953\n",
            "Epoch: 1, Training Loss: 2.533729314804077\n",
            "Epoch: 1, Training Loss: 2.4178473949432373\n",
            "Epoch: 1, Training Loss: 2.506010055541992\n",
            "Epoch: 1, Training Loss: 2.555684804916382\n",
            "Epoch: 1, Training Loss: 2.433232069015503\n",
            "Epoch: 1, Training Loss: 2.6355769634246826\n",
            "Epoch: 1, Training Loss: 2.3432607650756836\n",
            "Epoch: 1, Training Loss: 2.2898166179656982\n",
            "Epoch: 1, Training Loss: 2.2612597942352295\n",
            "Epoch: 1, Training Loss: 2.305009126663208\n",
            "Epoch: 1, Training Loss: 2.347234010696411\n",
            "Epoch: 1, Training Loss: 2.2119991779327393\n",
            "Epoch: 1, Training Loss: 2.3968887329101562\n",
            "Epoch: 1, Training Loss: 2.617466688156128\n",
            "Epoch: 1, Training Loss: 2.436696767807007\n",
            "Epoch: 1, Training Loss: 2.55265474319458\n",
            "Epoch: 1, Training Loss: 2.4608206748962402\n",
            "Epoch: 1, Training Loss: 2.097785472869873\n",
            "Epoch: 1, Training Loss: 2.605900526046753\n",
            "Epoch: 1, Training Loss: 2.642397403717041\n",
            "Epoch: 1, Training Loss: 2.470597743988037\n",
            "Epoch: 1, Training Loss: 2.3955161571502686\n",
            "Epoch: 1, Training Loss: 2.4625794887542725\n",
            "Epoch: 1, Training Loss: 2.3322818279266357\n",
            "Epoch: 1, Training Loss: 2.4191722869873047\n",
            "Epoch: 1, Training Loss: 2.6049277782440186\n",
            "Epoch: 1, Training Loss: 2.411201000213623\n",
            "Epoch: 1, Training Loss: 2.329674482345581\n",
            "Epoch: 1, Training Loss: 2.2129437923431396\n",
            "Epoch: 1, Training Loss: 2.4384944438934326\n",
            "Epoch: 1, Training Loss: 2.253136157989502\n",
            "Epoch: 1, Training Loss: 2.1533591747283936\n",
            "Epoch: 1, Training Loss: 2.264988660812378\n",
            "Epoch: 1, Training Loss: 2.3805365562438965\n",
            "Epoch: 1, Training Loss: 2.608086585998535\n",
            "Epoch: 1, Training Loss: 2.542541742324829\n",
            "Epoch: 1, Training Loss: 2.0676748752593994\n",
            "Epoch: 1, Training Loss: 2.249868631362915\n",
            "Epoch: 1, Training Loss: 2.4639699459075928\n",
            "Epoch: 1, Training Loss: 2.486065149307251\n",
            "Epoch: 1, Training Loss: 2.6405482292175293\n",
            "Epoch: 1, Training Loss: 2.29026460647583\n",
            "Epoch: 1, Training Loss: 2.54184889793396\n",
            "Epoch: 1, Training Loss: 2.904844284057617\n",
            "Epoch: 1, Training Loss: 2.394986867904663\n",
            "Epoch: 1, Training Loss: 2.5113368034362793\n",
            "Epoch: 1, Training Loss: 2.2829699516296387\n",
            "Epoch: 1, Training Loss: 2.5381863117218018\n",
            "Epoch: 1, Training Loss: 2.3497719764709473\n",
            "Epoch: 1, Training Loss: 2.367039918899536\n",
            "Epoch: 1, Training Loss: 2.3058359622955322\n",
            "Epoch: 1, Training Loss: 2.2173819541931152\n",
            "Epoch: 1, Training Loss: 2.2414212226867676\n",
            "Epoch: 1, Training Loss: 2.4532878398895264\n",
            "Epoch: 1, Training Loss: 2.0093331336975098\n",
            "Epoch: 1, Training Loss: 2.297017812728882\n",
            "Epoch: 1, Training Loss: 2.370326042175293\n",
            "Epoch: 1, Training Loss: 2.4080586433410645\n",
            "Epoch: 1, Training Loss: 2.145477056503296\n",
            "Epoch: 1, Training Loss: 2.6218442916870117\n",
            "Epoch: 1, Training Loss: 2.394928216934204\n",
            "Epoch: 1, Training Loss: 2.3007781505584717\n",
            "Epoch: 1, Training Loss: 2.3719124794006348\n",
            "Epoch: 1, Training Loss: 2.3655803203582764\n",
            "Epoch: 1, Training Loss: 2.518249988555908\n",
            "Epoch: 1, Training Loss: 2.1358985900878906\n",
            "Epoch: 1, Training Loss: 2.345529556274414\n",
            "Epoch: 1, Training Loss: 2.466794013977051\n",
            "Epoch: 1, Training Loss: 2.2458930015563965\n",
            "Epoch: 1, Training Loss: 2.012087106704712\n",
            "Epoch: 1, Training Loss: 2.316538095474243\n",
            "Epoch: 1, Training Loss: 2.208219289779663\n",
            "Epoch: 1, Training Loss: 2.3994123935699463\n",
            "Epoch: 1, Training Loss: 2.275585889816284\n",
            "Epoch: 1, Training Loss: 2.602750062942505\n",
            "Epoch: 1, Training Loss: 2.297090768814087\n",
            "Epoch: 1, Training Loss: 2.469350576400757\n",
            "Epoch: 1, Training Loss: 2.605635166168213\n",
            "Epoch: 1, Training Loss: 2.4834787845611572\n",
            "Epoch: 1, Training Loss: 2.2995824813842773\n",
            "Epoch: 1, Training Loss: 2.524683713912964\n",
            "Epoch: 1, Training Loss: 2.272599458694458\n",
            "Epoch: 1, Training Loss: 2.3071036338806152\n",
            "Epoch: 1, Training Loss: 2.439932107925415\n",
            "Epoch: 1, Training Loss: 2.514315605163574\n",
            "Epoch: 1, Training Loss: 2.154332399368286\n",
            "Epoch: 1, Training Loss: 2.4461710453033447\n",
            "Epoch: 1, Training Loss: 2.344975709915161\n",
            "Epoch: 1, Training Loss: 2.7552452087402344\n",
            "Epoch: 1, Training Loss: 2.288652181625366\n",
            "Epoch: 1, Training Loss: 2.218522071838379\n",
            "Epoch: 1, Training Loss: 2.540419816970825\n",
            "Epoch: 1, Training Loss: 2.342522144317627\n",
            "Epoch: 1, Training Loss: 2.5714094638824463\n",
            "Epoch: 1, Training Loss: 2.4707889556884766\n",
            "Epoch: 1, Training Loss: 2.5396745204925537\n",
            "Epoch: 1, Training Loss: 2.560680627822876\n",
            "Epoch: 1, Training Loss: 2.2817370891571045\n",
            "Epoch: 1, Training Loss: 2.5116093158721924\n",
            "Epoch: 1, Training Loss: 2.837292194366455\n",
            "Epoch: 1, Training Loss: 2.1275134086608887\n",
            "Epoch: 1, Training Loss: 2.5566349029541016\n",
            "Epoch: 1, Training Loss: 2.4035589694976807\n",
            "Epoch: 1, Training Loss: 2.58095383644104\n",
            "Epoch: 1, Training Loss: 2.3775999546051025\n",
            "Epoch: 1, Training Loss: 2.523662805557251\n",
            "Epoch: 1, Training Loss: 2.4363861083984375\n",
            "Epoch: 1, Training Loss: 2.4658613204956055\n",
            "Epoch: 1, Training Loss: 2.253084421157837\n",
            "Epoch: 1, Training Loss: 2.1645212173461914\n",
            "Epoch: 1, Training Loss: 2.1993448734283447\n",
            "Epoch: 1, Training Loss: 2.3827247619628906\n",
            "Epoch: 1, Training Loss: 2.3462462425231934\n",
            "Epoch: 1, Training Loss: 2.2764313220977783\n",
            "Epoch: 1, Training Loss: 2.315992832183838\n",
            "Epoch: 1, Training Loss: 2.2083334922790527\n",
            "Epoch: 1, Training Loss: 2.4055519104003906\n",
            "Epoch: 1, Training Loss: 2.216620445251465\n",
            "Epoch: 1, Training Loss: 2.378980875015259\n",
            "Epoch: 1, Training Loss: 2.430907964706421\n",
            "Epoch: 1, Training Loss: 2.184556722640991\n",
            "Epoch: 1, Training Loss: 2.4597811698913574\n",
            "Epoch: 1, Training Loss: 2.377969980239868\n",
            "Epoch: 1, Training Loss: 2.2243711948394775\n",
            "Epoch: 1, Training Loss: 2.4273829460144043\n",
            "Epoch: 1, Training Loss: 2.3192834854125977\n",
            "Epoch: 1, Training Loss: 2.5033695697784424\n",
            "Epoch: 1, Training Loss: 2.2723584175109863\n",
            "Epoch: 1, Training Loss: 2.67287278175354\n",
            "Epoch: 1, Training Loss: 2.5708327293395996\n",
            "Epoch: 1, Training Loss: 2.35788893699646\n",
            "Epoch: 1, Training Loss: 2.5163817405700684\n",
            "Epoch: 1, Training Loss: 2.3587253093719482\n",
            "Epoch: 1, Training Loss: 2.251023769378662\n",
            "Epoch: 1, Training Loss: 1.9653613567352295\n",
            "Epoch: 1, Training Loss: 2.2578396797180176\n",
            "Epoch: 1, Training Loss: 2.373579740524292\n",
            "Epoch: 1, Training Loss: 2.122990846633911\n",
            "Epoch: 1, Training Loss: 2.3068485260009766\n",
            "Epoch: 1, Training Loss: 2.1773171424865723\n",
            "Epoch: 1, Training Loss: 2.2827255725860596\n",
            "Epoch: 1, Training Loss: 2.283085346221924\n",
            "Epoch: 1, Training Loss: 2.416036605834961\n",
            "Epoch: 1, Training Loss: 2.5218381881713867\n",
            "Epoch: 1, Training Loss: 2.223538637161255\n",
            "Epoch: 1, Training Loss: 2.298287868499756\n",
            "Epoch: 1, Training Loss: 2.433022975921631\n",
            "Epoch: 1, Training Loss: 2.303267240524292\n",
            "Epoch: 1, Training Loss: 2.262876272201538\n",
            "Epoch: 1, Training Loss: 2.5065107345581055\n",
            "Epoch: 1, Training Loss: 2.3258464336395264\n",
            "Epoch: 1, Training Loss: 2.3867180347442627\n",
            "Epoch: 1, Training Loss: 2.5134902000427246\n",
            "Epoch: 1, Training Loss: 2.4838857650756836\n",
            "Epoch: 1, Training Loss: 2.292978525161743\n",
            "Epoch: 1, Training Loss: 2.6850767135620117\n",
            "Epoch: 1, Training Loss: 2.381880760192871\n",
            "Epoch: 1, Training Loss: 2.277053117752075\n",
            "Epoch: 1, Training Loss: 2.471031665802002\n",
            "Epoch: 1, Training Loss: 2.3798673152923584\n",
            "Epoch: 1, Training Loss: 2.1175284385681152\n",
            "Epoch: 1, Training Loss: 2.352534770965576\n",
            "Epoch: 1, Training Loss: 2.4752695560455322\n",
            "Epoch: 1, Training Loss: 2.4684386253356934\n",
            "Epoch: 1, Training Loss: 2.3004727363586426\n",
            "Epoch: 1, Training Loss: 2.4476475715637207\n",
            "Epoch: 1, Training Loss: 2.261134624481201\n",
            "Epoch: 1, Training Loss: 2.4876816272735596\n",
            "Epoch: 1, Training Loss: 2.569659948348999\n",
            "Epoch: 1, Training Loss: 2.2683122158050537\n",
            "Epoch: 1, Training Loss: 2.6850709915161133\n",
            "Epoch: 1, Training Loss: 2.048288106918335\n",
            "Epoch: 1, Training Loss: 2.341524839401245\n",
            "Epoch: 1, Training Loss: 2.4754526615142822\n",
            "Epoch: 1, Training Loss: 1.9434267282485962\n",
            "Epoch: 1, Training Loss: 2.2957077026367188\n",
            "Epoch: 1, Training Loss: 2.3046607971191406\n",
            "Epoch: 1, Training Loss: 2.1442506313323975\n",
            "Epoch: 1, Training Loss: 2.206310987472534\n",
            "Epoch: 1, Training Loss: 2.3200900554656982\n",
            "Epoch: 1, Training Loss: 2.1408231258392334\n",
            "Epoch: 1, Training Loss: 2.153684139251709\n",
            "Epoch: 1, Training Loss: 2.531344175338745\n",
            "Epoch: 1, Training Loss: 2.3922908306121826\n",
            "Epoch: 1, Training Loss: 2.0772132873535156\n",
            "Epoch: 1, Training Loss: 2.35395884513855\n",
            "Epoch: 1, Training Loss: 2.3419957160949707\n",
            "Epoch: 1, Training Loss: 2.4027597904205322\n",
            "Epoch: 1, Training Loss: 2.345072031021118\n",
            "Epoch: 1, Training Loss: 2.471006155014038\n",
            "Epoch: 1, Training Loss: 2.549802541732788\n",
            "Epoch: 1, Training Loss: 2.326291084289551\n",
            "Epoch: 1, Training Loss: 2.481283664703369\n",
            "Epoch: 1, Training Loss: 2.46512508392334\n",
            "Epoch: 1, Training Loss: 2.2051455974578857\n",
            "Epoch: 1, Training Loss: 2.5641815662384033\n",
            "Epoch: 1, Training Loss: 2.3581557273864746\n",
            "Epoch: 1, Training Loss: 2.4045462608337402\n",
            "Epoch: 1, Training Loss: 2.146557331085205\n",
            "Epoch: 1, Training Loss: 2.2767980098724365\n",
            "Epoch: 1, Training Loss: 2.414536237716675\n",
            "Epoch: 1, Training Loss: 1.973289966583252\n",
            "Epoch: 1, Training Loss: 2.322657585144043\n",
            "Epoch: 1, Training Loss: 2.1837453842163086\n",
            "Epoch: 1, Training Loss: 2.335989475250244\n",
            "Epoch: 1, Training Loss: 2.4668796062469482\n",
            "Epoch: 1, Training Loss: 2.389260768890381\n",
            "Epoch: 1, Training Loss: 2.4120872020721436\n",
            "Epoch: 1, Training Loss: 2.3244705200195312\n",
            "Epoch: 1, Training Loss: 2.2616403102874756\n",
            "Epoch: 1, Training Loss: 2.646900177001953\n",
            "Epoch: 1, Training Loss: 2.390547037124634\n",
            "Epoch: 1, Training Loss: 2.331987142562866\n",
            "Epoch: 1, Training Loss: 2.350879192352295\n",
            "Epoch: 1, Training Loss: 2.3940305709838867\n",
            "Epoch: 1, Training Loss: 2.2171318531036377\n",
            "Epoch: 1, Training Loss: 2.52127742767334\n",
            "Epoch: 1, Training Loss: 2.459711790084839\n",
            "Epoch: 1, Training Loss: 2.284407138824463\n",
            "Epoch: 1, Training Loss: 2.3119163513183594\n",
            "Epoch: 1, Training Loss: 2.400036573410034\n",
            "Epoch: 1, Training Loss: 2.3214221000671387\n",
            "Epoch: 1, Training Loss: 2.155167579650879\n",
            "Epoch: 1, Training Loss: 2.1311278343200684\n",
            "Epoch: 1, Training Loss: 2.039557933807373\n",
            "Epoch: 1, Training Loss: 2.3542726039886475\n",
            "Epoch: 1, Training Loss: 2.297184944152832\n",
            "Epoch: 1, Training Loss: 2.319762945175171\n",
            "Epoch: 1, Training Loss: 2.2560713291168213\n",
            "Epoch: 1, Training Loss: 2.657304048538208\n",
            "Epoch: 1, Training Loss: 2.2775325775146484\n",
            "Epoch: 1, Training Loss: 2.491645097732544\n",
            "Epoch: 1, Training Loss: 2.1518115997314453\n",
            "Epoch: 1, Training Loss: 2.5629680156707764\n",
            "Epoch: 1, Training Loss: 2.40317702293396\n",
            "Epoch: 1, Training Loss: 2.234407901763916\n",
            "Epoch: 1, Training Loss: 2.509488105773926\n",
            "Epoch: 1, Training Loss: 2.3249809741973877\n",
            "Epoch: 1, Training Loss: 2.2876408100128174\n",
            "Epoch: 1, Training Loss: 2.168466329574585\n",
            "Epoch: 1, Training Loss: 2.6648051738739014\n",
            "Epoch: 1, Training Loss: 2.1800997257232666\n",
            "Epoch: 1, Training Loss: 2.5123467445373535\n",
            "Epoch: 1, Training Loss: 2.189324140548706\n",
            "Epoch: 1, Training Loss: 2.629481077194214\n",
            "Epoch: 1, Training Loss: 2.445712089538574\n",
            "Epoch: 1, Training Loss: 2.070871114730835\n",
            "Epoch: 1, Training Loss: 2.3412563800811768\n",
            "Epoch: 1, Training Loss: 2.1981568336486816\n",
            "Epoch: 1, Training Loss: 2.180609941482544\n",
            "Epoch: 1, Training Loss: 2.28660249710083\n",
            "Epoch: 1, Training Loss: 2.249469518661499\n",
            "Epoch: 1, Training Loss: 2.2870163917541504\n",
            "Epoch: 1, Training Loss: 2.233748197555542\n",
            "Epoch: 1, Training Loss: 2.051682710647583\n",
            "Epoch: 1, Training Loss: 2.483577013015747\n",
            "Epoch: 1, Training Loss: 2.2314224243164062\n",
            "Epoch: 1, Training Loss: 2.266880989074707\n",
            "Epoch: 1, Training Loss: 2.625356674194336\n",
            "Epoch: 1, Training Loss: 2.527362108230591\n",
            "Epoch: 1, Training Loss: 2.4420151710510254\n",
            "Epoch: 1, Training Loss: 2.3022029399871826\n",
            "Epoch: 1, Training Loss: 2.3925983905792236\n",
            "Epoch: 1, Training Loss: 2.3823626041412354\n",
            "Epoch: 1, Training Loss: 2.3878543376922607\n",
            "Epoch: 1, Training Loss: 2.366429567337036\n",
            "Epoch: 1, Training Loss: 2.3818540573120117\n",
            "Epoch: 1, Training Loss: 2.6881494522094727\n",
            "Epoch: 1, Training Loss: 2.219809055328369\n",
            "Epoch: 1, Training Loss: 2.2666420936584473\n",
            "Epoch: 1, Training Loss: 2.563106060028076\n",
            "Epoch: 1, Training Loss: 1.9814647436141968\n",
            "Epoch: 1, Training Loss: 2.2310774326324463\n",
            "Epoch: 1, Training Loss: 2.195666551589966\n",
            "Epoch: 1, Training Loss: 2.436593532562256\n",
            "Epoch: 1, Training Loss: 2.2728188037872314\n",
            "Epoch: 1, Training Loss: 2.278783082962036\n",
            "Epoch: 1, Training Loss: 2.4083688259124756\n",
            "Epoch: 1, Training Loss: 2.4036757946014404\n",
            "Epoch: 1, Training Loss: 2.39473557472229\n",
            "Epoch: 1, Training Loss: 2.210339069366455\n",
            "Epoch: 1, Training Loss: 2.672496795654297\n",
            "Epoch: 1, Training Loss: 2.4384195804595947\n",
            "Epoch: 1, Training Loss: 2.563779830932617\n",
            "Epoch: 1, Training Loss: 2.343712568283081\n",
            "Epoch: 1, Training Loss: 2.268796682357788\n",
            "Epoch: 1, Training Loss: 2.173515558242798\n",
            "Epoch: 1, Training Loss: 2.348315477371216\n",
            "Epoch: 1, Training Loss: 2.043658494949341\n",
            "Epoch: 1, Training Loss: 2.3303945064544678\n",
            "Epoch: 1, Training Loss: 2.256894826889038\n",
            "Epoch: 1, Training Loss: 2.0167970657348633\n",
            "Epoch: 1, Training Loss: 2.0499982833862305\n",
            "Epoch: 1, Training Loss: 2.367420196533203\n",
            "Epoch: 1, Training Loss: 2.254755973815918\n",
            "Epoch: 1, Training Loss: 2.0150580406188965\n",
            "Epoch: 1, Training Loss: 2.223644495010376\n",
            "Epoch: 1, Training Loss: 2.331563711166382\n",
            "Epoch: 1, Training Loss: 2.2943339347839355\n",
            "Epoch: 1, Training Loss: 2.4321460723876953\n",
            "Epoch: 1, Training Loss: 2.4127721786499023\n",
            "Epoch: 1, Training Loss: 1.8552472591400146\n",
            "Epoch: 1, Training Loss: 2.278456687927246\n",
            "Epoch: 1, Training Loss: 2.1803271770477295\n",
            "Epoch: 1, Training Loss: 2.5442490577697754\n",
            "Epoch: 1, Training Loss: 2.1025564670562744\n",
            "Epoch: 1, Training Loss: 2.4727561473846436\n",
            "Epoch: 1, Training Loss: 2.440617561340332\n",
            "Epoch: 1, Training Loss: 2.1939966678619385\n",
            "Epoch: 1, Training Loss: 2.4604711532592773\n",
            "Epoch: 1, Training Loss: 2.3795223236083984\n",
            "Epoch: 1, Training Loss: 2.1583211421966553\n",
            "Epoch: 1, Training Loss: 2.5260491371154785\n",
            "Epoch: 1, Training Loss: 2.591813564300537\n",
            "Epoch: 1, Training Loss: 2.1603612899780273\n",
            "Epoch: 1, Training Loss: 2.1745047569274902\n",
            "Epoch: 1, Training Loss: 2.2155771255493164\n",
            "Epoch: 1, Training Loss: 2.1880948543548584\n",
            "Epoch: 1, Training Loss: 2.031811237335205\n",
            "Epoch: 1, Training Loss: 2.494843006134033\n",
            "Epoch: 1, Training Loss: 2.468609571456909\n",
            "Epoch: 1, Training Loss: 2.4258227348327637\n",
            "Epoch: 1, Training Loss: 2.527033805847168\n",
            "Epoch: 1, Training Loss: 2.146294355392456\n",
            "Epoch: 1, Training Loss: 2.417769432067871\n",
            "Epoch: 1, Training Loss: 2.1516973972320557\n",
            "Epoch: 1, Training Loss: 2.330894947052002\n",
            "Epoch: 1, Training Loss: 2.457151412963867\n",
            "Epoch: 1, Training Loss: 2.3182783126831055\n",
            "Epoch: 1, Training Loss: 2.1553027629852295\n",
            "Epoch: 1, Training Loss: 2.1541061401367188\n",
            "Epoch: 1, Training Loss: 2.349435329437256\n",
            "Epoch: 1, Training Loss: 2.35147762298584\n",
            "Epoch: 1, Training Loss: 2.503836154937744\n",
            "Epoch: 1, Training Loss: 2.4859657287597656\n",
            "Epoch: 1, Training Loss: 2.4184939861297607\n",
            "Epoch: 1, Training Loss: 2.2308168411254883\n",
            "Epoch: 1, Training Loss: 2.174163341522217\n",
            "Epoch: 1, Training Loss: 2.333225727081299\n",
            "Epoch: 1, Training Loss: 2.4778265953063965\n",
            "Epoch: 1, Training Loss: 2.4017109870910645\n",
            "Epoch: 1, Training Loss: 2.5020084381103516\n",
            "Epoch: 1, Training Loss: 2.4065515995025635\n",
            "Epoch: 1, Training Loss: 2.3833813667297363\n",
            "Epoch: 1, Training Loss: 2.0803213119506836\n",
            "Epoch: 1, Training Loss: 2.350374937057495\n",
            "Epoch: 1, Training Loss: 2.4870150089263916\n",
            "Epoch: 1, Training Loss: 2.616305351257324\n",
            "Epoch: 1, Training Loss: 2.278367519378662\n",
            "Epoch: 1, Training Loss: 2.1374058723449707\n",
            "Epoch: 1, Training Loss: 2.428844451904297\n",
            "Epoch: 1, Training Loss: 2.4866445064544678\n",
            "Epoch: 1, Training Loss: 2.2848572731018066\n",
            "Epoch: 1, Training Loss: 2.1446285247802734\n",
            "Epoch: 1, Training Loss: 2.3059775829315186\n",
            "Epoch: 1, Training Loss: 2.4353411197662354\n",
            "Epoch: 1, Training Loss: 2.3486154079437256\n",
            "Epoch: 1, Training Loss: 2.2123429775238037\n",
            "Epoch: 1, Training Loss: 2.324398994445801\n",
            "Epoch: 1, Training Loss: 2.169832944869995\n",
            "Epoch: 1, Training Loss: 2.508068561553955\n",
            "Epoch: 1, Training Loss: 2.21108341217041\n",
            "Epoch: 1, Training Loss: 2.0922043323516846\n",
            "Epoch: 1, Training Loss: 2.39626407623291\n",
            "Epoch: 1, Training Loss: 2.1402320861816406\n",
            "Epoch: 1, Training Loss: 2.2495064735412598\n",
            "Epoch: 1, Training Loss: 2.0774619579315186\n",
            "Epoch: 1, Validation Loss: 1.8814785480499268\n",
            "Epoch: 1, Validation Loss: 2.139530658721924\n",
            "Epoch: 1, Validation Loss: 2.1997296810150146\n",
            "Epoch: 1, Validation Loss: 1.85032057762146\n",
            "Epoch: 1, Validation Loss: 2.132154941558838\n",
            "Epoch: 1, Validation Loss: 2.205993175506592\n",
            "Epoch: 1, Validation Loss: 1.863224983215332\n",
            "Epoch: 1, Validation Loss: 2.0164525508880615\n",
            "Epoch: 1, Validation Loss: 2.2422077655792236\n",
            "Epoch: 1, Validation Loss: 2.126307487487793\n",
            "Epoch: 1, Validation Loss: 2.092960834503174\n",
            "Epoch: 1, Validation Loss: 1.9763007164001465\n",
            "Epoch: 1, Validation Loss: 2.2591195106506348\n",
            "Epoch: 1, Validation Loss: 1.967459797859192\n",
            "Epoch: 1, Validation Loss: 2.115835428237915\n",
            "Epoch: 1, Validation Loss: 2.1372787952423096\n",
            "Epoch: 1, Validation Loss: 2.1940557956695557\n",
            "Epoch: 1, Validation Loss: 2.1011319160461426\n",
            "Epoch: 1, Validation Loss: 1.9210302829742432\n",
            "Epoch: 1, Validation Loss: 2.1724135875701904\n",
            "Epoch: 1, Validation Loss: 2.3266730308532715\n",
            "Epoch: 1, Validation Loss: 2.271557331085205\n",
            "Epoch: 1, Validation Loss: 1.9460824728012085\n",
            "Epoch: 1, Validation Loss: 1.9643887281417847\n",
            "Epoch: 1, Validation Loss: 1.921342372894287\n",
            "Epoch: 1, Validation Loss: 2.2001500129699707\n",
            "Epoch: 1, Validation Loss: 2.096174478530884\n",
            "Epoch: 1, Validation Loss: 2.167084217071533\n",
            "Epoch: 1, Validation Loss: 2.060622215270996\n",
            "Epoch: 1, Validation Loss: 2.189674139022827\n",
            "Epoch: 1, Validation Loss: 2.25681734085083\n",
            "Epoch: 1, Validation Loss: 2.39111065864563\n",
            "Epoch: 1, Validation Loss: 2.0735158920288086\n",
            "Epoch: 1, Validation Loss: 2.134779453277588\n",
            "Epoch: 1, Validation Loss: 2.1599717140197754\n",
            "Epoch: 1, Validation Loss: 2.232999801635742\n",
            "Epoch: 1, Validation Loss: 2.079296588897705\n",
            "Epoch: 1, Validation Loss: 2.0814640522003174\n",
            "Epoch: 1, Validation Loss: 2.0726420879364014\n",
            "Epoch: 1, Validation Loss: 1.8931300640106201\n",
            "Epoch: 1, Validation Loss: 1.988219976425171\n",
            "Epoch: 1, Validation Loss: 2.3823771476745605\n",
            "Epoch: 1, Validation Loss: 2.1527602672576904\n",
            "Epoch: 1, Validation Loss: 2.186264753341675\n",
            "Epoch: 1, Validation Loss: 2.2231640815734863\n",
            "Epoch: 1, Validation Loss: 1.8863722085952759\n",
            "Epoch: 1, Validation Loss: 1.9141316413879395\n",
            "Epoch: 1, Validation Loss: 2.0653865337371826\n",
            "Epoch: 1, Validation Loss: 2.093365430831909\n",
            "Epoch: 1, Validation Loss: 2.1478207111358643\n",
            "Epoch: 1, Validation Loss: 1.8573063611984253\n",
            "Epoch: 1, Validation Loss: 2.1093509197235107\n",
            "Epoch: 1, Validation Loss: 1.935813069343567\n",
            "Epoch: 1, Validation Loss: 2.2718491554260254\n",
            "Epoch: 1, Validation Loss: 1.988634705543518\n",
            "Epoch: 1, Validation Loss: 2.2008347511291504\n",
            "Epoch: 1, Validation Loss: 2.105454683303833\n",
            "Epoch: 1, Validation Loss: 2.109283447265625\n",
            "Epoch: 1, Validation Loss: 1.7725015878677368\n",
            "Epoch: 1, Validation Loss: 2.121776819229126\n",
            "Epoch: 1, Validation Loss: 2.107640504837036\n",
            "Epoch: 1, Validation Loss: 2.105109214782715\n",
            "Epoch: 1, Validation Loss: 1.9771775007247925\n",
            "Epoch: 1, Validation Loss: 2.1247668266296387\n",
            "Epoch: 1, Validation Loss: 2.1364119052886963\n",
            "Epoch: 1, Validation Loss: 1.91387939453125\n",
            "Epoch: 1, Validation Loss: 2.2726564407348633\n",
            "Epoch: 1, Validation Loss: 2.0743911266326904\n",
            "Epoch: 1, Validation Loss: 1.8844817876815796\n",
            "Epoch: 1, Validation Loss: 2.239386796951294\n",
            "Epoch: 1, Validation Loss: 2.052732229232788\n",
            "Epoch: 1, Validation Loss: 2.149303436279297\n",
            "Epoch: 1, Validation Loss: 1.818585991859436\n",
            "Epoch: 1, Validation Loss: 2.246840238571167\n",
            "Epoch: 1, Validation Loss: 2.140407085418701\n",
            "Epoch: 1, Validation Loss: 2.502584457397461\n",
            "Epoch: 1, Validation Loss: 1.9267308712005615\n",
            "Epoch: 1, Validation Loss: 1.8480058908462524\n",
            "Epoch: 1, Validation Loss: 1.912896990776062\n",
            "Epoch: 1, Validation Loss: 2.1205098628997803\n",
            "Epoch: 1, Validation Loss: 2.132575511932373\n",
            "Epoch: 1, Validation Loss: 1.934002161026001\n",
            "Epoch: 1, Validation Loss: 2.149867057800293\n",
            "Epoch: 1, Validation Loss: 1.9940197467803955\n",
            "Epoch: 1, Validation Loss: 2.096482992172241\n",
            "Epoch: 1, Validation Loss: 2.2709310054779053\n",
            "Epoch: 1, Validation Loss: 2.1496360301971436\n",
            "Epoch: 1, Validation Loss: 2.2356812953948975\n",
            "Epoch: 1, Validation Loss: 2.1611602306365967\n",
            "Epoch: 1, Validation Loss: 2.1563403606414795\n",
            "Epoch: 1, Validation Loss: 2.091423749923706\n",
            "Epoch: 1, Validation Loss: 1.9783318042755127\n",
            "Epoch: 1, Validation Loss: 2.050661325454712\n",
            "Epoch: 1, Validation Loss: 2.0529017448425293\n",
            "Epoch: 1, Validation Loss: 2.2208616733551025\n",
            "Epoch: 1, Validation Loss: 2.1828856468200684\n",
            "Epoch: 1, Validation Loss: 2.2505176067352295\n",
            "Epoch: 1, Validation Loss: 1.994471788406372\n",
            "Epoch: 1, Validation Loss: 2.0533010959625244\n",
            "Epoch: 1, Validation Loss: 2.191013813018799\n",
            "Epoch: 1, Validation Loss: 2.3455543518066406\n",
            "Epoch: 1, Validation Loss: 2.1119253635406494\n",
            "Epoch: 1, Validation Loss: 2.2771189212799072\n",
            "Epoch: 1, Validation Loss: 2.2512736320495605\n",
            "Epoch: 1, Validation Loss: 2.040888786315918\n",
            "Epoch: 1, Validation Loss: 2.3517284393310547\n",
            "Epoch: 1, Validation Loss: 2.081862211227417\n",
            "Epoch: 1, Validation Loss: 1.9679621458053589\n",
            "Epoch: 1, Validation Loss: 1.9601279497146606\n",
            "Epoch: 1, Validation Loss: 2.1150872707366943\n",
            "Epoch: 1, Validation Loss: 1.9455294609069824\n",
            "Epoch: 1, Validation Loss: 1.751896619796753\n",
            "Epoch: 1, Validation Loss: 2.1311583518981934\n",
            "Epoch: 1, Validation Loss: 1.726117491722107\n",
            "Epoch: 1, Validation Loss: 2.2061450481414795\n",
            "Epoch: 1, Validation Loss: 2.141178846359253\n",
            "Epoch: 1, Validation Loss: 1.7117915153503418\n",
            "Epoch: 1, Validation Loss: 1.987998366355896\n",
            "Epoch: 1, Validation Loss: 1.9318156242370605\n",
            "Epoch: 1, Validation Loss: 2.043423891067505\n",
            "Epoch: 1, Validation Loss: 2.126862049102783\n",
            "Epoch: 1, Validation Loss: 2.0889410972595215\n",
            "Epoch: 1, Validation Loss: 2.17338228225708\n",
            "Epoch: 1, Validation Loss: 2.189157009124756\n",
            "Epoch: 1, Validation Loss: 2.2306840419769287\n",
            "Epoch: 1, Validation Loss: 1.8789492845535278\n",
            "Epoch: 1, Validation Loss: 2.296869993209839\n",
            "Epoch: 1, Validation Loss: 2.1848981380462646\n",
            "Epoch: 1, Validation Loss: 2.2378742694854736\n",
            "Epoch: 1, Validation Loss: 2.207925319671631\n",
            "Epoch: 1, Validation Loss: 2.1882035732269287\n",
            "Epoch: 1, Validation Loss: 2.103731870651245\n",
            "Epoch: 1, Validation Loss: 2.2516250610351562\n",
            "Epoch: 1, Validation Loss: 2.37014102935791\n",
            "Epoch: 1, Validation Loss: 1.954723596572876\n",
            "Epoch: 1, Validation Loss: 1.978337049484253\n",
            "Epoch: 1, Validation Loss: 1.9192768335342407\n",
            "Epoch: 1, Validation Loss: 2.1388089656829834\n",
            "Epoch: 1, Validation Loss: 2.146510601043701\n",
            "Epoch: 1, Validation Loss: 2.3579261302948\n",
            "Epoch: 1, Validation Loss: 1.9534029960632324\n",
            "Epoch: 1, Validation Loss: 1.8962515592575073\n",
            "Epoch: 1, Validation Loss: 2.0783019065856934\n",
            "Epoch: 1, Validation Loss: 2.0953011512756348\n",
            "Epoch: 1, Validation Loss: 2.3121960163116455\n",
            "Epoch: 1, Validation Loss: 1.8486626148223877\n",
            "Epoch: 1, Validation Loss: 1.9900890588760376\n",
            "Epoch: 1, Validation Loss: 2.414947509765625\n",
            "Epoch: 1, Validation Loss: 1.790623426437378\n",
            "Epoch: 1, Validation Loss: 1.9997938871383667\n",
            "Epoch: 1, Validation Loss: 1.9751654863357544\n",
            "Epoch: 1, Validation Loss: 2.1943142414093018\n",
            "Epoch: 1, Validation Loss: 2.1053900718688965\n",
            "Epoch: 1, Validation Loss: 1.9547778367996216\n",
            "Epoch: 1, Validation Loss: 2.151211977005005\n",
            "Epoch: 1, Validation Loss: 2.301133155822754\n",
            "Epoch: 1, Validation Loss: 2.24869704246521\n",
            "Epoch: 1, Validation Loss: 2.1938533782958984\n",
            "Epoch: 1, Validation Loss: 2.028625965118408\n",
            "Epoch: 1, Validation Loss: 2.2094175815582275\n",
            "Epoch: 1, Validation Loss: 1.8872933387756348\n",
            "Epoch: 1, Validation Loss: 2.187018394470215\n",
            "Epoch: 1, Validation Loss: 2.075937032699585\n",
            "Epoch: 1, Validation Loss: 2.2051901817321777\n",
            "Epoch: 1, Validation Loss: 2.1858301162719727\n",
            "Epoch: 1, Validation Loss: 2.0196757316589355\n",
            "Epoch: 1, Validation Loss: 2.147462844848633\n",
            "Epoch: 1, Validation Loss: 2.3120925426483154\n",
            "Epoch: 1, Validation Loss: 1.6935007572174072\n",
            "Epoch: 1, Validation Loss: 2.1059534549713135\n",
            "Epoch: 1, Validation Loss: 2.189131021499634\n",
            "Epoch: 1, Validation Loss: 1.7532867193222046\n",
            "Epoch: 1, Validation Loss: 1.9498006105422974\n",
            "Epoch: 1, Validation Loss: 2.2155213356018066\n",
            "Epoch: 1, Validation Loss: 2.0521180629730225\n",
            "Epoch: 1, Validation Loss: 1.9850304126739502\n",
            "Epoch: 1, Validation Loss: 2.103013753890991\n",
            "Epoch: 1, Validation Loss: 1.9571722745895386\n",
            "Epoch: 1, Validation Loss: 2.121490955352783\n",
            "Epoch: 1, Validation Loss: 2.251237630844116\n",
            "Epoch: 1, Validation Loss: 2.0718724727630615\n",
            "Epoch: 1, Validation Loss: 1.9592201709747314\n",
            "Epoch: 1, Validation Loss: 2.102858066558838\n",
            "Epoch: 1, Validation Loss: 2.1555869579315186\n",
            "Epoch: 1, Validation Loss: 2.103841543197632\n",
            "Epoch: 1, Validation Loss: 1.8868319988250732\n",
            "Epoch: 1, Validation Loss: 2.2560055255889893\n",
            "Epoch: 1, Validation Loss: 2.199674129486084\n",
            "Epoch: 1, Validation Loss: 1.9853988885879517\n",
            "Epoch: 1, Validation Loss: 2.0880870819091797\n",
            "Epoch: 1, Validation Loss: 2.154279947280884\n",
            "Epoch: 1, Validation Loss: 2.0642435550689697\n",
            "Epoch: 1, Validation Loss: 2.240217685699463\n",
            "Epoch: 1, Validation Loss: 1.9033503532409668\n",
            "Epoch: 1, Validation Loss: 2.1275742053985596\n",
            "Epoch: 1, Validation Loss: 1.910369873046875\n",
            "Epoch: 1, Validation Loss: 2.2224223613739014\n",
            "Epoch: 1, Validation Loss: 2.3382809162139893\n",
            "Epoch: 1, Validation Loss: 2.1638200283050537\n",
            "Epoch: 1, Validation Loss: 2.2175981998443604\n",
            "Epoch: 1, Validation Loss: 2.0652663707733154\n",
            "Epoch: 1, Validation Loss: 2.3589258193969727\n",
            "Epoch: 1, Validation Loss: 2.360973358154297\n",
            "Epoch: 1, Validation Loss: 2.1380903720855713\n",
            "Epoch: 1, Validation Loss: 1.8712841272354126\n",
            "Epoch: 1, Validation Loss: 2.1337265968322754\n",
            "Epoch: 1, Validation Loss: 2.3820104598999023\n",
            "Epoch: 1, Validation Loss: 2.0851938724517822\n",
            "Epoch: 1, Validation Loss: 2.182154655456543\n",
            "Epoch: 1, Validation Loss: 2.0811917781829834\n",
            "Epoch: 1, Validation Loss: 2.1971757411956787\n",
            "Epoch: 1, Validation Loss: 1.9995300769805908\n",
            "Epoch: 1, Validation Loss: 2.0311100482940674\n",
            "Epoch: 1, Validation Loss: 2.0625298023223877\n",
            "Epoch: 1, Validation Loss: 1.995308756828308\n",
            "Epoch: 1, Validation Loss: 2.1672613620758057\n",
            "Epoch: 1, Validation Loss: 2.16159987449646\n",
            "Epoch: 1, Validation Loss: 2.1483676433563232\n",
            "Epoch: 1, Validation Loss: 2.0606350898742676\n",
            "Epoch: 1, Validation Loss: 2.324381113052368\n",
            "Epoch: 1, Validation Loss: 2.352959394454956\n",
            "Epoch: 2\n",
            "------------------------------\n",
            "Epoch: 2, Training Loss: 2.379366159439087\n",
            "Epoch: 2, Training Loss: 2.3310914039611816\n",
            "Epoch: 2, Training Loss: 2.335571765899658\n",
            "Epoch: 2, Training Loss: 2.2298381328582764\n",
            "Epoch: 2, Training Loss: 2.256035327911377\n",
            "Epoch: 2, Training Loss: 2.0547239780426025\n",
            "Epoch: 2, Training Loss: 2.240316867828369\n",
            "Epoch: 2, Training Loss: 2.17997407913208\n",
            "Epoch: 2, Training Loss: 2.101048469543457\n",
            "Epoch: 2, Training Loss: 2.069911241531372\n",
            "Epoch: 2, Training Loss: 2.0736284255981445\n",
            "Epoch: 2, Training Loss: 2.0392630100250244\n",
            "Epoch: 2, Training Loss: 1.9465891122817993\n",
            "Epoch: 2, Training Loss: 2.0348637104034424\n",
            "Epoch: 2, Training Loss: 2.2345616817474365\n",
            "Epoch: 2, Training Loss: 2.1939125061035156\n",
            "Epoch: 2, Training Loss: 2.166520357131958\n",
            "Epoch: 2, Training Loss: 1.998952031135559\n",
            "Epoch: 2, Training Loss: 2.1877081394195557\n",
            "Epoch: 2, Training Loss: 1.9609589576721191\n",
            "Epoch: 2, Training Loss: 2.4431819915771484\n",
            "Epoch: 2, Training Loss: 2.279205560684204\n",
            "Epoch: 2, Training Loss: 2.2008254528045654\n",
            "Epoch: 2, Training Loss: 1.9824551343917847\n",
            "Epoch: 2, Training Loss: 2.0040431022644043\n",
            "Epoch: 2, Training Loss: 2.197962760925293\n",
            "Epoch: 2, Training Loss: 2.5385584831237793\n",
            "Epoch: 2, Training Loss: 2.288072109222412\n",
            "Epoch: 2, Training Loss: 2.3453426361083984\n",
            "Epoch: 2, Training Loss: 2.2819180488586426\n",
            "Epoch: 2, Training Loss: 2.0277512073516846\n",
            "Epoch: 2, Training Loss: 2.2833473682403564\n",
            "Epoch: 2, Training Loss: 1.9637150764465332\n",
            "Epoch: 2, Training Loss: 1.9255722761154175\n",
            "Epoch: 2, Training Loss: 2.2764153480529785\n",
            "Epoch: 2, Training Loss: 2.493356704711914\n",
            "Epoch: 2, Training Loss: 2.278099775314331\n",
            "Epoch: 2, Training Loss: 1.9140781164169312\n",
            "Epoch: 2, Training Loss: 2.5770623683929443\n",
            "Epoch: 2, Training Loss: 2.218691349029541\n",
            "Epoch: 2, Training Loss: 2.113267660140991\n",
            "Epoch: 2, Training Loss: 2.069563150405884\n",
            "Epoch: 2, Training Loss: 1.9989168643951416\n",
            "Epoch: 2, Training Loss: 2.2163641452789307\n",
            "Epoch: 2, Training Loss: 2.193978786468506\n",
            "Epoch: 2, Training Loss: 1.8709758520126343\n",
            "Epoch: 2, Training Loss: 2.1330649852752686\n",
            "Epoch: 2, Training Loss: 2.0155112743377686\n",
            "Epoch: 2, Training Loss: 1.932118535041809\n",
            "Epoch: 2, Training Loss: 2.06547212600708\n",
            "Epoch: 2, Training Loss: 2.312209129333496\n",
            "Epoch: 2, Training Loss: 2.1083459854125977\n",
            "Epoch: 2, Training Loss: 2.1377971172332764\n",
            "Epoch: 2, Training Loss: 1.9181309938430786\n",
            "Epoch: 2, Training Loss: 2.116093635559082\n",
            "Epoch: 2, Training Loss: 2.0660743713378906\n",
            "Epoch: 2, Training Loss: 2.276514768600464\n",
            "Epoch: 2, Training Loss: 2.0768332481384277\n",
            "Epoch: 2, Training Loss: 2.1595473289489746\n",
            "Epoch: 2, Training Loss: 2.0597739219665527\n",
            "Epoch: 2, Training Loss: 2.179555892944336\n",
            "Epoch: 2, Training Loss: 2.2335357666015625\n",
            "Epoch: 2, Training Loss: 2.1046810150146484\n",
            "Epoch: 2, Training Loss: 2.032569169998169\n",
            "Epoch: 2, Training Loss: 2.3318450450897217\n",
            "Epoch: 2, Training Loss: 2.136934995651245\n",
            "Epoch: 2, Training Loss: 1.9862828254699707\n",
            "Epoch: 2, Training Loss: 2.037688732147217\n",
            "Epoch: 2, Training Loss: 2.378967046737671\n",
            "Epoch: 2, Training Loss: 1.8594096899032593\n",
            "Epoch: 2, Training Loss: 2.072143316268921\n",
            "Epoch: 2, Training Loss: 2.165289878845215\n",
            "Epoch: 2, Training Loss: 2.2366011142730713\n",
            "Epoch: 2, Training Loss: 2.355088949203491\n",
            "Epoch: 2, Training Loss: 2.3344736099243164\n",
            "Epoch: 2, Training Loss: 1.7278361320495605\n",
            "Epoch: 2, Training Loss: 2.0829477310180664\n",
            "Epoch: 2, Training Loss: 2.007558822631836\n",
            "Epoch: 2, Training Loss: 2.034714460372925\n",
            "Epoch: 2, Training Loss: 2.257070541381836\n",
            "Epoch: 2, Training Loss: 2.0277204513549805\n",
            "Epoch: 2, Training Loss: 2.077364921569824\n",
            "Epoch: 2, Training Loss: 2.0346198081970215\n",
            "Epoch: 2, Training Loss: 2.20215106010437\n",
            "Epoch: 2, Training Loss: 1.9284113645553589\n",
            "Epoch: 2, Training Loss: 2.185135841369629\n",
            "Epoch: 2, Training Loss: 2.4262478351593018\n",
            "Epoch: 2, Training Loss: 2.2918992042541504\n",
            "Epoch: 2, Training Loss: 1.9027791023254395\n",
            "Epoch: 2, Training Loss: 1.9932047128677368\n",
            "Epoch: 2, Training Loss: 2.2125000953674316\n",
            "Epoch: 2, Training Loss: 2.0846850872039795\n",
            "Epoch: 2, Training Loss: 2.3280258178710938\n",
            "Epoch: 2, Training Loss: 2.091872215270996\n",
            "Epoch: 2, Training Loss: 1.9841328859329224\n",
            "Epoch: 2, Training Loss: 2.001950263977051\n",
            "Epoch: 2, Training Loss: 2.2044870853424072\n",
            "Epoch: 2, Training Loss: 2.0756149291992188\n",
            "Epoch: 2, Training Loss: 2.194103240966797\n",
            "Epoch: 2, Training Loss: 2.1084611415863037\n",
            "Epoch: 2, Training Loss: 2.175358533859253\n",
            "Epoch: 2, Training Loss: 2.093869209289551\n",
            "Epoch: 2, Training Loss: 1.8652136325836182\n",
            "Epoch: 2, Training Loss: 1.734734058380127\n",
            "Epoch: 2, Training Loss: 1.9942257404327393\n",
            "Epoch: 2, Training Loss: 2.341815948486328\n",
            "Epoch: 2, Training Loss: 1.9279865026474\n",
            "Epoch: 2, Training Loss: 1.9819409847259521\n",
            "Epoch: 2, Training Loss: 2.0344533920288086\n",
            "Epoch: 2, Training Loss: 2.1575119495391846\n",
            "Epoch: 2, Training Loss: 2.103365898132324\n",
            "Epoch: 2, Training Loss: 2.092414140701294\n",
            "Epoch: 2, Training Loss: 2.2134742736816406\n",
            "Epoch: 2, Training Loss: 2.2054569721221924\n",
            "Epoch: 2, Training Loss: 2.1519618034362793\n",
            "Epoch: 2, Training Loss: 1.9544872045516968\n",
            "Epoch: 2, Training Loss: 2.1531331539154053\n",
            "Epoch: 2, Training Loss: 2.1616086959838867\n",
            "Epoch: 2, Training Loss: 1.976151466369629\n",
            "Epoch: 2, Training Loss: 2.2450778484344482\n",
            "Epoch: 2, Training Loss: 2.1758270263671875\n",
            "Epoch: 2, Training Loss: 2.0936667919158936\n",
            "Epoch: 2, Training Loss: 1.933501124382019\n",
            "Epoch: 2, Training Loss: 1.940001368522644\n",
            "Epoch: 2, Training Loss: 2.0178990364074707\n",
            "Epoch: 2, Training Loss: 2.3463973999023438\n",
            "Epoch: 2, Training Loss: 2.1761248111724854\n",
            "Epoch: 2, Training Loss: 2.028327226638794\n",
            "Epoch: 2, Training Loss: 2.1973209381103516\n",
            "Epoch: 2, Training Loss: 1.9448517560958862\n",
            "Epoch: 2, Training Loss: 2.282909870147705\n",
            "Epoch: 2, Training Loss: 2.3231565952301025\n",
            "Epoch: 2, Training Loss: 1.8517628908157349\n",
            "Epoch: 2, Training Loss: 2.0512328147888184\n",
            "Epoch: 2, Training Loss: 2.1060831546783447\n",
            "Epoch: 2, Training Loss: 2.131051778793335\n",
            "Epoch: 2, Training Loss: 1.9858061075210571\n",
            "Epoch: 2, Training Loss: 2.2682242393493652\n",
            "Epoch: 2, Training Loss: 2.0401248931884766\n",
            "Epoch: 2, Training Loss: 2.192225933074951\n",
            "Epoch: 2, Training Loss: 2.1714706420898438\n",
            "Epoch: 2, Training Loss: 2.071051597595215\n",
            "Epoch: 2, Training Loss: 2.0727319717407227\n",
            "Epoch: 2, Training Loss: 2.0036137104034424\n",
            "Epoch: 2, Training Loss: 2.0845787525177\n",
            "Epoch: 2, Training Loss: 1.910946249961853\n",
            "Epoch: 2, Training Loss: 2.079613208770752\n",
            "Epoch: 2, Training Loss: 2.0364768505096436\n",
            "Epoch: 2, Training Loss: 2.023267984390259\n",
            "Epoch: 2, Training Loss: 1.913151502609253\n",
            "Epoch: 2, Training Loss: 2.040447235107422\n",
            "Epoch: 2, Training Loss: 2.1121714115142822\n",
            "Epoch: 2, Training Loss: 2.0339019298553467\n",
            "Epoch: 2, Training Loss: 2.011096477508545\n",
            "Epoch: 2, Training Loss: 2.243302583694458\n",
            "Epoch: 2, Training Loss: 1.9469799995422363\n",
            "Epoch: 2, Training Loss: 2.3073174953460693\n",
            "Epoch: 2, Training Loss: 2.5761847496032715\n",
            "Epoch: 2, Training Loss: 2.137396812438965\n",
            "Epoch: 2, Training Loss: 1.8594093322753906\n",
            "Epoch: 2, Training Loss: 2.0842339992523193\n",
            "Epoch: 2, Training Loss: 2.212022066116333\n",
            "Epoch: 2, Training Loss: 1.928647518157959\n",
            "Epoch: 2, Training Loss: 2.28887939453125\n",
            "Epoch: 2, Training Loss: 1.8210047483444214\n",
            "Epoch: 2, Training Loss: 2.037052869796753\n",
            "Epoch: 2, Training Loss: 2.145848512649536\n",
            "Epoch: 2, Training Loss: 2.0004425048828125\n",
            "Epoch: 2, Training Loss: 1.8720377683639526\n",
            "Epoch: 2, Training Loss: 2.0082595348358154\n",
            "Epoch: 2, Training Loss: 1.9256722927093506\n",
            "Epoch: 2, Training Loss: 2.230468988418579\n",
            "Epoch: 2, Training Loss: 2.0380048751831055\n",
            "Epoch: 2, Training Loss: 2.3388161659240723\n",
            "Epoch: 2, Training Loss: 2.2865397930145264\n",
            "Epoch: 2, Training Loss: 2.1123979091644287\n",
            "Epoch: 2, Training Loss: 2.0910730361938477\n",
            "Epoch: 2, Training Loss: 2.050297498703003\n",
            "Epoch: 2, Training Loss: 1.7338738441467285\n",
            "Epoch: 2, Training Loss: 2.0447142124176025\n",
            "Epoch: 2, Training Loss: 2.1314687728881836\n",
            "Epoch: 2, Training Loss: 2.0998294353485107\n",
            "Epoch: 2, Training Loss: 1.9420245885849\n",
            "Epoch: 2, Training Loss: 1.9107354879379272\n",
            "Epoch: 2, Training Loss: 2.010289192199707\n",
            "Epoch: 2, Training Loss: 1.858515739440918\n",
            "Epoch: 2, Training Loss: 1.8339550495147705\n",
            "Epoch: 2, Training Loss: 1.9446524381637573\n",
            "Epoch: 2, Training Loss: 2.223752498626709\n",
            "Epoch: 2, Training Loss: 1.9398932456970215\n",
            "Epoch: 2, Training Loss: 1.8568294048309326\n",
            "Epoch: 2, Training Loss: 2.1513476371765137\n",
            "Epoch: 2, Training Loss: 2.2973062992095947\n",
            "Epoch: 2, Training Loss: 2.055131435394287\n",
            "Epoch: 2, Training Loss: 2.1102488040924072\n",
            "Epoch: 2, Training Loss: 2.004612922668457\n",
            "Epoch: 2, Training Loss: 2.3044800758361816\n",
            "Epoch: 2, Training Loss: 2.0642802715301514\n",
            "Epoch: 2, Training Loss: 2.0588114261627197\n",
            "Epoch: 2, Training Loss: 2.0253679752349854\n",
            "Epoch: 2, Training Loss: 1.7431892156600952\n",
            "Epoch: 2, Training Loss: 2.0932488441467285\n",
            "Epoch: 2, Training Loss: 2.222161293029785\n",
            "Epoch: 2, Training Loss: 2.2268781661987305\n",
            "Epoch: 2, Training Loss: 1.723885416984558\n",
            "Epoch: 2, Training Loss: 2.1150660514831543\n",
            "Epoch: 2, Training Loss: 1.8472062349319458\n",
            "Epoch: 2, Training Loss: 1.8448405265808105\n",
            "Epoch: 2, Training Loss: 2.2012157440185547\n",
            "Epoch: 2, Training Loss: 2.2377336025238037\n",
            "Epoch: 2, Training Loss: 2.15879487991333\n",
            "Epoch: 2, Training Loss: 1.7922264337539673\n",
            "Epoch: 2, Training Loss: 2.208080291748047\n",
            "Epoch: 2, Training Loss: 2.09822940826416\n",
            "Epoch: 2, Training Loss: 2.0303454399108887\n",
            "Epoch: 2, Training Loss: 2.007260322570801\n",
            "Epoch: 2, Training Loss: 2.3352091312408447\n",
            "Epoch: 2, Training Loss: 1.9635037183761597\n",
            "Epoch: 2, Training Loss: 2.1883552074432373\n",
            "Epoch: 2, Training Loss: 2.1224544048309326\n",
            "Epoch: 2, Training Loss: 2.2221128940582275\n",
            "Epoch: 2, Training Loss: 2.1132442951202393\n",
            "Epoch: 2, Training Loss: 1.8339159488677979\n",
            "Epoch: 2, Training Loss: 1.9534564018249512\n",
            "Epoch: 2, Training Loss: 2.185199737548828\n",
            "Epoch: 2, Training Loss: 2.1859540939331055\n",
            "Epoch: 2, Training Loss: 1.847901463508606\n",
            "Epoch: 2, Training Loss: 2.17933988571167\n",
            "Epoch: 2, Training Loss: 2.008327007293701\n",
            "Epoch: 2, Training Loss: 1.948613166809082\n",
            "Epoch: 2, Training Loss: 1.9321739673614502\n",
            "Epoch: 2, Training Loss: 1.986729383468628\n",
            "Epoch: 2, Training Loss: 1.7646697759628296\n",
            "Epoch: 2, Training Loss: 2.1246883869171143\n",
            "Epoch: 2, Training Loss: 1.9794868230819702\n",
            "Epoch: 2, Training Loss: 1.8668091297149658\n",
            "Epoch: 2, Training Loss: 1.8585516214370728\n",
            "Epoch: 2, Training Loss: 2.2071337699890137\n",
            "Epoch: 2, Training Loss: 1.9411239624023438\n",
            "Epoch: 2, Training Loss: 1.999667763710022\n",
            "Epoch: 2, Training Loss: 2.2854819297790527\n",
            "Epoch: 2, Training Loss: 2.1023054122924805\n",
            "Epoch: 2, Training Loss: 2.2035539150238037\n",
            "Epoch: 2, Training Loss: 1.9731167554855347\n",
            "Epoch: 2, Training Loss: 2.2316322326660156\n",
            "Epoch: 2, Training Loss: 2.251950740814209\n",
            "Epoch: 2, Training Loss: 2.042865037918091\n",
            "Epoch: 2, Training Loss: 2.207455635070801\n",
            "Epoch: 2, Training Loss: 2.051527261734009\n",
            "Epoch: 2, Training Loss: 2.1219322681427\n",
            "Epoch: 2, Training Loss: 2.001833915710449\n",
            "Epoch: 2, Training Loss: 2.0631961822509766\n",
            "Epoch: 2, Training Loss: 2.2051219940185547\n",
            "Epoch: 2, Training Loss: 1.9255430698394775\n",
            "Epoch: 2, Training Loss: 2.2577950954437256\n",
            "Epoch: 2, Training Loss: 2.079542636871338\n",
            "Epoch: 2, Training Loss: 1.926671028137207\n",
            "Epoch: 2, Training Loss: 2.2249507904052734\n",
            "Epoch: 2, Training Loss: 2.1573307514190674\n",
            "Epoch: 2, Training Loss: 2.107872247695923\n",
            "Epoch: 2, Training Loss: 2.248671531677246\n",
            "Epoch: 2, Training Loss: 1.9817532300949097\n",
            "Epoch: 2, Training Loss: 1.9336491823196411\n",
            "Epoch: 2, Training Loss: 1.906072735786438\n",
            "Epoch: 2, Training Loss: 2.3257522583007812\n",
            "Epoch: 2, Training Loss: 2.140660047531128\n",
            "Epoch: 2, Training Loss: 1.955323576927185\n",
            "Epoch: 2, Training Loss: 2.0223326683044434\n",
            "Epoch: 2, Training Loss: 1.87928307056427\n",
            "Epoch: 2, Training Loss: 2.166468381881714\n",
            "Epoch: 2, Training Loss: 2.2294187545776367\n",
            "Epoch: 2, Training Loss: 2.2291226387023926\n",
            "Epoch: 2, Training Loss: 2.2700493335723877\n",
            "Epoch: 2, Training Loss: 2.1160316467285156\n",
            "Epoch: 2, Training Loss: 1.9025318622589111\n",
            "Epoch: 2, Training Loss: 1.9903736114501953\n",
            "Epoch: 2, Training Loss: 1.98789381980896\n",
            "Epoch: 2, Training Loss: 2.3866560459136963\n",
            "Epoch: 2, Training Loss: 1.9021940231323242\n",
            "Epoch: 2, Training Loss: 1.9887148141860962\n",
            "Epoch: 2, Training Loss: 2.018967628479004\n",
            "Epoch: 2, Training Loss: 2.0882341861724854\n",
            "Epoch: 2, Training Loss: 2.078524589538574\n",
            "Epoch: 2, Training Loss: 1.793535828590393\n",
            "Epoch: 2, Training Loss: 2.0041065216064453\n",
            "Epoch: 2, Training Loss: 1.838056206703186\n",
            "Epoch: 2, Training Loss: 1.8249473571777344\n",
            "Epoch: 2, Training Loss: 2.071441411972046\n",
            "Epoch: 2, Training Loss: 2.2250142097473145\n",
            "Epoch: 2, Training Loss: 2.1527278423309326\n",
            "Epoch: 2, Training Loss: 1.964984655380249\n",
            "Epoch: 2, Training Loss: 1.969360113143921\n",
            "Epoch: 2, Training Loss: 2.17000150680542\n",
            "Epoch: 2, Training Loss: 1.91077721118927\n",
            "Epoch: 2, Training Loss: 1.7445080280303955\n",
            "Epoch: 2, Training Loss: 2.3311290740966797\n",
            "Epoch: 2, Training Loss: 1.9921435117721558\n",
            "Epoch: 2, Training Loss: 1.9853951930999756\n",
            "Epoch: 2, Training Loss: 1.9742646217346191\n",
            "Epoch: 2, Training Loss: 2.0155179500579834\n",
            "Epoch: 2, Training Loss: 2.3375468254089355\n",
            "Epoch: 2, Training Loss: 2.1710219383239746\n",
            "Epoch: 2, Training Loss: 1.8711113929748535\n",
            "Epoch: 2, Training Loss: 2.0505523681640625\n",
            "Epoch: 2, Training Loss: 1.6954675912857056\n",
            "Epoch: 2, Training Loss: 2.2224462032318115\n",
            "Epoch: 2, Training Loss: 2.2288098335266113\n",
            "Epoch: 2, Training Loss: 1.8962420225143433\n",
            "Epoch: 2, Training Loss: 2.145158290863037\n",
            "Epoch: 2, Training Loss: 2.2125415802001953\n",
            "Epoch: 2, Training Loss: 1.996785283088684\n",
            "Epoch: 2, Training Loss: 2.0103302001953125\n",
            "Epoch: 2, Training Loss: 2.01241135597229\n",
            "Epoch: 2, Training Loss: 1.9526790380477905\n",
            "Epoch: 2, Training Loss: 2.1279184818267822\n",
            "Epoch: 2, Training Loss: 1.933849573135376\n",
            "Epoch: 2, Training Loss: 1.9426929950714111\n",
            "Epoch: 2, Training Loss: 1.9734389781951904\n",
            "Epoch: 2, Training Loss: 2.0653746128082275\n",
            "Epoch: 2, Training Loss: 2.0995006561279297\n",
            "Epoch: 2, Training Loss: 2.263124942779541\n",
            "Epoch: 2, Training Loss: 1.9178510904312134\n",
            "Epoch: 2, Training Loss: 1.9008036851882935\n",
            "Epoch: 2, Training Loss: 2.2070231437683105\n",
            "Epoch: 2, Training Loss: 2.15232253074646\n",
            "Epoch: 2, Training Loss: 2.022435188293457\n",
            "Epoch: 2, Training Loss: 2.137906074523926\n",
            "Epoch: 2, Training Loss: 2.05720591545105\n",
            "Epoch: 2, Training Loss: 1.9761121273040771\n",
            "Epoch: 2, Training Loss: 1.962087631225586\n",
            "Epoch: 2, Training Loss: 2.3689355850219727\n",
            "Epoch: 2, Training Loss: 2.1415860652923584\n",
            "Epoch: 2, Training Loss: 1.8719384670257568\n",
            "Epoch: 2, Training Loss: 2.0524842739105225\n",
            "Epoch: 2, Training Loss: 2.0622506141662598\n",
            "Epoch: 2, Training Loss: 2.2267370223999023\n",
            "Epoch: 2, Training Loss: 1.8082985877990723\n",
            "Epoch: 2, Training Loss: 2.188201665878296\n",
            "Epoch: 2, Training Loss: 2.0241782665252686\n",
            "Epoch: 2, Training Loss: 2.0310733318328857\n",
            "Epoch: 2, Training Loss: 1.9578027725219727\n",
            "Epoch: 2, Training Loss: 2.0827906131744385\n",
            "Epoch: 2, Training Loss: 1.9222378730773926\n",
            "Epoch: 2, Training Loss: 1.9508432149887085\n",
            "Epoch: 2, Training Loss: 2.2332167625427246\n",
            "Epoch: 2, Training Loss: 1.7899444103240967\n",
            "Epoch: 2, Training Loss: 2.2399580478668213\n",
            "Epoch: 2, Training Loss: 1.7993857860565186\n",
            "Epoch: 2, Training Loss: 2.113234043121338\n",
            "Epoch: 2, Training Loss: 1.9135377407073975\n",
            "Epoch: 2, Training Loss: 2.166004180908203\n",
            "Epoch: 2, Training Loss: 2.175062656402588\n",
            "Epoch: 2, Training Loss: 1.832716941833496\n",
            "Epoch: 2, Training Loss: 2.063424587249756\n",
            "Epoch: 2, Training Loss: 1.9821370840072632\n",
            "Epoch: 2, Training Loss: 1.9567196369171143\n",
            "Epoch: 2, Training Loss: 1.9789844751358032\n",
            "Epoch: 2, Training Loss: 1.7998549938201904\n",
            "Epoch: 2, Training Loss: 1.9874845743179321\n",
            "Epoch: 2, Training Loss: 2.0665135383605957\n",
            "Epoch: 2, Training Loss: 2.0257599353790283\n",
            "Epoch: 2, Training Loss: 2.0909361839294434\n",
            "Epoch: 2, Training Loss: 1.9989299774169922\n",
            "Epoch: 2, Training Loss: 2.0505478382110596\n",
            "Epoch: 2, Training Loss: 1.9834463596343994\n",
            "Epoch: 2, Training Loss: 2.0635459423065186\n",
            "Epoch: 2, Training Loss: 2.149528980255127\n",
            "Epoch: 2, Training Loss: 1.9973520040512085\n",
            "Epoch: 2, Training Loss: 2.0875158309936523\n",
            "Epoch: 2, Training Loss: 1.8931270837783813\n",
            "Epoch: 2, Training Loss: 2.052741289138794\n",
            "Epoch: 2, Training Loss: 2.0759477615356445\n",
            "Epoch: 2, Training Loss: 2.268221139907837\n",
            "Epoch: 2, Training Loss: 2.1419808864593506\n",
            "Epoch: 2, Training Loss: 2.0849697589874268\n",
            "Epoch: 2, Training Loss: 1.9292881488800049\n",
            "Epoch: 2, Training Loss: 1.8143354654312134\n",
            "Epoch: 2, Training Loss: 2.0980398654937744\n",
            "Epoch: 2, Training Loss: 2.0662503242492676\n",
            "Epoch: 2, Training Loss: 2.274705410003662\n",
            "Epoch: 2, Training Loss: 1.637017846107483\n",
            "Epoch: 2, Training Loss: 1.8143430948257446\n",
            "Epoch: 2, Training Loss: 2.0932869911193848\n",
            "Epoch: 2, Training Loss: 1.8477524518966675\n",
            "Epoch: 2, Training Loss: 2.0158207416534424\n",
            "Epoch: 2, Training Loss: 1.900832176208496\n",
            "Epoch: 2, Training Loss: 1.8976993560791016\n",
            "Epoch: 2, Training Loss: 2.1339027881622314\n",
            "Epoch: 2, Training Loss: 1.9315217733383179\n",
            "Epoch: 2, Training Loss: 1.7043509483337402\n",
            "Epoch: 2, Training Loss: 1.9870790243148804\n",
            "Epoch: 2, Training Loss: 1.7432751655578613\n",
            "Epoch: 2, Training Loss: 2.0281927585601807\n",
            "Epoch: 2, Training Loss: 1.8825312852859497\n",
            "Epoch: 2, Training Loss: 2.0140891075134277\n",
            "Epoch: 2, Training Loss: 1.9058265686035156\n",
            "Epoch: 2, Training Loss: 1.995649814605713\n",
            "Epoch: 2, Training Loss: 2.1930606365203857\n",
            "Epoch: 2, Training Loss: 2.0307040214538574\n",
            "Epoch: 2, Training Loss: 2.072025775909424\n",
            "Epoch: 2, Training Loss: 1.8960692882537842\n",
            "Epoch: 2, Training Loss: 1.9905033111572266\n",
            "Epoch: 2, Training Loss: 1.8557924032211304\n",
            "Epoch: 2, Training Loss: 2.0274806022644043\n",
            "Epoch: 2, Training Loss: 2.158115863800049\n",
            "Epoch: 2, Training Loss: 2.123894453048706\n",
            "Epoch: 2, Training Loss: 2.228149175643921\n",
            "Epoch: 2, Training Loss: 2.2332921028137207\n",
            "Epoch: 2, Training Loss: 2.0661306381225586\n",
            "Epoch: 2, Training Loss: 1.8805575370788574\n",
            "Epoch: 2, Training Loss: 1.9984476566314697\n",
            "Epoch: 2, Training Loss: 2.2369091510772705\n",
            "Epoch: 2, Training Loss: 2.0564539432525635\n",
            "Epoch: 2, Training Loss: 1.9945889711380005\n",
            "Epoch: 2, Training Loss: 1.98727285861969\n",
            "Epoch: 2, Training Loss: 1.9796967506408691\n",
            "Epoch: 2, Training Loss: 1.881170630455017\n",
            "Epoch: 2, Training Loss: 2.285242795944214\n",
            "Epoch: 2, Training Loss: 1.970108151435852\n",
            "Epoch: 2, Training Loss: 2.204706907272339\n",
            "Epoch: 2, Training Loss: 2.007707357406616\n",
            "Epoch: 2, Training Loss: 1.8327345848083496\n",
            "Epoch: 2, Training Loss: 1.9110108613967896\n",
            "Epoch: 2, Training Loss: 2.1672415733337402\n",
            "Epoch: 2, Training Loss: 2.0363359451293945\n",
            "Epoch: 2, Training Loss: 2.047544240951538\n",
            "Epoch: 2, Training Loss: 2.348773956298828\n",
            "Epoch: 2, Training Loss: 2.0533134937286377\n",
            "Epoch: 2, Training Loss: 1.8232319355010986\n",
            "Epoch: 2, Training Loss: 1.8923879861831665\n",
            "Epoch: 2, Training Loss: 2.0452919006347656\n",
            "Epoch: 2, Training Loss: 2.0907089710235596\n",
            "Epoch: 2, Training Loss: 2.1178340911865234\n",
            "Epoch: 2, Training Loss: 2.009075403213501\n",
            "Epoch: 2, Training Loss: 2.0335373878479004\n",
            "Epoch: 2, Training Loss: 2.3253941535949707\n",
            "Epoch: 2, Training Loss: 2.0478265285491943\n",
            "Epoch: 2, Training Loss: 2.0075607299804688\n",
            "Epoch: 2, Training Loss: 2.218087911605835\n",
            "Epoch: 2, Training Loss: 1.7543326616287231\n",
            "Epoch: 2, Training Loss: 2.0204248428344727\n",
            "Epoch: 2, Training Loss: 1.8255759477615356\n",
            "Epoch: 2, Training Loss: 2.0783486366271973\n",
            "Epoch: 2, Training Loss: 2.2623910903930664\n",
            "Epoch: 2, Training Loss: 1.9216821193695068\n",
            "Epoch: 2, Training Loss: 1.8745830059051514\n",
            "Epoch: 2, Training Loss: 1.9933844804763794\n",
            "Epoch: 2, Training Loss: 2.1974430084228516\n",
            "Epoch: 2, Training Loss: 2.012410879135132\n",
            "Epoch: 2, Training Loss: 2.1309423446655273\n",
            "Epoch: 2, Training Loss: 1.909053921699524\n",
            "Epoch: 2, Training Loss: 2.071277141571045\n",
            "Epoch: 2, Training Loss: 1.8543373346328735\n",
            "Epoch: 2, Training Loss: 1.9471912384033203\n",
            "Epoch: 2, Training Loss: 1.8472599983215332\n",
            "Epoch: 2, Training Loss: 1.9135205745697021\n",
            "Epoch: 2, Training Loss: 2.1773295402526855\n",
            "Epoch: 2, Training Loss: 2.078411102294922\n",
            "Epoch: 2, Training Loss: 2.2858381271362305\n",
            "Epoch: 2, Training Loss: 2.0668423175811768\n",
            "Epoch: 2, Training Loss: 1.9113984107971191\n",
            "Epoch: 2, Training Loss: 1.9051544666290283\n",
            "Epoch: 2, Training Loss: 2.0192079544067383\n",
            "Epoch: 2, Training Loss: 2.1832332611083984\n",
            "Epoch: 2, Training Loss: 2.1193182468414307\n",
            "Epoch: 2, Training Loss: 1.880187749862671\n",
            "Epoch: 2, Training Loss: 2.159555673599243\n",
            "Epoch: 2, Training Loss: 2.0289130210876465\n",
            "Epoch: 2, Training Loss: 2.0110716819763184\n",
            "Epoch: 2, Training Loss: 2.117100477218628\n",
            "Epoch: 2, Training Loss: 1.8428080081939697\n",
            "Epoch: 2, Training Loss: 2.0005154609680176\n",
            "Epoch: 2, Training Loss: 1.6282422542572021\n",
            "Epoch: 2, Training Loss: 1.8766123056411743\n",
            "Epoch: 2, Training Loss: 1.9264832735061646\n",
            "Epoch: 2, Training Loss: 2.0108706951141357\n",
            "Epoch: 2, Training Loss: 1.9385368824005127\n",
            "Epoch: 2, Training Loss: 2.1370391845703125\n",
            "Epoch: 2, Training Loss: 2.2481908798217773\n",
            "Epoch: 2, Training Loss: 1.8374464511871338\n",
            "Epoch: 2, Training Loss: 1.956697940826416\n",
            "Epoch: 2, Training Loss: 2.020920991897583\n",
            "Epoch: 2, Training Loss: 1.8705809116363525\n",
            "Epoch: 2, Training Loss: 1.9386775493621826\n",
            "Epoch: 2, Training Loss: 1.854853868484497\n",
            "Epoch: 2, Training Loss: 2.0429372787475586\n",
            "Epoch: 2, Training Loss: 2.1127288341522217\n",
            "Epoch: 2, Training Loss: 1.9184043407440186\n",
            "Epoch: 2, Training Loss: 1.9715909957885742\n",
            "Epoch: 2, Training Loss: 1.8151217699050903\n",
            "Epoch: 2, Training Loss: 1.9174200296401978\n",
            "Epoch: 2, Training Loss: 2.2132365703582764\n",
            "Epoch: 2, Training Loss: 2.000396728515625\n",
            "Epoch: 2, Training Loss: 2.0869977474212646\n",
            "Epoch: 2, Training Loss: 2.0374982357025146\n",
            "Epoch: 2, Training Loss: 2.080254554748535\n",
            "Epoch: 2, Training Loss: 1.8397877216339111\n",
            "Epoch: 2, Training Loss: 2.0635204315185547\n",
            "Epoch: 2, Training Loss: 2.0849387645721436\n",
            "Epoch: 2, Training Loss: 1.8759305477142334\n",
            "Epoch: 2, Training Loss: 2.1054065227508545\n",
            "Epoch: 2, Training Loss: 2.27935528755188\n",
            "Epoch: 2, Training Loss: 1.7956680059432983\n",
            "Epoch: 2, Training Loss: 2.0052051544189453\n",
            "Epoch: 2, Training Loss: 2.180619478225708\n",
            "Epoch: 2, Training Loss: 1.9041802883148193\n",
            "Epoch: 2, Training Loss: 1.9892793893814087\n",
            "Epoch: 2, Training Loss: 1.6537779569625854\n",
            "Epoch: 2, Training Loss: 2.1384902000427246\n",
            "Epoch: 2, Training Loss: 2.0588059425354004\n",
            "Epoch: 2, Training Loss: 1.8776694536209106\n",
            "Epoch: 2, Training Loss: 2.0069727897644043\n",
            "Epoch: 2, Training Loss: 2.073432683944702\n",
            "Epoch: 2, Training Loss: 1.8489832878112793\n",
            "Epoch: 2, Training Loss: 2.103464365005493\n",
            "Epoch: 2, Training Loss: 2.2695116996765137\n",
            "Epoch: 2, Training Loss: 2.0461883544921875\n",
            "Epoch: 2, Training Loss: 2.0777828693389893\n",
            "Epoch: 2, Training Loss: 1.8697891235351562\n",
            "Epoch: 2, Training Loss: 2.111017942428589\n",
            "Epoch: 2, Training Loss: 1.9292820692062378\n",
            "Epoch: 2, Training Loss: 1.9804974794387817\n",
            "Epoch: 2, Training Loss: 1.8896855115890503\n",
            "Epoch: 2, Training Loss: 2.243114471435547\n",
            "Epoch: 2, Training Loss: 1.8654550313949585\n",
            "Epoch: 2, Training Loss: 2.0288493633270264\n",
            "Epoch: 2, Training Loss: 2.10762357711792\n",
            "Epoch: 2, Training Loss: 1.9873422384262085\n",
            "Epoch: 2, Training Loss: 2.0277867317199707\n",
            "Epoch: 2, Training Loss: 1.9925700426101685\n",
            "Epoch: 2, Training Loss: 2.1640515327453613\n",
            "Epoch: 2, Training Loss: 1.7221505641937256\n",
            "Epoch: 2, Training Loss: 2.244152307510376\n",
            "Epoch: 2, Training Loss: 2.108621835708618\n",
            "Epoch: 2, Training Loss: 1.8719574213027954\n",
            "Epoch: 2, Training Loss: 2.0464634895324707\n",
            "Epoch: 2, Training Loss: 1.7783138751983643\n",
            "Epoch: 2, Training Loss: 1.9470345973968506\n",
            "Epoch: 2, Training Loss: 2.313591718673706\n",
            "Epoch: 2, Training Loss: 2.408414363861084\n",
            "Epoch: 2, Training Loss: 1.8635728359222412\n",
            "Epoch: 2, Training Loss: 1.9657480716705322\n",
            "Epoch: 2, Training Loss: 1.7234618663787842\n",
            "Epoch: 2, Training Loss: 1.8895570039749146\n",
            "Epoch: 2, Training Loss: 2.050560474395752\n",
            "Epoch: 2, Training Loss: 1.9112449884414673\n",
            "Epoch: 2, Training Loss: 2.083427906036377\n",
            "Epoch: 2, Training Loss: 2.1128311157226562\n",
            "Epoch: 2, Training Loss: 2.0568301677703857\n",
            "Epoch: 2, Training Loss: 1.7510933876037598\n",
            "Epoch: 2, Training Loss: 2.0118579864501953\n",
            "Epoch: 2, Training Loss: 1.957968831062317\n",
            "Epoch: 2, Training Loss: 1.8534142971038818\n",
            "Epoch: 2, Training Loss: 2.000159502029419\n",
            "Epoch: 2, Training Loss: 1.8781719207763672\n",
            "Epoch: 2, Training Loss: 1.9275166988372803\n",
            "Epoch: 2, Training Loss: 1.8191553354263306\n",
            "Epoch: 2, Training Loss: 1.900939702987671\n",
            "Epoch: 2, Training Loss: 2.1119542121887207\n",
            "Epoch: 2, Training Loss: 2.2066752910614014\n",
            "Epoch: 2, Training Loss: 1.8499141931533813\n",
            "Epoch: 2, Training Loss: 1.9778811931610107\n",
            "Epoch: 2, Training Loss: 1.6287577152252197\n",
            "Epoch: 2, Training Loss: 1.7406281232833862\n",
            "Epoch: 2, Training Loss: 1.8577885627746582\n",
            "Epoch: 2, Training Loss: 1.9821501970291138\n",
            "Epoch: 2, Training Loss: 1.845885992050171\n",
            "Epoch: 2, Training Loss: 1.9820725917816162\n",
            "Epoch: 2, Training Loss: 1.8304493427276611\n",
            "Epoch: 2, Training Loss: 2.1194798946380615\n",
            "Epoch: 2, Training Loss: 1.750161051750183\n",
            "Epoch: 2, Training Loss: 1.8745436668395996\n",
            "Epoch: 2, Training Loss: 2.019484281539917\n",
            "Epoch: 2, Training Loss: 1.975585699081421\n",
            "Epoch: 2, Training Loss: 2.169978380203247\n",
            "Epoch: 2, Training Loss: 2.0751123428344727\n",
            "Epoch: 2, Training Loss: 2.01743483543396\n",
            "Epoch: 2, Training Loss: 1.9682470560073853\n",
            "Epoch: 2, Training Loss: 1.9554064273834229\n",
            "Epoch: 2, Training Loss: 2.0843029022216797\n",
            "Epoch: 2, Training Loss: 1.6810604333877563\n",
            "Epoch: 2, Training Loss: 1.7826203107833862\n",
            "Epoch: 2, Training Loss: 1.9959827661514282\n",
            "Epoch: 2, Training Loss: 2.1774423122406006\n",
            "Epoch: 2, Training Loss: 2.143022298812866\n",
            "Epoch: 2, Training Loss: 1.8777551651000977\n",
            "Epoch: 2, Training Loss: 1.6962144374847412\n",
            "Epoch: 2, Training Loss: 1.860127568244934\n",
            "Epoch: 2, Training Loss: 2.128051996231079\n",
            "Epoch: 2, Training Loss: 1.975275993347168\n",
            "Epoch: 2, Training Loss: 2.0454068183898926\n",
            "Epoch: 2, Training Loss: 2.0465269088745117\n",
            "Epoch: 2, Training Loss: 1.7041338682174683\n",
            "Epoch: 2, Training Loss: 1.891912579536438\n",
            "Epoch: 2, Training Loss: 2.3377785682678223\n",
            "Epoch: 2, Training Loss: 1.691476821899414\n",
            "Epoch: 2, Training Loss: 2.124603271484375\n",
            "Epoch: 2, Training Loss: 1.790112018585205\n",
            "Epoch: 2, Training Loss: 2.0390734672546387\n",
            "Epoch: 2, Training Loss: 1.617570400238037\n",
            "Epoch: 2, Training Loss: 2.2483997344970703\n",
            "Epoch: 2, Training Loss: 2.0788609981536865\n",
            "Epoch: 2, Training Loss: 1.5581034421920776\n",
            "Epoch: 2, Training Loss: 1.861836314201355\n",
            "Epoch: 2, Training Loss: 1.7636443376541138\n",
            "Epoch: 2, Training Loss: 1.9962623119354248\n",
            "Epoch: 2, Training Loss: 1.8554543256759644\n",
            "Epoch: 2, Training Loss: 2.0514190196990967\n",
            "Epoch: 2, Training Loss: 2.1003024578094482\n",
            "Epoch: 2, Training Loss: 1.952771782875061\n",
            "Epoch: 2, Training Loss: 1.9220010042190552\n",
            "Epoch: 2, Training Loss: 1.8998502492904663\n",
            "Epoch: 2, Training Loss: 1.9576935768127441\n",
            "Epoch: 2, Training Loss: 1.842568278312683\n",
            "Epoch: 2, Training Loss: 2.1414830684661865\n",
            "Epoch: 2, Training Loss: 1.8929266929626465\n",
            "Epoch: 2, Training Loss: 1.869367003440857\n",
            "Epoch: 2, Training Loss: 2.2114064693450928\n",
            "Epoch: 2, Training Loss: 1.9954051971435547\n",
            "Epoch: 2, Training Loss: 1.97854745388031\n",
            "Epoch: 2, Training Loss: 1.940361499786377\n",
            "Epoch: 2, Training Loss: 2.0558581352233887\n",
            "Epoch: 2, Training Loss: 2.0716745853424072\n",
            "Epoch: 2, Training Loss: 2.178330183029175\n",
            "Epoch: 2, Training Loss: 1.798469066619873\n",
            "Epoch: 2, Training Loss: 2.062246561050415\n",
            "Epoch: 2, Training Loss: 2.245634078979492\n",
            "Epoch: 2, Training Loss: 1.7507357597351074\n",
            "Epoch: 2, Training Loss: 2.087259531021118\n",
            "Epoch: 2, Training Loss: 1.9052770137786865\n",
            "Epoch: 2, Training Loss: 1.709445834159851\n",
            "Epoch: 2, Training Loss: 1.5410622358322144\n",
            "Epoch: 2, Training Loss: 1.701068639755249\n",
            "Epoch: 2, Training Loss: 1.6803985834121704\n",
            "Epoch: 2, Training Loss: 1.9206115007400513\n",
            "Epoch: 2, Training Loss: 1.9715410470962524\n",
            "Epoch: 2, Training Loss: 2.1342613697052\n",
            "Epoch: 2, Training Loss: 1.9511080980300903\n",
            "Epoch: 2, Training Loss: 2.050226926803589\n",
            "Epoch: 2, Training Loss: 1.7150816917419434\n",
            "Epoch: 2, Training Loss: 2.054135322570801\n",
            "Epoch: 2, Training Loss: 1.8412773609161377\n",
            "Epoch: 2, Training Loss: 1.9664944410324097\n",
            "Epoch: 2, Training Loss: 1.8383359909057617\n",
            "Epoch: 2, Training Loss: 1.9692376852035522\n",
            "Epoch: 2, Training Loss: 1.883672833442688\n",
            "Epoch: 2, Training Loss: 1.7644731998443604\n",
            "Epoch: 2, Training Loss: 1.866562008857727\n",
            "Epoch: 2, Training Loss: 1.870224952697754\n",
            "Epoch: 2, Training Loss: 2.131812572479248\n",
            "Epoch: 2, Training Loss: 1.8969806432724\n",
            "Epoch: 2, Training Loss: 2.0209455490112305\n",
            "Epoch: 2, Training Loss: 1.8481415510177612\n",
            "Epoch: 2, Training Loss: 1.6720712184906006\n",
            "Epoch: 2, Training Loss: 1.87733793258667\n",
            "Epoch: 2, Training Loss: 2.184824228286743\n",
            "Epoch: 2, Training Loss: 2.1892974376678467\n",
            "Epoch: 2, Training Loss: 1.7372874021530151\n",
            "Epoch: 2, Training Loss: 1.960960865020752\n",
            "Epoch: 2, Training Loss: 2.1776630878448486\n",
            "Epoch: 2, Training Loss: 1.8735697269439697\n",
            "Epoch: 2, Training Loss: 2.183122158050537\n",
            "Epoch: 2, Training Loss: 2.1526260375976562\n",
            "Epoch: 2, Training Loss: 1.8025482892990112\n",
            "Epoch: 2, Training Loss: 2.244858980178833\n",
            "Epoch: 2, Training Loss: 1.6837563514709473\n",
            "Epoch: 2, Training Loss: 2.1425857543945312\n",
            "Epoch: 2, Training Loss: 1.7411859035491943\n",
            "Epoch: 2, Training Loss: 1.7713993787765503\n",
            "Epoch: 2, Training Loss: 1.7907483577728271\n",
            "Epoch: 2, Training Loss: 1.9708878993988037\n",
            "Epoch: 2, Training Loss: 2.054396390914917\n",
            "Epoch: 2, Training Loss: 1.7254443168640137\n",
            "Epoch: 2, Training Loss: 1.8384418487548828\n",
            "Epoch: 2, Training Loss: 1.7976423501968384\n",
            "Epoch: 2, Training Loss: 1.8959152698516846\n",
            "Epoch: 2, Training Loss: 2.2861013412475586\n",
            "Epoch: 2, Training Loss: 1.820573329925537\n",
            "Epoch: 2, Training Loss: 2.1345531940460205\n",
            "Epoch: 2, Training Loss: 1.7569671869277954\n",
            "Epoch: 2, Training Loss: 2.200533628463745\n",
            "Epoch: 2, Training Loss: 1.7741987705230713\n",
            "Epoch: 2, Training Loss: 1.7463719844818115\n",
            "Epoch: 2, Training Loss: 2.238647937774658\n",
            "Epoch: 2, Training Loss: 1.9178823232650757\n",
            "Epoch: 2, Training Loss: 2.1372525691986084\n",
            "Epoch: 2, Training Loss: 1.8013418912887573\n",
            "Epoch: 2, Training Loss: 2.177931070327759\n",
            "Epoch: 2, Training Loss: 1.9955246448516846\n",
            "Epoch: 2, Training Loss: 1.9359958171844482\n",
            "Epoch: 2, Training Loss: 1.8186697959899902\n",
            "Epoch: 2, Training Loss: 1.9866324663162231\n",
            "Epoch: 2, Training Loss: 1.769543170928955\n",
            "Epoch: 2, Training Loss: 2.1104371547698975\n",
            "Epoch: 2, Training Loss: 1.7901829481124878\n",
            "Epoch: 2, Training Loss: 1.905626654624939\n",
            "Epoch: 2, Training Loss: 2.1100430488586426\n",
            "Epoch: 2, Training Loss: 2.144749879837036\n",
            "Epoch: 2, Training Loss: 1.9560078382492065\n",
            "Epoch: 2, Training Loss: 1.7664110660552979\n",
            "Epoch: 2, Training Loss: 1.8007018566131592\n",
            "Epoch: 2, Training Loss: 2.109400749206543\n",
            "Epoch: 2, Training Loss: 1.959797739982605\n",
            "Epoch: 2, Training Loss: 1.9205646514892578\n",
            "Epoch: 2, Training Loss: 1.922182559967041\n",
            "Epoch: 2, Training Loss: 2.046269178390503\n",
            "Epoch: 2, Training Loss: 2.192620277404785\n",
            "Epoch: 2, Training Loss: 1.9050443172454834\n",
            "Epoch: 2, Training Loss: 1.9081425666809082\n",
            "Epoch: 2, Training Loss: 1.985445261001587\n",
            "Epoch: 2, Training Loss: 1.8888009786605835\n",
            "Epoch: 2, Training Loss: 1.7327046394348145\n",
            "Epoch: 2, Training Loss: 2.1892895698547363\n",
            "Epoch: 2, Training Loss: 1.9714841842651367\n",
            "Epoch: 2, Training Loss: 1.8907747268676758\n",
            "Epoch: 2, Training Loss: 1.8850195407867432\n",
            "Epoch: 2, Training Loss: 1.6891871690750122\n",
            "Epoch: 2, Training Loss: 1.9895609617233276\n",
            "Epoch: 2, Training Loss: 1.9882639646530151\n",
            "Epoch: 2, Training Loss: 2.2101190090179443\n",
            "Epoch: 2, Training Loss: 1.853573203086853\n",
            "Epoch: 2, Training Loss: 2.0065078735351562\n",
            "Epoch: 2, Training Loss: 1.9699392318725586\n",
            "Epoch: 2, Training Loss: 1.796616792678833\n",
            "Epoch: 2, Training Loss: 2.011890172958374\n",
            "Epoch: 2, Training Loss: 1.938970923423767\n",
            "Epoch: 2, Training Loss: 2.0054590702056885\n",
            "Epoch: 2, Training Loss: 1.9881610870361328\n",
            "Epoch: 2, Training Loss: 1.7672446966171265\n",
            "Epoch: 2, Training Loss: 1.896691083908081\n",
            "Epoch: 2, Training Loss: 1.7741432189941406\n",
            "Epoch: 2, Training Loss: 1.9027711153030396\n",
            "Epoch: 2, Training Loss: 1.7800135612487793\n",
            "Epoch: 2, Training Loss: 2.0255467891693115\n",
            "Epoch: 2, Training Loss: 1.8176772594451904\n",
            "Epoch: 2, Training Loss: 1.8156864643096924\n",
            "Epoch: 2, Training Loss: 1.808379888534546\n",
            "Epoch: 2, Training Loss: 2.040341377258301\n",
            "Epoch: 2, Training Loss: 2.0160257816314697\n",
            "Epoch: 2, Training Loss: 2.0827200412750244\n",
            "Epoch: 2, Training Loss: 1.7694880962371826\n",
            "Epoch: 2, Training Loss: 1.9440333843231201\n",
            "Epoch: 2, Training Loss: 1.8132554292678833\n",
            "Epoch: 2, Training Loss: 2.125683546066284\n",
            "Epoch: 2, Training Loss: 1.9695322513580322\n",
            "Epoch: 2, Training Loss: 2.1215145587921143\n",
            "Epoch: 2, Training Loss: 2.0424001216888428\n",
            "Epoch: 2, Training Loss: 1.9428355693817139\n",
            "Epoch: 2, Training Loss: 1.8390144109725952\n",
            "Epoch: 2, Training Loss: 1.8663146495819092\n",
            "Epoch: 2, Training Loss: 1.9283207654953003\n",
            "Epoch: 2, Training Loss: 1.995732307434082\n",
            "Epoch: 2, Training Loss: 1.8508845567703247\n",
            "Epoch: 2, Training Loss: 1.8605600595474243\n",
            "Epoch: 2, Training Loss: 1.996387004852295\n",
            "Epoch: 2, Training Loss: 1.8900030851364136\n",
            "Epoch: 2, Training Loss: 1.89808988571167\n",
            "Epoch: 2, Training Loss: 2.0147347450256348\n",
            "Epoch: 2, Training Loss: 1.9261620044708252\n",
            "Epoch: 2, Training Loss: 1.9500706195831299\n",
            "Epoch: 2, Training Loss: 2.108447313308716\n",
            "Epoch: 2, Training Loss: 1.9303443431854248\n",
            "Epoch: 2, Training Loss: 1.9920601844787598\n",
            "Epoch: 2, Training Loss: 2.0559322834014893\n",
            "Epoch: 2, Training Loss: 1.9812337160110474\n",
            "Epoch: 2, Training Loss: 1.695217490196228\n",
            "Epoch: 2, Training Loss: 1.8214025497436523\n",
            "Epoch: 2, Training Loss: 1.8641587495803833\n",
            "Epoch: 2, Training Loss: 1.7707687616348267\n",
            "Epoch: 2, Training Loss: 1.8041551113128662\n",
            "Epoch: 2, Training Loss: 1.7953381538391113\n",
            "Epoch: 2, Training Loss: 2.0787477493286133\n",
            "Epoch: 2, Training Loss: 1.831304907798767\n",
            "Epoch: 2, Training Loss: 1.7929636240005493\n",
            "Epoch: 2, Training Loss: 1.9477297067642212\n",
            "Epoch: 2, Training Loss: 2.1324198246002197\n",
            "Epoch: 2, Training Loss: 1.887376070022583\n",
            "Epoch: 2, Training Loss: 1.8322534561157227\n",
            "Epoch: 2, Training Loss: 1.7582803964614868\n",
            "Epoch: 2, Training Loss: 1.8720964193344116\n",
            "Epoch: 2, Training Loss: 2.119645833969116\n",
            "Epoch: 2, Training Loss: 1.8410227298736572\n",
            "Epoch: 2, Training Loss: 1.6163969039916992\n",
            "Epoch: 2, Training Loss: 1.9292702674865723\n",
            "Epoch: 2, Training Loss: 1.8730664253234863\n",
            "Epoch: 2, Training Loss: 1.978327751159668\n",
            "Epoch: 2, Training Loss: 1.8932489156723022\n",
            "Epoch: 2, Training Loss: 1.8676918745040894\n",
            "Epoch: 2, Training Loss: 1.8241853713989258\n",
            "Epoch: 2, Training Loss: 1.8539327383041382\n",
            "Epoch: 2, Training Loss: 2.086378335952759\n",
            "Epoch: 2, Training Loss: 1.9214730262756348\n",
            "Epoch: 2, Training Loss: 1.8348731994628906\n",
            "Epoch: 2, Training Loss: 2.1347970962524414\n",
            "Epoch: 2, Training Loss: 1.6953580379486084\n",
            "Epoch: 2, Training Loss: 2.011143684387207\n",
            "Epoch: 2, Training Loss: 1.8602294921875\n",
            "Epoch: 2, Training Loss: 1.7850974798202515\n",
            "Epoch: 2, Training Loss: 2.081494092941284\n",
            "Epoch: 2, Training Loss: 1.8400269746780396\n",
            "Epoch: 2, Training Loss: 1.6826777458190918\n",
            "Epoch: 2, Training Loss: 1.783887505531311\n",
            "Epoch: 2, Training Loss: 1.697898507118225\n",
            "Epoch: 2, Training Loss: 1.6712636947631836\n",
            "Epoch: 2, Training Loss: 1.6927152872085571\n",
            "Epoch: 2, Training Loss: 1.833639144897461\n",
            "Epoch: 2, Training Loss: 1.7679989337921143\n",
            "Epoch: 2, Training Loss: 2.1919875144958496\n",
            "Epoch: 2, Training Loss: 2.003845691680908\n",
            "Epoch: 2, Training Loss: 2.127394676208496\n",
            "Epoch: 2, Training Loss: 2.1110763549804688\n",
            "Epoch: 2, Training Loss: 2.0941717624664307\n",
            "Epoch: 2, Training Loss: 1.8952406644821167\n",
            "Epoch: 2, Training Loss: 1.9694782495498657\n",
            "Epoch: 2, Training Loss: 1.8290653228759766\n",
            "Epoch: 2, Training Loss: 1.9204992055892944\n",
            "Epoch: 2, Training Loss: 1.8879125118255615\n",
            "Epoch: 2, Training Loss: 1.7910971641540527\n",
            "Epoch: 2, Training Loss: 1.9864628314971924\n",
            "Epoch: 2, Training Loss: 1.825581431388855\n",
            "Epoch: 2, Training Loss: 1.8779983520507812\n",
            "Epoch: 2, Training Loss: 1.7432435750961304\n",
            "Epoch: 2, Training Loss: 2.0689287185668945\n",
            "Epoch: 2, Training Loss: 1.9385144710540771\n",
            "Epoch: 2, Training Loss: 1.8572665452957153\n",
            "Epoch: 2, Training Loss: 1.9727104902267456\n",
            "Epoch: 2, Training Loss: 1.9467203617095947\n",
            "Epoch: 2, Training Loss: 1.8579930067062378\n",
            "Epoch: 2, Training Loss: 1.9945223331451416\n",
            "Epoch: 2, Training Loss: 2.042313575744629\n",
            "Epoch: 2, Training Loss: 2.003197193145752\n",
            "Epoch: 2, Training Loss: 1.8958091735839844\n",
            "Epoch: 2, Training Loss: 2.076781988143921\n",
            "Epoch: 2, Training Loss: 2.019497871398926\n",
            "Epoch: 2, Training Loss: 1.8919377326965332\n",
            "Epoch: 2, Training Loss: 1.7985342741012573\n",
            "Epoch: 2, Training Loss: 1.7414919137954712\n",
            "Epoch: 2, Training Loss: 1.742213487625122\n",
            "Epoch: 2, Training Loss: 1.8859175443649292\n",
            "Epoch: 2, Training Loss: 2.2583980560302734\n",
            "Epoch: 2, Training Loss: 1.9804190397262573\n",
            "Epoch: 2, Training Loss: 1.8522015810012817\n",
            "Epoch: 2, Training Loss: 1.808809518814087\n",
            "Epoch: 2, Training Loss: 2.014387369155884\n",
            "Epoch: 2, Training Loss: 1.7371646165847778\n",
            "Epoch: 2, Training Loss: 2.185138702392578\n",
            "Epoch: 2, Training Loss: 1.9916423559188843\n",
            "Epoch: 2, Training Loss: 1.939175009727478\n",
            "Epoch: 2, Training Loss: 1.9566680192947388\n",
            "Epoch: 2, Training Loss: 1.8827378749847412\n",
            "Epoch: 2, Training Loss: 1.8222861289978027\n",
            "Epoch: 2, Training Loss: 1.8155858516693115\n",
            "Epoch: 2, Training Loss: 2.067664384841919\n",
            "Epoch: 2, Training Loss: 1.6149388551712036\n",
            "Epoch: 2, Training Loss: 1.859079122543335\n",
            "Epoch: 2, Training Loss: 1.9378068447113037\n",
            "Epoch: 2, Training Loss: 1.832313895225525\n",
            "Epoch: 2, Training Loss: 1.8853243589401245\n",
            "Epoch: 2, Training Loss: 1.9065948724746704\n",
            "Epoch: 2, Training Loss: 2.0903995037078857\n",
            "Epoch: 2, Training Loss: 1.7961735725402832\n",
            "Epoch: 2, Training Loss: 1.8009309768676758\n",
            "Epoch: 2, Training Loss: 2.156202554702759\n",
            "Epoch: 2, Training Loss: 2.1654417514801025\n",
            "Epoch: 2, Training Loss: 1.86210298538208\n",
            "Epoch: 2, Training Loss: 1.6692866086959839\n",
            "Epoch: 2, Training Loss: 2.073014259338379\n",
            "Epoch: 2, Training Loss: 2.105175256729126\n",
            "Epoch: 2, Training Loss: 1.828503966331482\n",
            "Epoch: 2, Training Loss: 1.9066996574401855\n",
            "Epoch: 2, Training Loss: 1.814594030380249\n",
            "Epoch: 2, Training Loss: 1.78351891040802\n",
            "Epoch: 2, Training Loss: 1.7374399900436401\n",
            "Epoch: 2, Training Loss: 1.7333667278289795\n",
            "Epoch: 2, Training Loss: 1.9427639245986938\n",
            "Epoch: 2, Training Loss: 1.8749961853027344\n",
            "Epoch: 2, Training Loss: 1.8733844757080078\n",
            "Epoch: 2, Training Loss: 1.9180046319961548\n",
            "Epoch: 2, Training Loss: 1.8071743249893188\n",
            "Epoch: 2, Training Loss: 1.7197692394256592\n",
            "Epoch: 2, Training Loss: 2.0141780376434326\n",
            "Epoch: 2, Training Loss: 1.919055461883545\n",
            "Epoch: 2, Training Loss: 1.7220655679702759\n",
            "Epoch: 2, Training Loss: 2.018932580947876\n",
            "Epoch: 2, Training Loss: 1.913529872894287\n",
            "Epoch: 2, Training Loss: 1.6726553440093994\n",
            "Epoch: 2, Training Loss: 2.027662754058838\n",
            "Epoch: 2, Training Loss: 1.8424943685531616\n",
            "Epoch: 2, Training Loss: 1.7724868059158325\n",
            "Epoch: 2, Training Loss: 1.8740510940551758\n",
            "Epoch: 2, Training Loss: 1.7423582077026367\n",
            "Epoch: 2, Training Loss: 2.015307903289795\n",
            "Epoch: 2, Training Loss: 1.7505766153335571\n",
            "Epoch: 2, Training Loss: 2.0174105167388916\n",
            "Epoch: 2, Training Loss: 1.7046458721160889\n",
            "Epoch: 2, Training Loss: 1.702526330947876\n",
            "Epoch: 2, Training Loss: 1.7716935873031616\n",
            "Epoch: 2, Training Loss: 1.8584190607070923\n",
            "Epoch: 2, Training Loss: 2.101269245147705\n",
            "Epoch: 2, Training Loss: 1.5601216554641724\n",
            "Epoch: 2, Training Loss: 1.841472864151001\n",
            "Epoch: 2, Training Loss: 1.7521958351135254\n",
            "Epoch: 2, Training Loss: 1.81552255153656\n",
            "Epoch: 2, Training Loss: 2.2677478790283203\n",
            "Epoch: 2, Training Loss: 1.9498014450073242\n",
            "Epoch: 2, Training Loss: 1.9003455638885498\n",
            "Epoch: 2, Training Loss: 1.8458927869796753\n",
            "Epoch: 2, Training Loss: 1.9531288146972656\n",
            "Epoch: 2, Training Loss: 1.9946750402450562\n",
            "Epoch: 2, Training Loss: 1.8118458986282349\n",
            "Epoch: 2, Training Loss: 1.793175458908081\n",
            "Epoch: 2, Training Loss: 1.5786972045898438\n",
            "Epoch: 2, Training Loss: 1.85789954662323\n",
            "Epoch: 2, Training Loss: 2.0003180503845215\n",
            "Epoch: 2, Training Loss: 1.698624849319458\n",
            "Epoch: 2, Training Loss: 2.129065990447998\n",
            "Epoch: 2, Training Loss: 2.0623624324798584\n",
            "Epoch: 2, Training Loss: 1.6923340559005737\n",
            "Epoch: 2, Training Loss: 1.7408465147018433\n",
            "Epoch: 2, Training Loss: 1.9277771711349487\n",
            "Epoch: 2, Training Loss: 1.576012372970581\n",
            "Epoch: 2, Training Loss: 2.055032730102539\n",
            "Epoch: 2, Training Loss: 1.9435069561004639\n",
            "Epoch: 2, Training Loss: 2.0008089542388916\n",
            "Epoch: 2, Training Loss: 1.7626051902770996\n",
            "Epoch: 2, Training Loss: 1.9110201597213745\n",
            "Epoch: 2, Training Loss: 1.8347370624542236\n",
            "Epoch: 2, Training Loss: 1.9560246467590332\n",
            "Epoch: 2, Training Loss: 2.0770606994628906\n",
            "Epoch: 2, Training Loss: 1.8959585428237915\n",
            "Epoch: 2, Training Loss: 1.7884385585784912\n",
            "Epoch: 2, Training Loss: 2.045635223388672\n",
            "Epoch: 2, Training Loss: 1.7489335536956787\n",
            "Epoch: 2, Training Loss: 1.7145816087722778\n",
            "Epoch: 2, Training Loss: 1.8203814029693604\n",
            "Epoch: 2, Training Loss: 1.8028191328048706\n",
            "Epoch: 2, Training Loss: 1.6621301174163818\n",
            "Epoch: 2, Training Loss: 1.9396897554397583\n",
            "Epoch: 2, Training Loss: 2.0910520553588867\n",
            "Epoch: 2, Training Loss: 1.8823164701461792\n",
            "Epoch: 2, Training Loss: 1.9749315977096558\n",
            "Epoch: 2, Training Loss: 2.010629415512085\n",
            "Epoch: 2, Training Loss: 1.8568629026412964\n",
            "Epoch: 2, Training Loss: 2.068471670150757\n",
            "Epoch: 2, Training Loss: 1.677919626235962\n",
            "Epoch: 2, Training Loss: 2.034541130065918\n",
            "Epoch: 2, Training Loss: 1.9305169582366943\n",
            "Epoch: 2, Training Loss: 1.888115406036377\n",
            "Epoch: 2, Training Loss: 1.83328115940094\n",
            "Epoch: 2, Training Loss: 1.8074522018432617\n",
            "Epoch: 2, Training Loss: 2.1003143787384033\n",
            "Epoch: 2, Training Loss: 1.90019690990448\n",
            "Epoch: 2, Training Loss: 1.9139446020126343\n",
            "Epoch: 2, Training Loss: 1.8499727249145508\n",
            "Epoch: 2, Training Loss: 1.842429280281067\n",
            "Epoch: 2, Training Loss: 1.968543529510498\n",
            "Epoch: 2, Training Loss: 1.8045504093170166\n",
            "Epoch: 2, Training Loss: 1.8883417844772339\n",
            "Epoch: 2, Training Loss: 1.6141873598098755\n",
            "Epoch: 2, Training Loss: 1.7782930135726929\n",
            "Epoch: 2, Training Loss: 2.0787112712860107\n",
            "Epoch: 2, Training Loss: 2.119554281234741\n",
            "Epoch: 2, Training Loss: 1.9075219631195068\n",
            "Epoch: 2, Training Loss: 1.627967357635498\n",
            "Epoch: 2, Training Loss: 1.9785252809524536\n",
            "Epoch: 2, Training Loss: 2.067168951034546\n",
            "Epoch: 2, Training Loss: 2.0019302368164062\n",
            "Epoch: 2, Training Loss: 2.040900468826294\n",
            "Epoch: 2, Training Loss: 1.9564616680145264\n",
            "Epoch: 2, Training Loss: 1.7470245361328125\n",
            "Epoch: 2, Training Loss: 1.4408830404281616\n",
            "Epoch: 2, Training Loss: 1.8486887216567993\n",
            "Epoch: 2, Training Loss: 1.7610303163528442\n",
            "Epoch: 2, Training Loss: 1.8685880899429321\n",
            "Epoch: 2, Training Loss: 2.0176384449005127\n",
            "Epoch: 2, Training Loss: 2.0240964889526367\n",
            "Epoch: 2, Training Loss: 1.6785107851028442\n",
            "Epoch: 2, Training Loss: 2.0027308464050293\n",
            "Epoch: 2, Training Loss: 1.7053996324539185\n",
            "Epoch: 2, Training Loss: 2.153266191482544\n",
            "Epoch: 2, Training Loss: 1.5995482206344604\n",
            "Epoch: 2, Training Loss: 1.7947182655334473\n",
            "Epoch: 2, Training Loss: 1.9796189069747925\n",
            "Epoch: 2, Training Loss: 1.9058837890625\n",
            "Epoch: 2, Training Loss: 1.9799822568893433\n",
            "Epoch: 2, Training Loss: 1.834901213645935\n",
            "Epoch: 2, Training Loss: 1.9254021644592285\n",
            "Epoch: 2, Training Loss: 1.8247692584991455\n",
            "Epoch: 2, Training Loss: 1.7033650875091553\n",
            "Epoch: 2, Training Loss: 1.6983528137207031\n",
            "Epoch: 2, Training Loss: 1.936253547668457\n",
            "Epoch: 2, Training Loss: 2.0472412109375\n",
            "Epoch: 2, Training Loss: 1.9760769605636597\n",
            "Epoch: 2, Training Loss: 1.7792251110076904\n",
            "Epoch: 2, Training Loss: 1.9411981105804443\n",
            "Epoch: 2, Training Loss: 1.951017141342163\n",
            "Epoch: 2, Training Loss: 1.951786994934082\n",
            "Epoch: 2, Training Loss: 1.7706236839294434\n",
            "Epoch: 2, Training Loss: 1.8571367263793945\n",
            "Epoch: 2, Training Loss: 1.740893840789795\n",
            "Epoch: 2, Training Loss: 1.9319878816604614\n",
            "Epoch: 2, Training Loss: 1.6620569229125977\n",
            "Epoch: 2, Training Loss: 1.9388189315795898\n",
            "Epoch: 2, Training Loss: 1.6570621728897095\n",
            "Epoch: 2, Training Loss: 2.055345058441162\n",
            "Epoch: 2, Training Loss: 1.5898034572601318\n",
            "Epoch: 2, Training Loss: 1.9483261108398438\n",
            "Epoch: 2, Training Loss: 2.1629161834716797\n",
            "Epoch: 2, Training Loss: 1.9992402791976929\n",
            "Epoch: 2, Training Loss: 1.707240343093872\n",
            "Epoch: 2, Training Loss: 1.8828307390213013\n",
            "Epoch: 2, Training Loss: 1.7588049173355103\n",
            "Epoch: 2, Training Loss: 1.783866286277771\n",
            "Epoch: 2, Training Loss: 2.1625611782073975\n",
            "Epoch: 2, Training Loss: 2.0362038612365723\n",
            "Epoch: 2, Training Loss: 1.7659019231796265\n",
            "Epoch: 2, Training Loss: 1.627272129058838\n",
            "Epoch: 2, Training Loss: 1.8093277215957642\n",
            "Epoch: 2, Training Loss: 1.9960683584213257\n",
            "Epoch: 2, Training Loss: 1.8947373628616333\n",
            "Epoch: 2, Training Loss: 1.7626088857650757\n",
            "Epoch: 2, Training Loss: 1.7951598167419434\n",
            "Epoch: 2, Training Loss: 1.7806346416473389\n",
            "Epoch: 2, Training Loss: 1.7985920906066895\n",
            "Epoch: 2, Training Loss: 2.178612232208252\n",
            "Epoch: 2, Training Loss: 2.0646872520446777\n",
            "Epoch: 2, Training Loss: 1.7724019289016724\n",
            "Epoch: 2, Training Loss: 1.8049631118774414\n",
            "Epoch: 2, Training Loss: 1.7679758071899414\n",
            "Epoch: 2, Training Loss: 1.9014719724655151\n",
            "Epoch: 2, Training Loss: 1.9697449207305908\n",
            "Epoch: 2, Training Loss: 1.934510350227356\n",
            "Epoch: 2, Training Loss: 1.9464837312698364\n",
            "Epoch: 2, Training Loss: 1.8000568151474\n",
            "Epoch: 2, Training Loss: 1.6452020406723022\n",
            "Epoch: 2, Training Loss: 1.6912719011306763\n",
            "Epoch: 2, Training Loss: 1.667494773864746\n",
            "Epoch: 2, Training Loss: 1.6982673406600952\n",
            "Epoch: 2, Training Loss: 1.742868185043335\n",
            "Epoch: 2, Training Loss: 1.8688592910766602\n",
            "Epoch: 2, Training Loss: 1.8794050216674805\n",
            "Epoch: 2, Training Loss: 1.8338199853897095\n",
            "Epoch: 2, Training Loss: 2.16656494140625\n",
            "Epoch: 2, Training Loss: 1.9483697414398193\n",
            "Epoch: 2, Training Loss: 1.7066800594329834\n",
            "Epoch: 2, Training Loss: 1.9525940418243408\n",
            "Epoch: 2, Training Loss: 2.1305835247039795\n",
            "Epoch: 2, Training Loss: 1.9881339073181152\n",
            "Epoch: 2, Training Loss: 1.559381127357483\n",
            "Epoch: 2, Training Loss: 1.8319979906082153\n",
            "Epoch: 2, Training Loss: 1.6383106708526611\n",
            "Epoch: 2, Training Loss: 1.914246678352356\n",
            "Epoch: 2, Training Loss: 1.7710953950881958\n",
            "Epoch: 2, Training Loss: 1.861029863357544\n",
            "Epoch: 2, Training Loss: 1.9109065532684326\n",
            "Epoch: 2, Training Loss: 1.7759705781936646\n",
            "Epoch: 2, Training Loss: 1.93641197681427\n",
            "Epoch: 2, Training Loss: 1.8507106304168701\n",
            "Epoch: 2, Training Loss: 1.9390819072723389\n",
            "Epoch: 2, Training Loss: 1.9786927700042725\n",
            "Epoch: 2, Training Loss: 1.938538908958435\n",
            "Epoch: 2, Training Loss: 2.124910354614258\n",
            "Epoch: 2, Training Loss: 1.919992446899414\n",
            "Epoch: 2, Training Loss: 1.9572426080703735\n",
            "Epoch: 2, Training Loss: 1.576379418373108\n",
            "Epoch: 2, Training Loss: 1.6806761026382446\n",
            "Epoch: 2, Training Loss: 1.893133282661438\n",
            "Epoch: 2, Training Loss: 1.7857136726379395\n",
            "Epoch: 2, Training Loss: 1.897178292274475\n",
            "Epoch: 2, Training Loss: 1.7611780166625977\n",
            "Epoch: 2, Training Loss: 1.8879153728485107\n",
            "Epoch: 2, Training Loss: 1.6463149785995483\n",
            "Epoch: 2, Training Loss: 2.0095956325531006\n",
            "Epoch: 2, Training Loss: 1.912994384765625\n",
            "Epoch: 2, Training Loss: 1.7348445653915405\n",
            "Epoch: 2, Training Loss: 1.7558385133743286\n",
            "Epoch: 2, Training Loss: 1.7232286930084229\n",
            "Epoch: 2, Training Loss: 1.999062180519104\n",
            "Epoch: 2, Training Loss: 2.0508859157562256\n",
            "Epoch: 2, Training Loss: 1.769114375114441\n",
            "Epoch: 2, Training Loss: 1.7478373050689697\n",
            "Epoch: 2, Training Loss: 1.9914528131484985\n",
            "Epoch: 2, Training Loss: 1.7550891637802124\n",
            "Epoch: 2, Training Loss: 1.9973386526107788\n",
            "Epoch: 2, Training Loss: 1.8840070962905884\n",
            "Epoch: 2, Training Loss: 1.9376150369644165\n",
            "Epoch: 2, Training Loss: 1.8927768468856812\n",
            "Epoch: 2, Training Loss: 1.7717112302780151\n",
            "Epoch: 2, Training Loss: 1.8906713724136353\n",
            "Epoch: 2, Training Loss: 1.6262751817703247\n",
            "Epoch: 2, Training Loss: 1.693060278892517\n",
            "Epoch: 2, Training Loss: 1.9055066108703613\n",
            "Epoch: 2, Training Loss: 1.9956108331680298\n",
            "Epoch: 2, Training Loss: 1.7842482328414917\n",
            "Epoch: 2, Training Loss: 2.084385395050049\n",
            "Epoch: 2, Training Loss: 1.7086765766143799\n",
            "Epoch: 2, Training Loss: 1.7721893787384033\n",
            "Epoch: 2, Training Loss: 1.6727583408355713\n",
            "Epoch: 2, Training Loss: 1.8463815450668335\n",
            "Epoch: 2, Training Loss: 1.5796595811843872\n",
            "Epoch: 2, Training Loss: 2.0855908393859863\n",
            "Epoch: 2, Training Loss: 1.9130595922470093\n",
            "Epoch: 2, Training Loss: 1.8175408840179443\n",
            "Epoch: 2, Training Loss: 2.0366263389587402\n",
            "Epoch: 2, Training Loss: 1.7580960988998413\n",
            "Epoch: 2, Training Loss: 1.9495689868927002\n",
            "Epoch: 2, Training Loss: 2.0586681365966797\n",
            "Epoch: 2, Training Loss: 1.7128498554229736\n",
            "Epoch: 2, Training Loss: 1.67418372631073\n",
            "Epoch: 2, Training Loss: 2.0953826904296875\n",
            "Epoch: 2, Training Loss: 1.7176512479782104\n",
            "Epoch: 2, Training Loss: 1.8467156887054443\n",
            "Epoch: 2, Training Loss: 1.7646846771240234\n",
            "Epoch: 2, Training Loss: 1.720236897468567\n",
            "Epoch: 2, Training Loss: 1.9408138990402222\n",
            "Epoch: 2, Training Loss: 1.7806525230407715\n",
            "Epoch: 2, Training Loss: 2.027850866317749\n",
            "Epoch: 2, Training Loss: 1.7350987195968628\n",
            "Epoch: 2, Training Loss: 1.5369764566421509\n",
            "Epoch: 2, Training Loss: 2.043983221054077\n",
            "Epoch: 2, Training Loss: 1.643739104270935\n",
            "Epoch: 2, Training Loss: 1.6162775754928589\n",
            "Epoch: 2, Training Loss: 1.76894211769104\n",
            "Epoch: 2, Training Loss: 1.8384090662002563\n",
            "Epoch: 2, Training Loss: 1.8315856456756592\n",
            "Epoch: 2, Training Loss: 1.868115782737732\n",
            "Epoch: 2, Training Loss: 1.8925421237945557\n",
            "Epoch: 2, Training Loss: 1.9811036586761475\n",
            "Epoch: 2, Training Loss: 1.7704218626022339\n",
            "Epoch: 2, Training Loss: 2.114630699157715\n",
            "Epoch: 2, Training Loss: 1.295423984527588\n",
            "Epoch: 2, Training Loss: 1.9814720153808594\n",
            "Epoch: 2, Training Loss: 1.9216268062591553\n",
            "Epoch: 2, Training Loss: 1.9275012016296387\n",
            "Epoch: 2, Training Loss: 1.8960236310958862\n",
            "Epoch: 2, Training Loss: 1.7502843141555786\n",
            "Epoch: 2, Training Loss: 1.93323814868927\n",
            "Epoch: 2, Training Loss: 1.7528870105743408\n",
            "Epoch: 2, Training Loss: 1.7772940397262573\n",
            "Epoch: 2, Training Loss: 1.865821361541748\n",
            "Epoch: 2, Training Loss: 1.8635457754135132\n",
            "Epoch: 2, Training Loss: 1.7165898084640503\n",
            "Epoch: 2, Training Loss: 1.738511085510254\n",
            "Epoch: 2, Training Loss: 1.7123241424560547\n",
            "Epoch: 2, Training Loss: 1.7046717405319214\n",
            "Epoch: 2, Training Loss: 1.8037233352661133\n",
            "Epoch: 2, Training Loss: 1.9668694734573364\n",
            "Epoch: 2, Training Loss: 1.8052639961242676\n",
            "Epoch: 2, Training Loss: 1.7497155666351318\n",
            "Epoch: 2, Training Loss: 1.8991414308547974\n",
            "Epoch: 2, Training Loss: 1.8010215759277344\n",
            "Epoch: 2, Training Loss: 1.6351829767227173\n",
            "Epoch: 2, Training Loss: 2.1288230419158936\n",
            "Epoch: 2, Training Loss: 1.8507663011550903\n",
            "Epoch: 2, Training Loss: 1.7703132629394531\n",
            "Epoch: 2, Training Loss: 2.2318360805511475\n",
            "Epoch: 2, Training Loss: 1.4848986864089966\n",
            "Epoch: 2, Training Loss: 1.5704339742660522\n",
            "Epoch: 2, Training Loss: 1.8957371711730957\n",
            "Epoch: 2, Training Loss: 1.9543187618255615\n",
            "Epoch: 2, Training Loss: 1.672078251838684\n",
            "Epoch: 2, Training Loss: 1.932665467262268\n",
            "Epoch: 2, Training Loss: 1.6812703609466553\n",
            "Epoch: 2, Training Loss: 2.1212270259857178\n",
            "Epoch: 2, Training Loss: 1.881648302078247\n",
            "Epoch: 2, Training Loss: 1.81753408908844\n",
            "Epoch: 2, Training Loss: 1.5496246814727783\n",
            "Epoch: 2, Training Loss: 1.6550441980361938\n",
            "Epoch: 2, Training Loss: 1.7246923446655273\n",
            "Epoch: 2, Training Loss: 2.0036489963531494\n",
            "Epoch: 2, Training Loss: 2.1590213775634766\n",
            "Epoch: 2, Training Loss: 1.8945872783660889\n",
            "Epoch: 2, Training Loss: 1.8675354719161987\n",
            "Epoch: 2, Training Loss: 1.8797723054885864\n",
            "Epoch: 2, Training Loss: 2.035177707672119\n",
            "Epoch: 2, Training Loss: 1.7918288707733154\n",
            "Epoch: 2, Training Loss: 1.7833669185638428\n",
            "Epoch: 2, Training Loss: 1.7586232423782349\n",
            "Epoch: 2, Training Loss: 1.5722819566726685\n",
            "Epoch: 2, Training Loss: 1.9991207122802734\n",
            "Epoch: 2, Training Loss: 1.895015835762024\n",
            "Epoch: 2, Training Loss: 1.9836469888687134\n",
            "Epoch: 2, Training Loss: 2.068803548812866\n",
            "Epoch: 2, Training Loss: 2.0877182483673096\n",
            "Epoch: 2, Training Loss: 1.8912805318832397\n",
            "Epoch: 2, Training Loss: 1.8056049346923828\n",
            "Epoch: 2, Training Loss: 1.894175410270691\n",
            "Epoch: 2, Training Loss: 2.068331003189087\n",
            "Epoch: 2, Training Loss: 1.9398584365844727\n",
            "Epoch: 2, Training Loss: 1.7558408975601196\n",
            "Epoch: 2, Training Loss: 1.9005405902862549\n",
            "Epoch: 2, Training Loss: 1.5879878997802734\n",
            "Epoch: 2, Training Loss: 1.6541013717651367\n",
            "Epoch: 2, Training Loss: 1.8014885187149048\n",
            "Epoch: 2, Training Loss: 1.9678540229797363\n",
            "Epoch: 2, Training Loss: 1.9989681243896484\n",
            "Epoch: 2, Training Loss: 1.9832932949066162\n",
            "Epoch: 2, Training Loss: 1.944190502166748\n",
            "Epoch: 2, Training Loss: 1.9297478199005127\n",
            "Epoch: 2, Training Loss: 1.8839513063430786\n",
            "Epoch: 2, Training Loss: 1.6714001893997192\n",
            "Epoch: 2, Training Loss: 2.0202794075012207\n",
            "Epoch: 2, Training Loss: 1.837829828262329\n",
            "Epoch: 2, Training Loss: 2.0250706672668457\n",
            "Epoch: 2, Training Loss: 1.878421664237976\n",
            "Epoch: 2, Training Loss: 1.8315551280975342\n",
            "Epoch: 2, Training Loss: 1.6419156789779663\n",
            "Epoch: 2, Training Loss: 1.740136742591858\n",
            "Epoch: 2, Training Loss: 1.580506682395935\n",
            "Epoch: 2, Training Loss: 1.6075372695922852\n",
            "Epoch: 2, Training Loss: 1.8041491508483887\n",
            "Epoch: 2, Training Loss: 1.8418887853622437\n",
            "Epoch: 2, Training Loss: 1.694449543952942\n",
            "Epoch: 2, Training Loss: 1.7489817142486572\n",
            "Epoch: 2, Training Loss: 1.8995593786239624\n",
            "Epoch: 2, Training Loss: 1.7617378234863281\n",
            "Epoch: 2, Training Loss: 1.8538708686828613\n",
            "Epoch: 2, Training Loss: 1.958168864250183\n",
            "Epoch: 2, Training Loss: 1.8021903038024902\n",
            "Epoch: 2, Training Loss: 1.8335291147232056\n",
            "Epoch: 2, Training Loss: 1.8188751935958862\n",
            "Epoch: 2, Training Loss: 1.9017765522003174\n",
            "Epoch: 2, Training Loss: 1.5436346530914307\n",
            "Epoch: 2, Training Loss: 1.736149549484253\n",
            "Epoch: 2, Training Loss: 1.4749125242233276\n",
            "Epoch: 2, Training Loss: 1.682210922241211\n",
            "Epoch: 2, Training Loss: 1.769952416419983\n",
            "Epoch: 2, Training Loss: 1.538907766342163\n",
            "Epoch: 2, Training Loss: 1.9190857410430908\n",
            "Epoch: 2, Training Loss: 1.9507088661193848\n",
            "Epoch: 2, Training Loss: 1.8912158012390137\n",
            "Epoch: 2, Training Loss: 1.6841998100280762\n",
            "Epoch: 2, Training Loss: 1.554381251335144\n",
            "Epoch: 2, Training Loss: 1.648810863494873\n",
            "Epoch: 2, Training Loss: 2.003509044647217\n",
            "Epoch: 2, Training Loss: 1.8946408033370972\n",
            "Epoch: 2, Training Loss: 1.8352004289627075\n",
            "Epoch: 2, Training Loss: 1.7593275308609009\n",
            "Epoch: 2, Training Loss: 1.7772338390350342\n",
            "Epoch: 2, Training Loss: 1.5305895805358887\n",
            "Epoch: 2, Training Loss: 1.8402782678604126\n",
            "Epoch: 2, Training Loss: 1.798298954963684\n",
            "Epoch: 2, Training Loss: 2.0180511474609375\n",
            "Epoch: 2, Training Loss: 1.7003376483917236\n",
            "Epoch: 2, Training Loss: 1.7068564891815186\n",
            "Epoch: 2, Training Loss: 1.719464898109436\n",
            "Epoch: 2, Training Loss: 1.6700847148895264\n",
            "Epoch: 2, Training Loss: 1.665819525718689\n",
            "Epoch: 2, Training Loss: 1.9117765426635742\n",
            "Epoch: 2, Training Loss: 1.5587133169174194\n",
            "Epoch: 2, Training Loss: 1.769017219543457\n",
            "Epoch: 2, Training Loss: 1.9600921869277954\n",
            "Epoch: 2, Training Loss: 1.7292444705963135\n",
            "Epoch: 2, Training Loss: 2.0113425254821777\n",
            "Epoch: 2, Training Loss: 1.7962781190872192\n",
            "Epoch: 2, Training Loss: 1.92091703414917\n",
            "Epoch: 2, Training Loss: 1.7827481031417847\n",
            "Epoch: 2, Training Loss: 1.8320802450180054\n",
            "Epoch: 2, Training Loss: 2.114433765411377\n",
            "Epoch: 2, Training Loss: 1.794319987297058\n",
            "Epoch: 2, Training Loss: 1.696501612663269\n",
            "Epoch: 2, Training Loss: 1.9121195077896118\n",
            "Epoch: 2, Training Loss: 1.5287894010543823\n",
            "Epoch: 2, Training Loss: 1.989302635192871\n",
            "Epoch: 2, Training Loss: 1.7595008611679077\n",
            "Epoch: 2, Training Loss: 1.9644678831100464\n",
            "Epoch: 2, Training Loss: 1.7464518547058105\n",
            "Epoch: 2, Training Loss: 1.6949526071548462\n",
            "Epoch: 2, Training Loss: 1.4895285367965698\n",
            "Epoch: 2, Training Loss: 1.4632656574249268\n",
            "Epoch: 2, Training Loss: 2.0832481384277344\n",
            "Epoch: 2, Training Loss: 1.845036268234253\n",
            "Epoch: 2, Training Loss: 1.571563959121704\n",
            "Epoch: 2, Training Loss: 1.6321531534194946\n",
            "Epoch: 2, Training Loss: 1.8155035972595215\n",
            "Epoch: 2, Training Loss: 1.8958603143692017\n",
            "Epoch: 2, Training Loss: 1.624006986618042\n",
            "Epoch: 2, Training Loss: 1.7042019367218018\n",
            "Epoch: 2, Training Loss: 1.7665302753448486\n",
            "Epoch: 2, Training Loss: 1.5722075700759888\n",
            "Epoch: 2, Training Loss: 1.900338888168335\n",
            "Epoch: 2, Training Loss: 1.752811074256897\n",
            "Epoch: 2, Training Loss: 1.711795449256897\n",
            "Epoch: 2, Training Loss: 1.9684075117111206\n",
            "Epoch: 2, Training Loss: 1.6842775344848633\n",
            "Epoch: 2, Training Loss: 1.7825284004211426\n",
            "Epoch: 2, Training Loss: 1.9201126098632812\n",
            "Epoch: 2, Training Loss: 1.6494418382644653\n",
            "Epoch: 2, Training Loss: 1.7441482543945312\n",
            "Epoch: 2, Training Loss: 1.8323681354522705\n",
            "Epoch: 2, Training Loss: 1.819440245628357\n",
            "Epoch: 2, Training Loss: 1.4603025913238525\n",
            "Epoch: 2, Training Loss: 1.7475864887237549\n",
            "Epoch: 2, Training Loss: 1.7713453769683838\n",
            "Epoch: 2, Training Loss: 1.9812067747116089\n",
            "Epoch: 2, Training Loss: 1.687575340270996\n",
            "Epoch: 2, Training Loss: 1.8617818355560303\n",
            "Epoch: 2, Training Loss: 1.6531107425689697\n",
            "Epoch: 2, Training Loss: 1.9521045684814453\n",
            "Epoch: 2, Training Loss: 2.068720817565918\n",
            "Epoch: 2, Training Loss: 1.6269093751907349\n",
            "Epoch: 2, Training Loss: 1.767486572265625\n",
            "Epoch: 2, Training Loss: 1.763264775276184\n",
            "Epoch: 2, Training Loss: 1.9619795083999634\n",
            "Epoch: 2, Training Loss: 1.750620722770691\n",
            "Epoch: 2, Training Loss: 1.802966594696045\n",
            "Epoch: 2, Training Loss: 1.7222135066986084\n",
            "Epoch: 2, Training Loss: 1.8712977170944214\n",
            "Epoch: 2, Training Loss: 1.8601510524749756\n",
            "Epoch: 2, Training Loss: 2.0106279850006104\n",
            "Epoch: 2, Training Loss: 1.6995131969451904\n",
            "Epoch: 2, Training Loss: 2.079022169113159\n",
            "Epoch: 2, Training Loss: 1.7448474168777466\n",
            "Epoch: 2, Training Loss: 1.8058514595031738\n",
            "Epoch: 2, Training Loss: 1.7482496500015259\n",
            "Epoch: 2, Training Loss: 1.7338483333587646\n",
            "Epoch: 2, Training Loss: 1.8073639869689941\n",
            "Epoch: 2, Training Loss: 1.8725371360778809\n",
            "Epoch: 2, Training Loss: 1.6310384273529053\n",
            "Epoch: 2, Training Loss: 1.5953214168548584\n",
            "Epoch: 2, Training Loss: 1.7844926118850708\n",
            "Epoch: 2, Training Loss: 1.752936601638794\n",
            "Epoch: 2, Training Loss: 1.8191876411437988\n",
            "Epoch: 2, Training Loss: 1.7203627824783325\n",
            "Epoch: 2, Training Loss: 1.9274449348449707\n",
            "Epoch: 2, Training Loss: 1.7144896984100342\n",
            "Epoch: 2, Training Loss: 1.9885485172271729\n",
            "Epoch: 2, Training Loss: 2.039015293121338\n",
            "Epoch: 2, Training Loss: 1.7793129682540894\n",
            "Epoch: 2, Training Loss: 1.7353119850158691\n",
            "Epoch: 2, Training Loss: 1.7032705545425415\n",
            "Epoch: 2, Training Loss: 1.534459114074707\n",
            "Epoch: 2, Training Loss: 1.7711642980575562\n",
            "Epoch: 2, Training Loss: 1.6054863929748535\n",
            "Epoch: 2, Training Loss: 1.8980638980865479\n",
            "Epoch: 2, Training Loss: 1.7601735591888428\n",
            "Epoch: 2, Training Loss: 1.8101656436920166\n",
            "Epoch: 2, Training Loss: 1.7750942707061768\n",
            "Epoch: 2, Training Loss: 1.549735426902771\n",
            "Epoch: 2, Training Loss: 1.7102744579315186\n",
            "Epoch: 2, Training Loss: 1.9340944290161133\n",
            "Epoch: 2, Training Loss: 1.7720900774002075\n",
            "Epoch: 2, Training Loss: 1.8293870687484741\n",
            "Epoch: 2, Training Loss: 1.5059752464294434\n",
            "Epoch: 2, Training Loss: 2.015009641647339\n",
            "Epoch: 2, Training Loss: 1.7538373470306396\n",
            "Epoch: 2, Training Loss: 2.0324482917785645\n",
            "Epoch: 2, Training Loss: 1.6606829166412354\n",
            "Epoch: 2, Training Loss: 1.7030075788497925\n",
            "Epoch: 2, Training Loss: 1.624127745628357\n",
            "Epoch: 2, Training Loss: 1.6651338338851929\n",
            "Epoch: 2, Training Loss: 1.6489654779434204\n",
            "Epoch: 2, Training Loss: 1.6845393180847168\n",
            "Epoch: 2, Training Loss: 1.6677534580230713\n",
            "Epoch: 2, Training Loss: 1.664680004119873\n",
            "Epoch: 2, Training Loss: 1.8231019973754883\n",
            "Epoch: 2, Training Loss: 1.8707009553909302\n",
            "Epoch: 2, Training Loss: 2.0942835807800293\n",
            "Epoch: 2, Training Loss: 2.1042580604553223\n",
            "Epoch: 2, Training Loss: 1.8351362943649292\n",
            "Epoch: 2, Training Loss: 1.8037728071212769\n",
            "Epoch: 2, Training Loss: 1.5917431116104126\n",
            "Epoch: 2, Training Loss: 1.5788476467132568\n",
            "Epoch: 2, Training Loss: 1.5937626361846924\n",
            "Epoch: 2, Training Loss: 1.8698943853378296\n",
            "Epoch: 2, Training Loss: 1.43796968460083\n",
            "Epoch: 2, Training Loss: 1.7908258438110352\n",
            "Epoch: 2, Training Loss: 1.8294841051101685\n",
            "Epoch: 2, Training Loss: 1.595569372177124\n",
            "Epoch: 2, Training Loss: 1.96748948097229\n",
            "Epoch: 2, Training Loss: 1.7250583171844482\n",
            "Epoch: 2, Training Loss: 1.7792342901229858\n",
            "Epoch: 2, Training Loss: 1.5865960121154785\n",
            "Epoch: 2, Training Loss: 1.8941301107406616\n",
            "Epoch: 2, Training Loss: 1.6663731336593628\n",
            "Epoch: 2, Training Loss: 1.8751780986785889\n",
            "Epoch: 2, Training Loss: 1.874699354171753\n",
            "Epoch: 2, Training Loss: 1.7606124877929688\n",
            "Epoch: 2, Training Loss: 1.8185458183288574\n",
            "Epoch: 2, Training Loss: 1.4920527935028076\n",
            "Epoch: 2, Training Loss: 1.7479021549224854\n",
            "Epoch: 2, Training Loss: 1.6016967296600342\n",
            "Epoch: 2, Training Loss: 1.5474960803985596\n",
            "Epoch: 2, Training Loss: 1.8428492546081543\n",
            "Epoch: 2, Training Loss: 1.7472517490386963\n",
            "Epoch: 2, Training Loss: 1.6540989875793457\n",
            "Epoch: 2, Training Loss: 1.7950886487960815\n",
            "Epoch: 2, Training Loss: 1.8005186319351196\n",
            "Epoch: 2, Training Loss: 1.8019012212753296\n",
            "Epoch: 2, Training Loss: 2.0306649208068848\n",
            "Epoch: 2, Training Loss: 1.4885889291763306\n",
            "Epoch: 2, Training Loss: 1.714521050453186\n",
            "Epoch: 2, Training Loss: 1.9440011978149414\n",
            "Epoch: 2, Training Loss: 1.7284268140792847\n",
            "Epoch: 2, Training Loss: 1.8465615510940552\n",
            "Epoch: 2, Training Loss: 1.6055923700332642\n",
            "Epoch: 2, Training Loss: 1.7578222751617432\n",
            "Epoch: 2, Training Loss: 1.8719260692596436\n",
            "Epoch: 2, Training Loss: 1.3831454515457153\n",
            "Epoch: 2, Training Loss: 2.0948004722595215\n",
            "Epoch: 2, Training Loss: 1.8636749982833862\n",
            "Epoch: 2, Training Loss: 1.8433687686920166\n",
            "Epoch: 2, Training Loss: 1.8497885465621948\n",
            "Epoch: 2, Training Loss: 1.8359971046447754\n",
            "Epoch: 2, Training Loss: 1.9447625875473022\n",
            "Epoch: 2, Training Loss: 1.771520733833313\n",
            "Epoch: 2, Training Loss: 1.6928532123565674\n",
            "Epoch: 2, Training Loss: 1.6176340579986572\n",
            "Epoch: 2, Training Loss: 1.702515721321106\n",
            "Epoch: 2, Training Loss: 1.8399755954742432\n",
            "Epoch: 2, Training Loss: 2.0235753059387207\n",
            "Epoch: 2, Training Loss: 1.7210496664047241\n",
            "Epoch: 2, Training Loss: 1.7191877365112305\n",
            "Epoch: 2, Training Loss: 1.818437099456787\n",
            "Epoch: 2, Training Loss: 1.355894923210144\n",
            "Epoch: 2, Training Loss: 1.652264952659607\n",
            "Epoch: 2, Training Loss: 1.6276675462722778\n",
            "Epoch: 2, Training Loss: 2.1429033279418945\n",
            "Epoch: 2, Training Loss: 1.910995364189148\n",
            "Epoch: 2, Training Loss: 2.046862840652466\n",
            "Epoch: 2, Training Loss: 1.8389204740524292\n",
            "Epoch: 2, Training Loss: 1.6619175672531128\n",
            "Epoch: 2, Training Loss: 2.0335922241210938\n",
            "Epoch: 2, Training Loss: 1.7134982347488403\n",
            "Epoch: 2, Training Loss: 1.9021421670913696\n",
            "Epoch: 2, Training Loss: 1.8533848524093628\n",
            "Epoch: 2, Training Loss: 1.8976128101348877\n",
            "Epoch: 2, Training Loss: 1.8927361965179443\n",
            "Epoch: 2, Training Loss: 1.6669734716415405\n",
            "Epoch: 2, Training Loss: 1.5805244445800781\n",
            "Epoch: 2, Training Loss: 1.878344178199768\n",
            "Epoch: 2, Training Loss: 1.726405143737793\n",
            "Epoch: 2, Training Loss: 1.64134681224823\n",
            "Epoch: 2, Training Loss: 1.565834879875183\n",
            "Epoch: 2, Training Loss: 1.9697719812393188\n",
            "Epoch: 2, Training Loss: 1.7830606698989868\n",
            "Epoch: 2, Training Loss: 1.8370237350463867\n",
            "Epoch: 2, Training Loss: 1.8370563983917236\n",
            "Epoch: 2, Training Loss: 1.6283332109451294\n",
            "Epoch: 2, Training Loss: 1.738309383392334\n",
            "Epoch: 2, Training Loss: 1.737947940826416\n",
            "Epoch: 2, Training Loss: 2.0112128257751465\n",
            "Epoch: 2, Training Loss: 1.8952102661132812\n",
            "Epoch: 2, Training Loss: 2.044807195663452\n",
            "Epoch: 2, Training Loss: 1.770634651184082\n",
            "Epoch: 2, Training Loss: 1.5562667846679688\n",
            "Epoch: 2, Training Loss: 1.6765726804733276\n",
            "Epoch: 2, Training Loss: 1.8176023960113525\n",
            "Epoch: 2, Training Loss: 2.070566415786743\n",
            "Epoch: 2, Training Loss: 1.872761845588684\n",
            "Epoch: 2, Training Loss: 1.8482953310012817\n",
            "Epoch: 2, Training Loss: 1.7812881469726562\n",
            "Epoch: 2, Training Loss: 1.892733097076416\n",
            "Epoch: 2, Training Loss: 1.8044565916061401\n",
            "Epoch: 2, Training Loss: 1.7979341745376587\n",
            "Epoch: 2, Training Loss: 1.9391074180603027\n",
            "Epoch: 2, Training Loss: 1.5387324094772339\n",
            "Epoch: 2, Training Loss: 1.8418099880218506\n",
            "Epoch: 2, Training Loss: 1.6639550924301147\n",
            "Epoch: 2, Training Loss: 1.8261184692382812\n",
            "Epoch: 2, Training Loss: 1.85209059715271\n",
            "Epoch: 2, Training Loss: 1.6826716661453247\n",
            "Epoch: 2, Training Loss: 1.8273417949676514\n",
            "Epoch: 2, Training Loss: 1.9798235893249512\n",
            "Epoch: 2, Training Loss: 1.6976679563522339\n",
            "Epoch: 2, Training Loss: 1.8868536949157715\n",
            "Epoch: 2, Training Loss: 1.802161693572998\n",
            "Epoch: 2, Training Loss: 1.6846086978912354\n",
            "Epoch: 2, Training Loss: 1.464483618736267\n",
            "Epoch: 2, Training Loss: 1.6965192556381226\n",
            "Epoch: 2, Training Loss: 1.6096714735031128\n",
            "Epoch: 2, Training Loss: 1.8094686269760132\n",
            "Epoch: 2, Training Loss: 1.8830004930496216\n",
            "Epoch: 2, Training Loss: 1.6656625270843506\n",
            "Epoch: 2, Training Loss: 1.7632522583007812\n",
            "Epoch: 2, Training Loss: 1.8879727125167847\n",
            "Epoch: 2, Training Loss: 1.579209566116333\n",
            "Epoch: 2, Training Loss: 1.9415757656097412\n",
            "Epoch: 2, Training Loss: 1.7817249298095703\n",
            "Epoch: 2, Training Loss: 1.9473516941070557\n",
            "Epoch: 2, Training Loss: 1.662150263786316\n",
            "Epoch: 2, Training Loss: 1.731216311454773\n",
            "Epoch: 2, Training Loss: 1.9077823162078857\n",
            "Epoch: 2, Training Loss: 1.6731765270233154\n",
            "Epoch: 2, Training Loss: 1.4522297382354736\n",
            "Epoch: 2, Training Loss: 1.7359791994094849\n",
            "Epoch: 2, Training Loss: 1.9992434978485107\n",
            "Epoch: 2, Training Loss: 1.7949497699737549\n",
            "Epoch: 2, Training Loss: 1.9191967248916626\n",
            "Epoch: 2, Training Loss: 1.4205057621002197\n",
            "Epoch: 2, Training Loss: 1.9563299417495728\n",
            "Epoch: 2, Training Loss: 1.8776463270187378\n",
            "Epoch: 2, Training Loss: 1.5721077919006348\n",
            "Epoch: 2, Training Loss: 1.822856068611145\n",
            "Epoch: 2, Training Loss: 1.812347650527954\n",
            "Epoch: 2, Training Loss: 1.9350472688674927\n",
            "Epoch: 2, Training Loss: 1.7989131212234497\n",
            "Epoch: 2, Training Loss: 1.8304194211959839\n",
            "Epoch: 2, Training Loss: 1.8062721490859985\n",
            "Epoch: 2, Training Loss: 1.8904982805252075\n",
            "Epoch: 2, Training Loss: 1.7549798488616943\n",
            "Epoch: 2, Training Loss: 1.744348406791687\n",
            "Epoch: 2, Training Loss: 1.6408069133758545\n",
            "Epoch: 2, Training Loss: 1.9115283489227295\n",
            "Epoch: 2, Training Loss: 1.825130581855774\n",
            "Epoch: 2, Training Loss: 1.6797059774398804\n",
            "Epoch: 2, Training Loss: 1.762587308883667\n",
            "Epoch: 2, Training Loss: 1.7386752367019653\n",
            "Epoch: 2, Training Loss: 1.6953929662704468\n",
            "Epoch: 2, Training Loss: 1.4213805198669434\n",
            "Epoch: 2, Training Loss: 1.5656781196594238\n",
            "Epoch: 2, Training Loss: 2.156008243560791\n",
            "Epoch: 2, Training Loss: 1.5319697856903076\n",
            "Epoch: 2, Training Loss: 1.702567458152771\n",
            "Epoch: 2, Training Loss: 1.8023624420166016\n",
            "Epoch: 2, Training Loss: 1.6202354431152344\n",
            "Epoch: 2, Training Loss: 2.0328128337860107\n",
            "Epoch: 2, Training Loss: 1.8881441354751587\n",
            "Epoch: 2, Training Loss: 1.7171828746795654\n",
            "Epoch: 2, Training Loss: 1.726222276687622\n",
            "Epoch: 2, Training Loss: 1.6810060739517212\n",
            "Epoch: 2, Training Loss: 1.6303212642669678\n",
            "Epoch: 2, Training Loss: 1.5474522113800049\n",
            "Epoch: 2, Training Loss: 1.7340854406356812\n",
            "Epoch: 2, Training Loss: 1.7969114780426025\n",
            "Epoch: 2, Training Loss: 1.882330298423767\n",
            "Epoch: 2, Training Loss: 2.0805110931396484\n",
            "Epoch: 2, Training Loss: 1.7249364852905273\n",
            "Epoch: 2, Training Loss: 1.8472052812576294\n",
            "Epoch: 2, Training Loss: 2.0176870822906494\n",
            "Epoch: 2, Training Loss: 1.701514720916748\n",
            "Epoch: 2, Training Loss: 2.063612222671509\n",
            "Epoch: 2, Training Loss: 1.5109326839447021\n",
            "Epoch: 2, Training Loss: 1.7980821132659912\n",
            "Epoch: 2, Training Loss: 1.7846691608428955\n",
            "Epoch: 2, Training Loss: 1.8046990633010864\n",
            "Epoch: 2, Training Loss: 1.7052019834518433\n",
            "Epoch: 2, Training Loss: 1.7078344821929932\n",
            "Epoch: 2, Training Loss: 1.8065968751907349\n",
            "Epoch: 2, Training Loss: 1.8539005517959595\n",
            "Epoch: 2, Training Loss: 1.6185063123703003\n",
            "Epoch: 2, Training Loss: 1.8333449363708496\n",
            "Epoch: 2, Training Loss: 1.5589699745178223\n",
            "Epoch: 2, Training Loss: 1.726549744606018\n",
            "Epoch: 2, Training Loss: 1.6465537548065186\n",
            "Epoch: 2, Training Loss: 2.0208072662353516\n",
            "Epoch: 2, Training Loss: 1.712167501449585\n",
            "Epoch: 2, Training Loss: 1.838658094406128\n",
            "Epoch: 2, Training Loss: 1.6702708005905151\n",
            "Epoch: 2, Training Loss: 1.6608117818832397\n",
            "Epoch: 2, Training Loss: 1.6992228031158447\n",
            "Epoch: 2, Training Loss: 1.8588356971740723\n",
            "Epoch: 2, Training Loss: 1.7355386018753052\n",
            "Epoch: 2, Training Loss: 1.6156789064407349\n",
            "Epoch: 2, Training Loss: 1.997788906097412\n",
            "Epoch: 2, Training Loss: 1.675573468208313\n",
            "Epoch: 2, Training Loss: 1.7291744947433472\n",
            "Epoch: 2, Training Loss: 1.9602084159851074\n",
            "Epoch: 2, Training Loss: 1.8453677892684937\n",
            "Epoch: 2, Training Loss: 1.8752082586288452\n",
            "Epoch: 2, Training Loss: 1.9776816368103027\n",
            "Epoch: 2, Training Loss: 1.6669857501983643\n",
            "Epoch: 2, Training Loss: 1.994978904724121\n",
            "Epoch: 2, Training Loss: 1.7410331964492798\n",
            "Epoch: 2, Training Loss: 1.491235613822937\n",
            "Epoch: 2, Training Loss: 1.7045525312423706\n",
            "Epoch: 2, Training Loss: 1.6776937246322632\n",
            "Epoch: 2, Training Loss: 2.030712842941284\n",
            "Epoch: 2, Training Loss: 1.5528593063354492\n",
            "Epoch: 2, Training Loss: 1.7530333995819092\n",
            "Epoch: 2, Training Loss: 1.5605632066726685\n",
            "Epoch: 2, Training Loss: 1.8105158805847168\n",
            "Epoch: 2, Training Loss: 1.7029595375061035\n",
            "Epoch: 2, Training Loss: 1.850783109664917\n",
            "Epoch: 2, Training Loss: 1.7220349311828613\n",
            "Epoch: 2, Training Loss: 1.6809520721435547\n",
            "Epoch: 2, Training Loss: 1.7768381834030151\n",
            "Epoch: 2, Training Loss: 1.6437242031097412\n",
            "Epoch: 2, Training Loss: 1.6869159936904907\n",
            "Epoch: 2, Training Loss: 1.5882512331008911\n",
            "Epoch: 2, Training Loss: 1.5763018131256104\n",
            "Epoch: 2, Training Loss: 1.9516472816467285\n",
            "Epoch: 2, Training Loss: 1.765540361404419\n",
            "Epoch: 2, Training Loss: 1.6886953115463257\n",
            "Epoch: 2, Training Loss: 1.6439467668533325\n",
            "Epoch: 2, Training Loss: 1.791822910308838\n",
            "Epoch: 2, Training Loss: 2.0295908451080322\n",
            "Epoch: 2, Training Loss: 1.729583740234375\n",
            "Epoch: 2, Training Loss: 1.3030916452407837\n",
            "Epoch: 2, Training Loss: 2.1707606315612793\n",
            "Epoch: 2, Training Loss: 1.891079068183899\n",
            "Epoch: 2, Training Loss: 1.5234782695770264\n",
            "Epoch: 2, Training Loss: 1.7958531379699707\n",
            "Epoch: 2, Training Loss: 1.8371690511703491\n",
            "Epoch: 2, Training Loss: 1.6960721015930176\n",
            "Epoch: 2, Training Loss: 1.6950231790542603\n",
            "Epoch: 2, Training Loss: 1.677055835723877\n",
            "Epoch: 2, Training Loss: 1.6393249034881592\n",
            "Epoch: 2, Training Loss: 1.5707471370697021\n",
            "Epoch: 2, Training Loss: 1.73768949508667\n",
            "Epoch: 2, Training Loss: 1.5430989265441895\n",
            "Epoch: 2, Training Loss: 1.697454810142517\n",
            "Epoch: 2, Training Loss: 1.788406252861023\n",
            "Epoch: 2, Training Loss: 1.8929474353790283\n",
            "Epoch: 2, Training Loss: 1.6911476850509644\n",
            "Epoch: 2, Training Loss: 1.6794614791870117\n",
            "Epoch: 2, Training Loss: 2.153337240219116\n",
            "Epoch: 2, Training Loss: 1.948911428451538\n",
            "Epoch: 2, Training Loss: 1.6310756206512451\n",
            "Epoch: 2, Training Loss: 1.8786191940307617\n",
            "Epoch: 2, Training Loss: 1.8729861974716187\n",
            "Epoch: 2, Training Loss: 1.7049254179000854\n",
            "Epoch: 2, Training Loss: 1.5194295644760132\n",
            "Epoch: 2, Training Loss: 1.6531119346618652\n",
            "Epoch: 2, Training Loss: 1.7566776275634766\n",
            "Epoch: 2, Training Loss: 1.863796591758728\n",
            "Epoch: 2, Training Loss: 1.813390851020813\n",
            "Epoch: 2, Training Loss: 1.7284605503082275\n",
            "Epoch: 2, Training Loss: 1.7536606788635254\n",
            "Epoch: 2, Training Loss: 1.8319458961486816\n",
            "Epoch: 2, Training Loss: 1.3826297521591187\n",
            "Epoch: 2, Training Loss: 1.8859975337982178\n",
            "Epoch: 2, Training Loss: 1.9171781539916992\n",
            "Epoch: 2, Training Loss: 1.6863733530044556\n",
            "Epoch: 2, Training Loss: 1.6904176473617554\n",
            "Epoch: 2, Training Loss: 1.6922687292099\n",
            "Epoch: 2, Training Loss: 1.7953670024871826\n",
            "Epoch: 2, Training Loss: 1.9594327211380005\n",
            "Epoch: 2, Training Loss: 1.7470426559448242\n",
            "Epoch: 2, Training Loss: 1.6734001636505127\n",
            "Epoch: 2, Training Loss: 1.7129789590835571\n",
            "Epoch: 2, Training Loss: 1.8537794351577759\n",
            "Epoch: 2, Training Loss: 1.7798433303833008\n",
            "Epoch: 2, Training Loss: 1.9477012157440186\n",
            "Epoch: 2, Training Loss: 1.7819081544876099\n",
            "Epoch: 2, Training Loss: 1.778123140335083\n",
            "Epoch: 2, Training Loss: 1.6942673921585083\n",
            "Epoch: 2, Training Loss: 1.808266520500183\n",
            "Epoch: 2, Training Loss: 1.780491590499878\n",
            "Epoch: 2, Training Loss: 1.834124207496643\n",
            "Epoch: 2, Training Loss: 1.8041423559188843\n",
            "Epoch: 2, Training Loss: 1.8706339597702026\n",
            "Epoch: 2, Training Loss: 1.7379952669143677\n",
            "Epoch: 2, Training Loss: 1.5660243034362793\n",
            "Epoch: 2, Training Loss: 1.67714262008667\n",
            "Epoch: 2, Training Loss: 1.7883580923080444\n",
            "Epoch: 2, Training Loss: 1.839983582496643\n",
            "Epoch: 2, Training Loss: 1.8501865863800049\n",
            "Epoch: 2, Training Loss: 1.7521286010742188\n",
            "Epoch: 2, Training Loss: 1.67583167552948\n",
            "Epoch: 2, Training Loss: 1.8082447052001953\n",
            "Epoch: 2, Training Loss: 2.0271100997924805\n",
            "Epoch: 2, Training Loss: 1.384717345237732\n",
            "Epoch: 2, Training Loss: 1.5742353200912476\n",
            "Epoch: 2, Training Loss: 1.7157745361328125\n",
            "Epoch: 2, Training Loss: 1.8013114929199219\n",
            "Epoch: 2, Training Loss: 1.704598069190979\n",
            "Epoch: 2, Training Loss: 1.92475426197052\n",
            "Epoch: 2, Training Loss: 2.003121852874756\n",
            "Epoch: 2, Training Loss: 1.666513204574585\n",
            "Epoch: 2, Training Loss: 1.8610540628433228\n",
            "Epoch: 2, Training Loss: 1.905297040939331\n",
            "Epoch: 2, Training Loss: 1.5836952924728394\n",
            "Epoch: 2, Training Loss: 1.8527175188064575\n",
            "Epoch: 2, Training Loss: 1.4899221658706665\n",
            "Epoch: 2, Training Loss: 1.6203705072402954\n",
            "Epoch: 2, Training Loss: 1.6630940437316895\n",
            "Epoch: 2, Training Loss: 1.6274422407150269\n",
            "Epoch: 2, Training Loss: 1.953551173210144\n",
            "Epoch: 2, Training Loss: 1.7953382730484009\n",
            "Epoch: 2, Training Loss: 1.8612654209136963\n",
            "Epoch: 2, Training Loss: 1.6424651145935059\n",
            "Epoch: 2, Training Loss: 1.7821629047393799\n",
            "Epoch: 2, Training Loss: 2.222723960876465\n",
            "Epoch: 2, Training Loss: 1.6797906160354614\n",
            "Epoch: 2, Training Loss: 1.5148855447769165\n",
            "Epoch: 2, Training Loss: 1.7519575357437134\n",
            "Epoch: 2, Training Loss: 1.698451280593872\n",
            "Epoch: 2, Training Loss: 1.8243821859359741\n",
            "Epoch: 2, Training Loss: 1.8730888366699219\n",
            "Epoch: 2, Training Loss: 1.999804973602295\n",
            "Epoch: 2, Training Loss: 1.6446006298065186\n",
            "Epoch: 2, Training Loss: 1.8361293077468872\n",
            "Epoch: 2, Training Loss: 1.6987271308898926\n",
            "Epoch: 2, Training Loss: 1.9824918508529663\n",
            "Epoch: 2, Training Loss: 2.0126492977142334\n",
            "Epoch: 2, Training Loss: 1.6725796461105347\n",
            "Epoch: 2, Training Loss: 1.8025888204574585\n",
            "Epoch: 2, Training Loss: 1.7517582178115845\n",
            "Epoch: 2, Training Loss: 1.7349517345428467\n",
            "Epoch: 2, Training Loss: 1.851986289024353\n",
            "Epoch: 2, Training Loss: 1.6800843477249146\n",
            "Epoch: 2, Training Loss: 1.6962251663208008\n",
            "Epoch: 2, Training Loss: 1.8102102279663086\n",
            "Epoch: 2, Training Loss: 1.4826894998550415\n",
            "Epoch: 2, Training Loss: 1.582085132598877\n",
            "Epoch: 2, Training Loss: 1.7470043897628784\n",
            "Epoch: 2, Training Loss: 1.7707901000976562\n",
            "Epoch: 2, Training Loss: 1.5574748516082764\n",
            "Epoch: 2, Training Loss: 1.5944867134094238\n",
            "Epoch: 2, Training Loss: 1.6114873886108398\n",
            "Epoch: 2, Training Loss: 1.6950079202651978\n",
            "Epoch: 2, Training Loss: 1.9680049419403076\n",
            "Epoch: 2, Training Loss: 1.691220998764038\n",
            "Epoch: 2, Training Loss: 1.7285594940185547\n",
            "Epoch: 2, Training Loss: 1.774994969367981\n",
            "Epoch: 2, Training Loss: 1.640518307685852\n",
            "Epoch: 2, Training Loss: 1.9795396327972412\n",
            "Epoch: 2, Training Loss: 1.7822414636611938\n",
            "Epoch: 2, Training Loss: 1.5844542980194092\n",
            "Epoch: 2, Training Loss: 1.900227427482605\n",
            "Epoch: 2, Training Loss: 1.7518374919891357\n",
            "Epoch: 2, Training Loss: 1.6817219257354736\n",
            "Epoch: 2, Training Loss: 1.7601451873779297\n",
            "Epoch: 2, Training Loss: 1.7084988355636597\n",
            "Epoch: 2, Training Loss: 1.64189875125885\n",
            "Epoch: 2, Training Loss: 1.499529480934143\n",
            "Epoch: 2, Training Loss: 1.7538987398147583\n",
            "Epoch: 2, Training Loss: 1.7793946266174316\n",
            "Epoch: 2, Training Loss: 1.820047378540039\n",
            "Epoch: 2, Training Loss: 1.8091939687728882\n",
            "Epoch: 2, Training Loss: 1.8302794694900513\n",
            "Epoch: 2, Training Loss: 1.7832269668579102\n",
            "Epoch: 2, Training Loss: 1.6336661577224731\n",
            "Epoch: 2, Training Loss: 1.852046012878418\n",
            "Epoch: 2, Training Loss: 1.7269420623779297\n",
            "Epoch: 2, Training Loss: 2.0851593017578125\n",
            "Epoch: 2, Training Loss: 1.9035708904266357\n",
            "Epoch: 2, Training Loss: 1.7610594034194946\n",
            "Epoch: 2, Training Loss: 1.6051098108291626\n",
            "Epoch: 2, Training Loss: 1.7982929944992065\n",
            "Epoch: 2, Training Loss: 1.9556214809417725\n",
            "Epoch: 2, Training Loss: 1.7317813634872437\n",
            "Epoch: 2, Training Loss: 1.5684574842453003\n",
            "Epoch: 2, Training Loss: 1.7822606563568115\n",
            "Epoch: 2, Training Loss: 1.7972941398620605\n",
            "Epoch: 2, Training Loss: 1.92237389087677\n",
            "Epoch: 2, Training Loss: 2.052487373352051\n",
            "Epoch: 2, Training Loss: 1.9738296270370483\n",
            "Epoch: 2, Training Loss: 1.6570923328399658\n",
            "Epoch: 2, Training Loss: 1.6480799913406372\n",
            "Epoch: 2, Training Loss: 1.6534276008605957\n",
            "Epoch: 2, Training Loss: 1.6661819219589233\n",
            "Epoch: 2, Training Loss: 1.8416513204574585\n",
            "Epoch: 2, Training Loss: 1.8102136850357056\n",
            "Epoch: 2, Training Loss: 1.6180834770202637\n",
            "Epoch: 2, Training Loss: 2.0818746089935303\n",
            "Epoch: 2, Training Loss: 1.911462664604187\n",
            "Epoch: 2, Training Loss: 1.6987968683242798\n",
            "Epoch: 2, Training Loss: 1.7913343906402588\n",
            "Epoch: 2, Training Loss: 1.677126169204712\n",
            "Epoch: 2, Training Loss: 1.7108427286148071\n",
            "Epoch: 2, Training Loss: 1.863865613937378\n",
            "Epoch: 2, Training Loss: 1.7682019472122192\n",
            "Epoch: 2, Training Loss: 1.6121805906295776\n",
            "Epoch: 2, Training Loss: 1.4917941093444824\n",
            "Epoch: 2, Training Loss: 1.8908387422561646\n",
            "Epoch: 2, Training Loss: 1.6911495923995972\n",
            "Epoch: 2, Training Loss: 1.8076162338256836\n",
            "Epoch: 2, Training Loss: 1.6944172382354736\n",
            "Epoch: 2, Training Loss: 1.581829309463501\n",
            "Epoch: 2, Validation Loss: 1.6522655487060547\n",
            "Epoch: 2, Validation Loss: 1.3172856569290161\n",
            "Epoch: 2, Validation Loss: 1.6844755411148071\n",
            "Epoch: 2, Validation Loss: 1.495613932609558\n",
            "Epoch: 2, Validation Loss: 1.8830159902572632\n",
            "Epoch: 2, Validation Loss: 1.8044217824935913\n",
            "Epoch: 2, Validation Loss: 1.6066055297851562\n",
            "Epoch: 2, Validation Loss: 1.7786489725112915\n",
            "Epoch: 2, Validation Loss: 1.6137986183166504\n",
            "Epoch: 2, Validation Loss: 1.678109049797058\n",
            "Epoch: 2, Validation Loss: 1.866362452507019\n",
            "Epoch: 2, Validation Loss: 1.4939626455307007\n",
            "Epoch: 2, Validation Loss: 1.8120218515396118\n",
            "Epoch: 2, Validation Loss: 1.741392731666565\n",
            "Epoch: 2, Validation Loss: 1.7105711698532104\n",
            "Epoch: 2, Validation Loss: 1.591945767402649\n",
            "Epoch: 2, Validation Loss: 1.4910117387771606\n",
            "Epoch: 2, Validation Loss: 1.6269687414169312\n",
            "Epoch: 2, Validation Loss: 1.484234094619751\n",
            "Epoch: 2, Validation Loss: 1.7195348739624023\n",
            "Epoch: 2, Validation Loss: 1.4349418878555298\n",
            "Epoch: 2, Validation Loss: 1.6672706604003906\n",
            "Epoch: 2, Validation Loss: 1.7899850606918335\n",
            "Epoch: 2, Validation Loss: 1.585896372795105\n",
            "Epoch: 2, Validation Loss: 1.552621841430664\n",
            "Epoch: 2, Validation Loss: 1.6182875633239746\n",
            "Epoch: 2, Validation Loss: 1.4424535036087036\n",
            "Epoch: 2, Validation Loss: 1.5642114877700806\n",
            "Epoch: 2, Validation Loss: 1.679192066192627\n",
            "Epoch: 2, Validation Loss: 1.8272360563278198\n",
            "Epoch: 2, Validation Loss: 1.6685243844985962\n",
            "Epoch: 2, Validation Loss: 1.557962417602539\n",
            "Epoch: 2, Validation Loss: 1.7991560697555542\n",
            "Epoch: 2, Validation Loss: 1.4043939113616943\n",
            "Epoch: 2, Validation Loss: 1.8140361309051514\n",
            "Epoch: 2, Validation Loss: 1.5652673244476318\n",
            "Epoch: 2, Validation Loss: 1.8236852884292603\n",
            "Epoch: 2, Validation Loss: 1.6009875535964966\n",
            "Epoch: 2, Validation Loss: 1.7151055335998535\n",
            "Epoch: 2, Validation Loss: 1.5417914390563965\n",
            "Epoch: 2, Validation Loss: 1.7495025396347046\n",
            "Epoch: 2, Validation Loss: 1.8366777896881104\n",
            "Epoch: 2, Validation Loss: 1.6119438409805298\n",
            "Epoch: 2, Validation Loss: 1.5799288749694824\n",
            "Epoch: 2, Validation Loss: 1.574007272720337\n",
            "Epoch: 2, Validation Loss: 1.7294574975967407\n",
            "Epoch: 2, Validation Loss: 1.6501779556274414\n",
            "Epoch: 2, Validation Loss: 1.6695791482925415\n",
            "Epoch: 2, Validation Loss: 1.485433578491211\n",
            "Epoch: 2, Validation Loss: 1.6327755451202393\n",
            "Epoch: 2, Validation Loss: 1.8986214399337769\n",
            "Epoch: 2, Validation Loss: 1.7493326663970947\n",
            "Epoch: 2, Validation Loss: 1.3153376579284668\n",
            "Epoch: 2, Validation Loss: 1.647449254989624\n",
            "Epoch: 2, Validation Loss: 1.6300255060195923\n",
            "Epoch: 2, Validation Loss: 1.6898618936538696\n",
            "Epoch: 2, Validation Loss: 1.7930622100830078\n",
            "Epoch: 2, Validation Loss: 1.6464132070541382\n",
            "Epoch: 2, Validation Loss: 1.4424859285354614\n",
            "Epoch: 2, Validation Loss: 1.5700504779815674\n",
            "Epoch: 2, Validation Loss: 1.704757571220398\n",
            "Epoch: 2, Validation Loss: 1.5315176248550415\n",
            "Epoch: 2, Validation Loss: 1.6550483703613281\n",
            "Epoch: 2, Validation Loss: 1.677268147468567\n",
            "Epoch: 2, Validation Loss: 1.5607346296310425\n",
            "Epoch: 2, Validation Loss: 1.711381196975708\n",
            "Epoch: 2, Validation Loss: 1.364670991897583\n",
            "Epoch: 2, Validation Loss: 1.6374244689941406\n",
            "Epoch: 2, Validation Loss: 1.623327612876892\n",
            "Epoch: 2, Validation Loss: 1.6393578052520752\n",
            "Epoch: 2, Validation Loss: 1.801368236541748\n",
            "Epoch: 2, Validation Loss: 1.4533369541168213\n",
            "Epoch: 2, Validation Loss: 1.8091440200805664\n",
            "Epoch: 2, Validation Loss: 1.6210025548934937\n",
            "Epoch: 2, Validation Loss: 1.4787477254867554\n",
            "Epoch: 2, Validation Loss: 1.6910384893417358\n",
            "Epoch: 2, Validation Loss: 1.4806402921676636\n",
            "Epoch: 2, Validation Loss: 1.6309064626693726\n",
            "Epoch: 2, Validation Loss: 1.9131650924682617\n",
            "Epoch: 2, Validation Loss: 1.6995760202407837\n",
            "Epoch: 2, Validation Loss: 1.667117953300476\n",
            "Epoch: 2, Validation Loss: 1.451695203781128\n",
            "Epoch: 2, Validation Loss: 1.4841010570526123\n",
            "Epoch: 2, Validation Loss: 1.691435694694519\n",
            "Epoch: 2, Validation Loss: 1.7822555303573608\n",
            "Epoch: 2, Validation Loss: 1.5497710704803467\n",
            "Epoch: 2, Validation Loss: 1.5452193021774292\n",
            "Epoch: 2, Validation Loss: 1.716685175895691\n",
            "Epoch: 2, Validation Loss: 1.6075325012207031\n",
            "Epoch: 2, Validation Loss: 1.5412081480026245\n",
            "Epoch: 2, Validation Loss: 1.6561251878738403\n",
            "Epoch: 2, Validation Loss: 1.7357020378112793\n",
            "Epoch: 2, Validation Loss: 1.4745078086853027\n",
            "Epoch: 2, Validation Loss: 1.743913173675537\n",
            "Epoch: 2, Validation Loss: 1.751121163368225\n",
            "Epoch: 2, Validation Loss: 1.616225242614746\n",
            "Epoch: 2, Validation Loss: 1.474447250366211\n",
            "Epoch: 2, Validation Loss: 1.4833896160125732\n",
            "Epoch: 2, Validation Loss: 1.5752537250518799\n",
            "Epoch: 2, Validation Loss: 1.5483345985412598\n",
            "Epoch: 2, Validation Loss: 1.5308074951171875\n",
            "Epoch: 2, Validation Loss: 1.7112308740615845\n",
            "Epoch: 2, Validation Loss: 1.7541664838790894\n",
            "Epoch: 2, Validation Loss: 1.3820255994796753\n",
            "Epoch: 2, Validation Loss: 1.5692678689956665\n",
            "Epoch: 2, Validation Loss: 1.7169041633605957\n",
            "Epoch: 2, Validation Loss: 1.7528468370437622\n",
            "Epoch: 2, Validation Loss: 1.7725025415420532\n",
            "Epoch: 2, Validation Loss: 1.683782935142517\n",
            "Epoch: 2, Validation Loss: 1.3620634078979492\n",
            "Epoch: 2, Validation Loss: 1.7547848224639893\n",
            "Epoch: 2, Validation Loss: 1.5145211219787598\n",
            "Epoch: 2, Validation Loss: 1.5157018899917603\n",
            "Epoch: 2, Validation Loss: 1.517089605331421\n",
            "Epoch: 2, Validation Loss: 1.641539216041565\n",
            "Epoch: 2, Validation Loss: 1.7461837530136108\n",
            "Epoch: 2, Validation Loss: 1.9115458726882935\n",
            "Epoch: 2, Validation Loss: 1.424013376235962\n",
            "Epoch: 2, Validation Loss: 1.8135863542556763\n",
            "Epoch: 2, Validation Loss: 1.4864397048950195\n",
            "Epoch: 2, Validation Loss: 1.74185049533844\n",
            "Epoch: 2, Validation Loss: 1.4470701217651367\n",
            "Epoch: 2, Validation Loss: 1.6647710800170898\n",
            "Epoch: 2, Validation Loss: 1.747800588607788\n",
            "Epoch: 2, Validation Loss: 1.661765217781067\n",
            "Epoch: 2, Validation Loss: 1.6755207777023315\n",
            "Epoch: 2, Validation Loss: 1.717536449432373\n",
            "Epoch: 2, Validation Loss: 1.5739600658416748\n",
            "Epoch: 2, Validation Loss: 1.7394487857818604\n",
            "Epoch: 2, Validation Loss: 1.9511977434158325\n",
            "Epoch: 2, Validation Loss: 1.7529131174087524\n",
            "Epoch: 2, Validation Loss: 1.29948091506958\n",
            "Epoch: 2, Validation Loss: 1.9540252685546875\n",
            "Epoch: 2, Validation Loss: 1.6454763412475586\n",
            "Epoch: 2, Validation Loss: 1.5387507677078247\n",
            "Epoch: 2, Validation Loss: 1.4877475500106812\n",
            "Epoch: 2, Validation Loss: 1.6449044942855835\n",
            "Epoch: 2, Validation Loss: 1.7884883880615234\n",
            "Epoch: 2, Validation Loss: 1.7495102882385254\n",
            "Epoch: 2, Validation Loss: 1.5520318746566772\n",
            "Epoch: 2, Validation Loss: 1.8450872898101807\n",
            "Epoch: 2, Validation Loss: 1.7189875841140747\n",
            "Epoch: 2, Validation Loss: 1.6169408559799194\n",
            "Epoch: 2, Validation Loss: 1.6151173114776611\n",
            "Epoch: 2, Validation Loss: 1.5412065982818604\n",
            "Epoch: 2, Validation Loss: 1.5201265811920166\n",
            "Epoch: 2, Validation Loss: 1.773767352104187\n",
            "Epoch: 2, Validation Loss: 1.6786085367202759\n",
            "Epoch: 2, Validation Loss: 1.815036416053772\n",
            "Epoch: 2, Validation Loss: 1.5665628910064697\n",
            "Epoch: 2, Validation Loss: 1.8718000650405884\n",
            "Epoch: 2, Validation Loss: 1.7582722902297974\n",
            "Epoch: 2, Validation Loss: 1.6546417474746704\n",
            "Epoch: 2, Validation Loss: 1.7437727451324463\n",
            "Epoch: 2, Validation Loss: 1.7581452131271362\n",
            "Epoch: 2, Validation Loss: 1.5619394779205322\n",
            "Epoch: 2, Validation Loss: 1.8285948038101196\n",
            "Epoch: 2, Validation Loss: 1.7604451179504395\n",
            "Epoch: 2, Validation Loss: 1.6375652551651\n",
            "Epoch: 2, Validation Loss: 1.797577977180481\n",
            "Epoch: 2, Validation Loss: 1.341273546218872\n",
            "Epoch: 2, Validation Loss: 1.7514983415603638\n",
            "Epoch: 2, Validation Loss: 1.868712067604065\n",
            "Epoch: 2, Validation Loss: 1.5101287364959717\n",
            "Epoch: 2, Validation Loss: 1.7791253328323364\n",
            "Epoch: 2, Validation Loss: 1.71080482006073\n",
            "Epoch: 2, Validation Loss: 1.3490458726882935\n",
            "Epoch: 2, Validation Loss: 1.5014417171478271\n",
            "Epoch: 2, Validation Loss: 1.5861754417419434\n",
            "Epoch: 2, Validation Loss: 1.5921748876571655\n",
            "Epoch: 2, Validation Loss: 1.7112317085266113\n",
            "Epoch: 2, Validation Loss: 1.5439693927764893\n",
            "Epoch: 2, Validation Loss: 1.7108798027038574\n",
            "Epoch: 2, Validation Loss: 1.461071252822876\n",
            "Epoch: 2, Validation Loss: 1.4940476417541504\n",
            "Epoch: 2, Validation Loss: 1.5872761011123657\n",
            "Epoch: 2, Validation Loss: 1.6550318002700806\n",
            "Epoch: 2, Validation Loss: 1.6146435737609863\n",
            "Epoch: 2, Validation Loss: 1.6989572048187256\n",
            "Epoch: 2, Validation Loss: 1.7242101430892944\n",
            "Epoch: 2, Validation Loss: 1.8178596496582031\n",
            "Epoch: 2, Validation Loss: 1.628144383430481\n",
            "Epoch: 2, Validation Loss: 1.4385052919387817\n",
            "Epoch: 2, Validation Loss: 1.5465514659881592\n",
            "Epoch: 2, Validation Loss: 1.5456539392471313\n",
            "Epoch: 2, Validation Loss: 1.9430900812149048\n",
            "Epoch: 2, Validation Loss: 1.5957636833190918\n",
            "Epoch: 2, Validation Loss: 1.9520595073699951\n",
            "Epoch: 2, Validation Loss: 1.5647917985916138\n",
            "Epoch: 2, Validation Loss: 1.5796152353286743\n",
            "Epoch: 2, Validation Loss: 1.5792200565338135\n",
            "Epoch: 2, Validation Loss: 1.3199025392532349\n",
            "Epoch: 2, Validation Loss: 1.4938931465148926\n",
            "Epoch: 2, Validation Loss: 1.6308130025863647\n",
            "Epoch: 2, Validation Loss: 1.6060538291931152\n",
            "Epoch: 2, Validation Loss: 1.6295075416564941\n",
            "Epoch: 2, Validation Loss: 1.463364601135254\n",
            "Epoch: 2, Validation Loss: 1.6783074140548706\n",
            "Epoch: 2, Validation Loss: 1.4933022260665894\n",
            "Epoch: 2, Validation Loss: 1.5597318410873413\n",
            "Epoch: 2, Validation Loss: 1.5357247591018677\n",
            "Epoch: 2, Validation Loss: 1.9178169965744019\n",
            "Epoch: 2, Validation Loss: 1.637786626815796\n",
            "Epoch: 2, Validation Loss: 1.7371878623962402\n",
            "Epoch: 2, Validation Loss: 1.7040441036224365\n",
            "Epoch: 2, Validation Loss: 1.7356528043746948\n",
            "Epoch: 2, Validation Loss: 1.5474821329116821\n",
            "Epoch: 2, Validation Loss: 1.7880909442901611\n",
            "Epoch: 2, Validation Loss: 1.675079107284546\n",
            "Epoch: 2, Validation Loss: 1.5385972261428833\n",
            "Epoch: 2, Validation Loss: 1.6082439422607422\n",
            "Epoch: 2, Validation Loss: 1.5848809480667114\n",
            "Epoch: 2, Validation Loss: 1.3417378664016724\n",
            "Epoch: 2, Validation Loss: 1.8169779777526855\n",
            "Epoch: 2, Validation Loss: 1.362596869468689\n",
            "Epoch: 2, Validation Loss: 1.753814458847046\n",
            "Epoch: 2, Validation Loss: 1.4822046756744385\n",
            "Epoch: 2, Validation Loss: 1.5979676246643066\n",
            "Epoch: 2, Validation Loss: 1.479970097541809\n",
            "Epoch: 2, Validation Loss: 1.5546512603759766\n",
            "Epoch: 2, Validation Loss: 1.527026891708374\n",
            "Epoch: 3\n",
            "------------------------------\n",
            "Epoch: 3, Training Loss: 1.6953433752059937\n",
            "Epoch: 3, Training Loss: 1.4029788970947266\n",
            "Epoch: 3, Training Loss: 1.5745515823364258\n",
            "Epoch: 3, Training Loss: 1.6192150115966797\n",
            "Epoch: 3, Training Loss: 1.5809135437011719\n",
            "Epoch: 3, Training Loss: 1.5711134672164917\n",
            "Epoch: 3, Training Loss: 1.5835012197494507\n",
            "Epoch: 3, Training Loss: 1.7102844715118408\n",
            "Epoch: 3, Training Loss: 1.6049352884292603\n",
            "Epoch: 3, Training Loss: 1.5371202230453491\n",
            "Epoch: 3, Training Loss: 1.739018201828003\n",
            "Epoch: 3, Training Loss: 1.5649337768554688\n",
            "Epoch: 3, Training Loss: 1.5429452657699585\n",
            "Epoch: 3, Training Loss: 1.5082283020019531\n",
            "Epoch: 3, Training Loss: 1.420486330986023\n",
            "Epoch: 3, Training Loss: 1.486575961112976\n",
            "Epoch: 3, Training Loss: 1.526554822921753\n",
            "Epoch: 3, Training Loss: 1.4581581354141235\n",
            "Epoch: 3, Training Loss: 1.919143557548523\n",
            "Epoch: 3, Training Loss: 1.5396113395690918\n",
            "Epoch: 3, Training Loss: 1.4106829166412354\n",
            "Epoch: 3, Training Loss: 1.608419418334961\n",
            "Epoch: 3, Training Loss: 1.775943636894226\n",
            "Epoch: 3, Training Loss: 1.3690003156661987\n",
            "Epoch: 3, Training Loss: 1.4742201566696167\n",
            "Epoch: 3, Training Loss: 1.432124137878418\n",
            "Epoch: 3, Training Loss: 1.5746431350708008\n",
            "Epoch: 3, Training Loss: 1.585475206375122\n",
            "Epoch: 3, Training Loss: 1.352835774421692\n",
            "Epoch: 3, Training Loss: 1.585201621055603\n",
            "Epoch: 3, Training Loss: 1.4584496021270752\n",
            "Epoch: 3, Training Loss: 1.4213272333145142\n",
            "Epoch: 3, Training Loss: 1.7673671245574951\n",
            "Epoch: 3, Training Loss: 1.6701713800430298\n",
            "Epoch: 3, Training Loss: 1.625409483909607\n",
            "Epoch: 3, Training Loss: 1.4777202606201172\n",
            "Epoch: 3, Training Loss: 1.58952796459198\n",
            "Epoch: 3, Training Loss: 1.5191200971603394\n",
            "Epoch: 3, Training Loss: 1.4065638780593872\n",
            "Epoch: 3, Training Loss: 1.471082329750061\n",
            "Epoch: 3, Training Loss: 1.157082200050354\n",
            "Epoch: 3, Training Loss: 1.4354819059371948\n",
            "Epoch: 3, Training Loss: 1.313036322593689\n",
            "Epoch: 3, Training Loss: 1.4823590517044067\n",
            "Epoch: 3, Training Loss: 1.7313975095748901\n",
            "Epoch: 3, Training Loss: 1.7730921506881714\n",
            "Epoch: 3, Training Loss: 1.574010968208313\n",
            "Epoch: 3, Training Loss: 1.530395269393921\n",
            "Epoch: 3, Training Loss: 1.4714879989624023\n",
            "Epoch: 3, Training Loss: 1.609919548034668\n",
            "Epoch: 3, Training Loss: 1.4064362049102783\n",
            "Epoch: 3, Training Loss: 1.4339009523391724\n",
            "Epoch: 3, Training Loss: 1.4232454299926758\n",
            "Epoch: 3, Training Loss: 1.777182936668396\n",
            "Epoch: 3, Training Loss: 1.3658984899520874\n",
            "Epoch: 3, Training Loss: 1.6210870742797852\n",
            "Epoch: 3, Training Loss: 1.5903432369232178\n",
            "Epoch: 3, Training Loss: 1.5683363676071167\n",
            "Epoch: 3, Training Loss: 1.596360445022583\n",
            "Epoch: 3, Training Loss: 1.5146591663360596\n",
            "Epoch: 3, Training Loss: 1.5880500078201294\n",
            "Epoch: 3, Training Loss: 1.5776153802871704\n",
            "Epoch: 3, Training Loss: 1.4676964282989502\n",
            "Epoch: 3, Training Loss: 1.5524282455444336\n",
            "Epoch: 3, Training Loss: 1.4540327787399292\n",
            "Epoch: 3, Training Loss: 1.5776218175888062\n",
            "Epoch: 3, Training Loss: 1.5072040557861328\n",
            "Epoch: 3, Training Loss: 1.5577392578125\n",
            "Epoch: 3, Training Loss: 1.4755743741989136\n",
            "Epoch: 3, Training Loss: 1.6456363201141357\n",
            "Epoch: 3, Training Loss: 1.70138680934906\n",
            "Epoch: 3, Training Loss: 1.5387656688690186\n",
            "Epoch: 3, Training Loss: 1.6337575912475586\n",
            "Epoch: 3, Training Loss: 1.4851329326629639\n",
            "Epoch: 3, Training Loss: 1.5339466333389282\n",
            "Epoch: 3, Training Loss: 1.7056523561477661\n",
            "Epoch: 3, Training Loss: 1.5905429124832153\n",
            "Epoch: 3, Training Loss: 1.3220771551132202\n",
            "Epoch: 3, Training Loss: 1.5868173837661743\n",
            "Epoch: 3, Training Loss: 1.369430661201477\n",
            "Epoch: 3, Training Loss: 1.4275208711624146\n",
            "Epoch: 3, Training Loss: 1.5649222135543823\n",
            "Epoch: 3, Training Loss: 1.4640039205551147\n",
            "Epoch: 3, Training Loss: 1.6031427383422852\n",
            "Epoch: 3, Training Loss: 1.5063142776489258\n",
            "Epoch: 3, Training Loss: 1.5957856178283691\n",
            "Epoch: 3, Training Loss: 1.6699345111846924\n",
            "Epoch: 3, Training Loss: 1.305721402168274\n",
            "Epoch: 3, Training Loss: 1.711517572402954\n",
            "Epoch: 3, Training Loss: 1.4471673965454102\n",
            "Epoch: 3, Training Loss: 1.5534216165542603\n",
            "Epoch: 3, Training Loss: 1.6409600973129272\n",
            "Epoch: 3, Training Loss: 1.4376999139785767\n",
            "Epoch: 3, Training Loss: 1.4970285892486572\n",
            "Epoch: 3, Training Loss: 1.5341905355453491\n",
            "Epoch: 3, Training Loss: 1.4616281986236572\n",
            "Epoch: 3, Training Loss: 1.5579181909561157\n",
            "Epoch: 3, Training Loss: 1.439246654510498\n",
            "Epoch: 3, Training Loss: 1.6299816370010376\n",
            "Epoch: 3, Training Loss: 1.7545965909957886\n",
            "Epoch: 3, Training Loss: 1.6712499856948853\n",
            "Epoch: 3, Training Loss: 1.6497553586959839\n",
            "Epoch: 3, Training Loss: 1.5223280191421509\n",
            "Epoch: 3, Training Loss: 1.61162531375885\n",
            "Epoch: 3, Training Loss: 1.4999130964279175\n",
            "Epoch: 3, Training Loss: 1.5290532112121582\n",
            "Epoch: 3, Training Loss: 1.3887311220169067\n",
            "Epoch: 3, Training Loss: 1.5901890993118286\n",
            "Epoch: 3, Training Loss: 1.6112171411514282\n",
            "Epoch: 3, Training Loss: 1.4445022344589233\n",
            "Epoch: 3, Training Loss: 1.6529422998428345\n",
            "Epoch: 3, Training Loss: 1.2497961521148682\n",
            "Epoch: 3, Training Loss: 1.5187246799468994\n",
            "Epoch: 3, Training Loss: 1.5308113098144531\n",
            "Epoch: 3, Training Loss: 1.520857810974121\n",
            "Epoch: 3, Training Loss: 1.7934247255325317\n",
            "Epoch: 3, Training Loss: 1.6703717708587646\n",
            "Epoch: 3, Training Loss: 1.8087583780288696\n",
            "Epoch: 3, Training Loss: 1.7082167863845825\n",
            "Epoch: 3, Training Loss: 1.5533881187438965\n",
            "Epoch: 3, Training Loss: 1.361707091331482\n",
            "Epoch: 3, Training Loss: 1.5159715414047241\n",
            "Epoch: 3, Training Loss: 1.4103729724884033\n",
            "Epoch: 3, Training Loss: 1.4322435855865479\n",
            "Epoch: 3, Training Loss: 1.7262990474700928\n",
            "Epoch: 3, Training Loss: 1.2907768487930298\n",
            "Epoch: 3, Training Loss: 1.221521019935608\n",
            "Epoch: 3, Training Loss: 1.5693813562393188\n",
            "Epoch: 3, Training Loss: 1.4221854209899902\n",
            "Epoch: 3, Training Loss: 1.6619645357131958\n",
            "Epoch: 3, Training Loss: 1.5290817022323608\n",
            "Epoch: 3, Training Loss: 1.4508575201034546\n",
            "Epoch: 3, Training Loss: 1.5734808444976807\n",
            "Epoch: 3, Training Loss: 1.9362024068832397\n",
            "Epoch: 3, Training Loss: 1.5759330987930298\n",
            "Epoch: 3, Training Loss: 1.412463665008545\n",
            "Epoch: 3, Training Loss: 1.2336664199829102\n",
            "Epoch: 3, Training Loss: 1.7372130155563354\n",
            "Epoch: 3, Training Loss: 1.6985186338424683\n",
            "Epoch: 3, Training Loss: 1.683531403541565\n",
            "Epoch: 3, Training Loss: 1.4022752046585083\n",
            "Epoch: 3, Training Loss: 1.6101858615875244\n",
            "Epoch: 3, Training Loss: 1.4264458417892456\n",
            "Epoch: 3, Training Loss: 1.5526000261306763\n",
            "Epoch: 3, Training Loss: 1.3738086223602295\n",
            "Epoch: 3, Training Loss: 1.4724425077438354\n",
            "Epoch: 3, Training Loss: 1.5839293003082275\n",
            "Epoch: 3, Training Loss: 1.4325532913208008\n",
            "Epoch: 3, Training Loss: 1.7962853908538818\n",
            "Epoch: 3, Training Loss: 1.6316289901733398\n",
            "Epoch: 3, Training Loss: 1.4862325191497803\n",
            "Epoch: 3, Training Loss: 1.4302536249160767\n",
            "Epoch: 3, Training Loss: 1.6830590963363647\n",
            "Epoch: 3, Training Loss: 1.4789084196090698\n",
            "Epoch: 3, Training Loss: 1.5366747379302979\n",
            "Epoch: 3, Training Loss: 1.2277294397354126\n",
            "Epoch: 3, Training Loss: 1.6076958179473877\n",
            "Epoch: 3, Training Loss: 1.7013263702392578\n",
            "Epoch: 3, Training Loss: 1.6855976581573486\n",
            "Epoch: 3, Training Loss: 1.4803065061569214\n",
            "Epoch: 3, Training Loss: 1.7187402248382568\n",
            "Epoch: 3, Training Loss: 1.8371680974960327\n",
            "Epoch: 3, Training Loss: 1.7801156044006348\n",
            "Epoch: 3, Training Loss: 1.8075019121170044\n",
            "Epoch: 3, Training Loss: 1.610175609588623\n",
            "Epoch: 3, Training Loss: 1.6550837755203247\n",
            "Epoch: 3, Training Loss: 1.5744597911834717\n",
            "Epoch: 3, Training Loss: 1.6873472929000854\n",
            "Epoch: 3, Training Loss: 1.8443934917449951\n",
            "Epoch: 3, Training Loss: 1.2818737030029297\n",
            "Epoch: 3, Training Loss: 1.6682984828948975\n",
            "Epoch: 3, Training Loss: 1.5543242692947388\n",
            "Epoch: 3, Training Loss: 1.6466416120529175\n",
            "Epoch: 3, Training Loss: 1.5445613861083984\n",
            "Epoch: 3, Training Loss: 1.4572476148605347\n",
            "Epoch: 3, Training Loss: 1.6308223009109497\n",
            "Epoch: 3, Training Loss: 1.5845409631729126\n",
            "Epoch: 3, Training Loss: 1.6057032346725464\n",
            "Epoch: 3, Training Loss: 1.4712586402893066\n",
            "Epoch: 3, Training Loss: 1.6724961996078491\n",
            "Epoch: 3, Training Loss: 1.5796911716461182\n",
            "Epoch: 3, Training Loss: 1.3819488286972046\n",
            "Epoch: 3, Training Loss: 1.6981137990951538\n",
            "Epoch: 3, Training Loss: 1.5684137344360352\n",
            "Epoch: 3, Training Loss: 1.385777235031128\n",
            "Epoch: 3, Training Loss: 1.4971882104873657\n",
            "Epoch: 3, Training Loss: 1.7916882038116455\n",
            "Epoch: 3, Training Loss: 1.5282526016235352\n",
            "Epoch: 3, Training Loss: 1.3554009199142456\n",
            "Epoch: 3, Training Loss: 1.3104019165039062\n",
            "Epoch: 3, Training Loss: 1.4342902898788452\n",
            "Epoch: 3, Training Loss: 1.5466035604476929\n",
            "Epoch: 3, Training Loss: 1.3438591957092285\n",
            "Epoch: 3, Training Loss: 1.3409457206726074\n",
            "Epoch: 3, Training Loss: 1.477316975593567\n",
            "Epoch: 3, Training Loss: 1.491370677947998\n",
            "Epoch: 3, Training Loss: 1.6910256147384644\n",
            "Epoch: 3, Training Loss: 1.7010201215744019\n",
            "Epoch: 3, Training Loss: 1.572670578956604\n",
            "Epoch: 3, Training Loss: 1.278781533241272\n",
            "Epoch: 3, Training Loss: 1.7786133289337158\n",
            "Epoch: 3, Training Loss: 1.6743539571762085\n",
            "Epoch: 3, Training Loss: 1.5722534656524658\n",
            "Epoch: 3, Training Loss: 1.5278117656707764\n",
            "Epoch: 3, Training Loss: 1.364497184753418\n",
            "Epoch: 3, Training Loss: 1.667568325996399\n",
            "Epoch: 3, Training Loss: 1.2602386474609375\n",
            "Epoch: 3, Training Loss: 1.7299433946609497\n",
            "Epoch: 3, Training Loss: 1.389242172241211\n",
            "Epoch: 3, Training Loss: 1.7546108961105347\n",
            "Epoch: 3, Training Loss: 1.5716508626937866\n",
            "Epoch: 3, Training Loss: 1.33505117893219\n",
            "Epoch: 3, Training Loss: 1.5929484367370605\n",
            "Epoch: 3, Training Loss: 1.7312660217285156\n",
            "Epoch: 3, Training Loss: 1.6493116617202759\n",
            "Epoch: 3, Training Loss: 1.619045615196228\n",
            "Epoch: 3, Training Loss: 1.7318414449691772\n",
            "Epoch: 3, Training Loss: 1.639585018157959\n",
            "Epoch: 3, Training Loss: 1.2622325420379639\n",
            "Epoch: 3, Training Loss: 1.5407699346542358\n",
            "Epoch: 3, Training Loss: 1.2425012588500977\n",
            "Epoch: 3, Training Loss: 1.3576215505599976\n",
            "Epoch: 3, Training Loss: 1.5922307968139648\n",
            "Epoch: 3, Training Loss: 1.6592061519622803\n",
            "Epoch: 3, Training Loss: 1.6710419654846191\n",
            "Epoch: 3, Training Loss: 1.5138624906539917\n",
            "Epoch: 3, Training Loss: 1.6803765296936035\n",
            "Epoch: 3, Training Loss: 1.507427453994751\n",
            "Epoch: 3, Training Loss: 1.5083436965942383\n",
            "Epoch: 3, Training Loss: 1.391754150390625\n",
            "Epoch: 3, Training Loss: 1.6459747552871704\n",
            "Epoch: 3, Training Loss: 1.6999127864837646\n",
            "Epoch: 3, Training Loss: 1.5472140312194824\n",
            "Epoch: 3, Training Loss: 1.410172462463379\n",
            "Epoch: 3, Training Loss: 1.4269037246704102\n",
            "Epoch: 3, Training Loss: 1.5218884944915771\n",
            "Epoch: 3, Training Loss: 1.770700454711914\n",
            "Epoch: 3, Training Loss: 1.6640980243682861\n",
            "Epoch: 3, Training Loss: 1.5199397802352905\n",
            "Epoch: 3, Training Loss: 1.5797654390335083\n",
            "Epoch: 3, Training Loss: 1.4505956172943115\n",
            "Epoch: 3, Training Loss: 1.6252273321151733\n",
            "Epoch: 3, Training Loss: 1.6608288288116455\n",
            "Epoch: 3, Training Loss: 1.761951208114624\n",
            "Epoch: 3, Training Loss: 1.4356063604354858\n",
            "Epoch: 3, Training Loss: 1.469380497932434\n",
            "Epoch: 3, Training Loss: 1.4374369382858276\n",
            "Epoch: 3, Training Loss: 1.7490785121917725\n",
            "Epoch: 3, Training Loss: 1.4312117099761963\n",
            "Epoch: 3, Training Loss: 1.637945294380188\n",
            "Epoch: 3, Training Loss: 1.5133576393127441\n",
            "Epoch: 3, Training Loss: 1.6468313932418823\n",
            "Epoch: 3, Training Loss: 1.5852468013763428\n",
            "Epoch: 3, Training Loss: 1.737460970878601\n",
            "Epoch: 3, Training Loss: 1.5435923337936401\n",
            "Epoch: 3, Training Loss: 1.6913031339645386\n",
            "Epoch: 3, Training Loss: 1.4546804428100586\n",
            "Epoch: 3, Training Loss: 1.4199334383010864\n",
            "Epoch: 3, Training Loss: 1.5122674703598022\n",
            "Epoch: 3, Training Loss: 1.7937018871307373\n",
            "Epoch: 3, Training Loss: 1.5717366933822632\n",
            "Epoch: 3, Training Loss: 1.6833211183547974\n",
            "Epoch: 3, Training Loss: 1.7831023931503296\n",
            "Epoch: 3, Training Loss: 1.6650824546813965\n",
            "Epoch: 3, Training Loss: 1.529123306274414\n",
            "Epoch: 3, Training Loss: 1.576522946357727\n",
            "Epoch: 3, Training Loss: 1.494935393333435\n",
            "Epoch: 3, Training Loss: 1.454275369644165\n",
            "Epoch: 3, Training Loss: 1.7131668329238892\n",
            "Epoch: 3, Training Loss: 1.5431492328643799\n",
            "Epoch: 3, Training Loss: 1.5183964967727661\n",
            "Epoch: 3, Training Loss: 1.6447534561157227\n",
            "Epoch: 3, Training Loss: 1.3381246328353882\n",
            "Epoch: 3, Training Loss: 1.3939876556396484\n",
            "Epoch: 3, Training Loss: 1.627112627029419\n",
            "Epoch: 3, Training Loss: 1.6668528318405151\n",
            "Epoch: 3, Training Loss: 1.4623253345489502\n",
            "Epoch: 3, Training Loss: 1.278395414352417\n",
            "Epoch: 3, Training Loss: 1.7462687492370605\n",
            "Epoch: 3, Training Loss: 1.8614563941955566\n",
            "Epoch: 3, Training Loss: 1.8561183214187622\n",
            "Epoch: 3, Training Loss: 1.6096978187561035\n",
            "Epoch: 3, Training Loss: 1.4872874021530151\n",
            "Epoch: 3, Training Loss: 1.333342432975769\n",
            "Epoch: 3, Training Loss: 1.608649492263794\n",
            "Epoch: 3, Training Loss: 1.2937690019607544\n",
            "Epoch: 3, Training Loss: 1.3363747596740723\n",
            "Epoch: 3, Training Loss: 1.5780689716339111\n",
            "Epoch: 3, Training Loss: 1.4617395401000977\n",
            "Epoch: 3, Training Loss: 1.516899824142456\n",
            "Epoch: 3, Training Loss: 1.3719545602798462\n",
            "Epoch: 3, Training Loss: 1.3742315769195557\n",
            "Epoch: 3, Training Loss: 1.5484589338302612\n",
            "Epoch: 3, Training Loss: 1.500421404838562\n",
            "Epoch: 3, Training Loss: 1.6987698078155518\n",
            "Epoch: 3, Training Loss: 1.5798616409301758\n",
            "Epoch: 3, Training Loss: 1.5065033435821533\n",
            "Epoch: 3, Training Loss: 2.034034013748169\n",
            "Epoch: 3, Training Loss: 1.762623906135559\n",
            "Epoch: 3, Training Loss: 1.381840705871582\n",
            "Epoch: 3, Training Loss: 1.511112093925476\n",
            "Epoch: 3, Training Loss: 1.812523365020752\n",
            "Epoch: 3, Training Loss: 1.5078173875808716\n",
            "Epoch: 3, Training Loss: 1.4254673719406128\n",
            "Epoch: 3, Training Loss: 1.6515297889709473\n",
            "Epoch: 3, Training Loss: 1.4033656120300293\n",
            "Epoch: 3, Training Loss: 1.3450020551681519\n",
            "Epoch: 3, Training Loss: 1.5468634366989136\n",
            "Epoch: 3, Training Loss: 1.4023504257202148\n",
            "Epoch: 3, Training Loss: 1.4296784400939941\n",
            "Epoch: 3, Training Loss: 1.794595718383789\n",
            "Epoch: 3, Training Loss: 1.6429365873336792\n",
            "Epoch: 3, Training Loss: 1.5613137483596802\n",
            "Epoch: 3, Training Loss: 1.6549136638641357\n",
            "Epoch: 3, Training Loss: 1.3241645097732544\n",
            "Epoch: 3, Training Loss: 1.5405125617980957\n",
            "Epoch: 3, Training Loss: 1.6973624229431152\n",
            "Epoch: 3, Training Loss: 1.522359848022461\n",
            "Epoch: 3, Training Loss: 1.6035722494125366\n",
            "Epoch: 3, Training Loss: 1.3930532932281494\n",
            "Epoch: 3, Training Loss: 1.8147594928741455\n",
            "Epoch: 3, Training Loss: 1.728183388710022\n",
            "Epoch: 3, Training Loss: 1.3711421489715576\n",
            "Epoch: 3, Training Loss: 1.6281487941741943\n",
            "Epoch: 3, Training Loss: 1.568486213684082\n",
            "Epoch: 3, Training Loss: 1.5199626684188843\n",
            "Epoch: 3, Training Loss: 1.6225769519805908\n",
            "Epoch: 3, Training Loss: 1.6059297323226929\n",
            "Epoch: 3, Training Loss: 1.4849071502685547\n",
            "Epoch: 3, Training Loss: 1.5316884517669678\n",
            "Epoch: 3, Training Loss: 1.6759666204452515\n",
            "Epoch: 3, Training Loss: 1.6078333854675293\n",
            "Epoch: 3, Training Loss: 1.5400793552398682\n",
            "Epoch: 3, Training Loss: 1.6499333381652832\n",
            "Epoch: 3, Training Loss: 1.7718894481658936\n",
            "Epoch: 3, Training Loss: 1.6835846900939941\n",
            "Epoch: 3, Training Loss: 1.6853224039077759\n",
            "Epoch: 3, Training Loss: 1.393983006477356\n",
            "Epoch: 3, Training Loss: 1.531388521194458\n",
            "Epoch: 3, Training Loss: 1.8850243091583252\n",
            "Epoch: 3, Training Loss: 1.387939453125\n",
            "Epoch: 3, Training Loss: 1.427761197090149\n",
            "Epoch: 3, Training Loss: 1.405328392982483\n",
            "Epoch: 3, Training Loss: 1.468914270401001\n",
            "Epoch: 3, Training Loss: 1.5296587944030762\n",
            "Epoch: 3, Training Loss: 1.5073143243789673\n",
            "Epoch: 3, Training Loss: 1.5908904075622559\n",
            "Epoch: 3, Training Loss: 1.3651784658432007\n",
            "Epoch: 3, Training Loss: 1.4336051940917969\n",
            "Epoch: 3, Training Loss: 1.5596122741699219\n",
            "Epoch: 3, Training Loss: 1.3369791507720947\n",
            "Epoch: 3, Training Loss: 1.6793891191482544\n",
            "Epoch: 3, Training Loss: 1.551445484161377\n",
            "Epoch: 3, Training Loss: 1.6777849197387695\n",
            "Epoch: 3, Training Loss: 1.8315091133117676\n",
            "Epoch: 3, Training Loss: 1.514912724494934\n",
            "Epoch: 3, Training Loss: 1.3720479011535645\n",
            "Epoch: 3, Training Loss: 1.7574480772018433\n",
            "Epoch: 3, Training Loss: 1.6961389780044556\n",
            "Epoch: 3, Training Loss: 1.2880562543869019\n",
            "Epoch: 3, Training Loss: 1.3090473413467407\n",
            "Epoch: 3, Training Loss: 1.437082290649414\n",
            "Epoch: 3, Training Loss: 1.7210252285003662\n",
            "Epoch: 3, Training Loss: 1.6363356113433838\n",
            "Epoch: 3, Training Loss: 1.5479153394699097\n",
            "Epoch: 3, Training Loss: 1.7308104038238525\n",
            "Epoch: 3, Training Loss: 1.6665030717849731\n",
            "Epoch: 3, Training Loss: 1.707786202430725\n",
            "Epoch: 3, Training Loss: 1.3206552267074585\n",
            "Epoch: 3, Training Loss: 1.3736907243728638\n",
            "Epoch: 3, Training Loss: 1.5608500242233276\n",
            "Epoch: 3, Training Loss: 1.5096333026885986\n",
            "Epoch: 3, Training Loss: 1.4435209035873413\n",
            "Epoch: 3, Training Loss: 1.9401347637176514\n",
            "Epoch: 3, Training Loss: 1.5575547218322754\n",
            "Epoch: 3, Training Loss: 1.3468455076217651\n",
            "Epoch: 3, Training Loss: 1.8395745754241943\n",
            "Epoch: 3, Training Loss: 1.4324357509613037\n",
            "Epoch: 3, Training Loss: 1.587253451347351\n",
            "Epoch: 3, Training Loss: 1.6882340908050537\n",
            "Epoch: 3, Training Loss: 1.745314359664917\n",
            "Epoch: 3, Training Loss: 1.3827145099639893\n",
            "Epoch: 3, Training Loss: 1.6547677516937256\n",
            "Epoch: 3, Training Loss: 1.6404502391815186\n",
            "Epoch: 3, Training Loss: 1.487681269645691\n",
            "Epoch: 3, Training Loss: 1.526997447013855\n",
            "Epoch: 3, Training Loss: 1.6493620872497559\n",
            "Epoch: 3, Training Loss: 1.6129953861236572\n",
            "Epoch: 3, Training Loss: 1.6393197774887085\n",
            "Epoch: 3, Training Loss: 1.4141294956207275\n",
            "Epoch: 3, Training Loss: 1.472612738609314\n",
            "Epoch: 3, Training Loss: 1.518797755241394\n",
            "Epoch: 3, Training Loss: 1.5607341527938843\n",
            "Epoch: 3, Training Loss: 1.234490156173706\n",
            "Epoch: 3, Training Loss: 1.4430627822875977\n",
            "Epoch: 3, Training Loss: 1.649992823600769\n",
            "Epoch: 3, Training Loss: 1.5148844718933105\n",
            "Epoch: 3, Training Loss: 1.61370849609375\n",
            "Epoch: 3, Training Loss: 1.6103986501693726\n",
            "Epoch: 3, Training Loss: 1.3541555404663086\n",
            "Epoch: 3, Training Loss: 1.5323822498321533\n",
            "Epoch: 3, Training Loss: 1.4234976768493652\n",
            "Epoch: 3, Training Loss: 1.6444977521896362\n",
            "Epoch: 3, Training Loss: 1.5195801258087158\n",
            "Epoch: 3, Training Loss: 1.7212401628494263\n",
            "Epoch: 3, Training Loss: 1.5776780843734741\n",
            "Epoch: 3, Training Loss: 1.6560462713241577\n",
            "Epoch: 3, Training Loss: 1.3394838571548462\n",
            "Epoch: 3, Training Loss: 1.5270870923995972\n",
            "Epoch: 3, Training Loss: 1.4717974662780762\n",
            "Epoch: 3, Training Loss: 1.4476984739303589\n",
            "Epoch: 3, Training Loss: 1.603740930557251\n",
            "Epoch: 3, Training Loss: 1.575156807899475\n",
            "Epoch: 3, Training Loss: 1.375603437423706\n",
            "Epoch: 3, Training Loss: 1.4203165769577026\n",
            "Epoch: 3, Training Loss: 1.474239468574524\n",
            "Epoch: 3, Training Loss: 1.6539785861968994\n",
            "Epoch: 3, Training Loss: 1.96017324924469\n",
            "Epoch: 3, Training Loss: 1.4668502807617188\n",
            "Epoch: 3, Training Loss: 1.8187363147735596\n",
            "Epoch: 3, Training Loss: 1.6728054285049438\n",
            "Epoch: 3, Training Loss: 1.366086483001709\n",
            "Epoch: 3, Training Loss: 1.5910472869873047\n",
            "Epoch: 3, Training Loss: 1.8194382190704346\n",
            "Epoch: 3, Training Loss: 1.381759524345398\n",
            "Epoch: 3, Training Loss: 1.463643193244934\n",
            "Epoch: 3, Training Loss: 1.7055200338363647\n",
            "Epoch: 3, Training Loss: 1.5949993133544922\n",
            "Epoch: 3, Training Loss: 1.7080998420715332\n",
            "Epoch: 3, Training Loss: 1.4524335861206055\n",
            "Epoch: 3, Training Loss: 1.3187979459762573\n",
            "Epoch: 3, Training Loss: 1.6784145832061768\n",
            "Epoch: 3, Training Loss: 1.4565342664718628\n",
            "Epoch: 3, Training Loss: 1.5427472591400146\n",
            "Epoch: 3, Training Loss: 1.3540970087051392\n",
            "Epoch: 3, Training Loss: 1.4489202499389648\n",
            "Epoch: 3, Training Loss: 1.3249174356460571\n",
            "Epoch: 3, Training Loss: 1.458814263343811\n",
            "Epoch: 3, Training Loss: 1.4873772859573364\n",
            "Epoch: 3, Training Loss: 1.2967039346694946\n",
            "Epoch: 3, Training Loss: 1.7538490295410156\n",
            "Epoch: 3, Training Loss: 1.4947168827056885\n",
            "Epoch: 3, Training Loss: 1.4350582361221313\n",
            "Epoch: 3, Training Loss: 1.826983094215393\n",
            "Epoch: 3, Training Loss: 1.4768537282943726\n",
            "Epoch: 3, Training Loss: 1.5609638690948486\n",
            "Epoch: 3, Training Loss: 1.343902826309204\n",
            "Epoch: 3, Training Loss: 1.4817304611206055\n",
            "Epoch: 3, Training Loss: 1.743213176727295\n",
            "Epoch: 3, Training Loss: 1.4793585538864136\n",
            "Epoch: 3, Training Loss: 1.347165584564209\n",
            "Epoch: 3, Training Loss: 1.480371356010437\n",
            "Epoch: 3, Training Loss: 1.726828694343567\n",
            "Epoch: 3, Training Loss: 1.5760438442230225\n",
            "Epoch: 3, Training Loss: 1.777886152267456\n",
            "Epoch: 3, Training Loss: 1.681571364402771\n",
            "Epoch: 3, Training Loss: 1.519229531288147\n",
            "Epoch: 3, Training Loss: 1.4486093521118164\n",
            "Epoch: 3, Training Loss: 1.752547264099121\n",
            "Epoch: 3, Training Loss: 1.6884214878082275\n",
            "Epoch: 3, Training Loss: 1.572153925895691\n",
            "Epoch: 3, Training Loss: 1.4528974294662476\n",
            "Epoch: 3, Training Loss: 1.445966124534607\n",
            "Epoch: 3, Training Loss: 1.5867736339569092\n",
            "Epoch: 3, Training Loss: 1.7431416511535645\n",
            "Epoch: 3, Training Loss: 1.6163541078567505\n",
            "Epoch: 3, Training Loss: 1.7603858709335327\n",
            "Epoch: 3, Training Loss: 1.4868611097335815\n",
            "Epoch: 3, Training Loss: 1.717332363128662\n",
            "Epoch: 3, Training Loss: 1.3664209842681885\n",
            "Epoch: 3, Training Loss: 1.6146159172058105\n",
            "Epoch: 3, Training Loss: 1.4701279401779175\n",
            "Epoch: 3, Training Loss: 1.7166415452957153\n",
            "Epoch: 3, Training Loss: 1.43471097946167\n",
            "Epoch: 3, Training Loss: 1.4718973636627197\n",
            "Epoch: 3, Training Loss: 1.4683622121810913\n",
            "Epoch: 3, Training Loss: 1.357837200164795\n",
            "Epoch: 3, Training Loss: 1.5923521518707275\n",
            "Epoch: 3, Training Loss: 1.459493637084961\n",
            "Epoch: 3, Training Loss: 1.5898125171661377\n",
            "Epoch: 3, Training Loss: 1.4247289896011353\n",
            "Epoch: 3, Training Loss: 1.4382656812667847\n",
            "Epoch: 3, Training Loss: 1.4920654296875\n",
            "Epoch: 3, Training Loss: 1.5131129026412964\n",
            "Epoch: 3, Training Loss: 1.5661447048187256\n",
            "Epoch: 3, Training Loss: 1.6652557849884033\n",
            "Epoch: 3, Training Loss: 1.4817893505096436\n",
            "Epoch: 3, Training Loss: 1.4650965929031372\n",
            "Epoch: 3, Training Loss: 1.352234125137329\n",
            "Epoch: 3, Training Loss: 1.290837049484253\n",
            "Epoch: 3, Training Loss: 1.496503472328186\n",
            "Epoch: 3, Training Loss: 1.6638379096984863\n",
            "Epoch: 3, Training Loss: 1.663870096206665\n",
            "Epoch: 3, Training Loss: 1.9483104944229126\n",
            "Epoch: 3, Training Loss: 1.6465201377868652\n",
            "Epoch: 3, Training Loss: 1.5648754835128784\n",
            "Epoch: 3, Training Loss: 1.593931794166565\n",
            "Epoch: 3, Training Loss: 1.2579714059829712\n",
            "Epoch: 3, Training Loss: 1.6234359741210938\n",
            "Epoch: 3, Training Loss: 1.4464534521102905\n",
            "Epoch: 3, Training Loss: 1.3885252475738525\n",
            "Epoch: 3, Training Loss: 1.4636430740356445\n",
            "Epoch: 3, Training Loss: 1.64005446434021\n",
            "Epoch: 3, Training Loss: 1.5002446174621582\n",
            "Epoch: 3, Training Loss: 1.3528437614440918\n",
            "Epoch: 3, Training Loss: 1.4802974462509155\n",
            "Epoch: 3, Training Loss: 1.2825255393981934\n",
            "Epoch: 3, Training Loss: 1.5686496496200562\n",
            "Epoch: 3, Training Loss: 1.7258882522583008\n",
            "Epoch: 3, Training Loss: 1.4323643445968628\n",
            "Epoch: 3, Training Loss: 1.5508521795272827\n",
            "Epoch: 3, Training Loss: 1.468442678451538\n",
            "Epoch: 3, Training Loss: 1.4199360609054565\n",
            "Epoch: 3, Training Loss: 1.7025189399719238\n",
            "Epoch: 3, Training Loss: 1.6901932954788208\n",
            "Epoch: 3, Training Loss: 1.769493579864502\n",
            "Epoch: 3, Training Loss: 1.303318977355957\n",
            "Epoch: 3, Training Loss: 1.7994351387023926\n",
            "Epoch: 3, Training Loss: 1.605454683303833\n",
            "Epoch: 3, Training Loss: 1.4184765815734863\n",
            "Epoch: 3, Training Loss: 1.6407477855682373\n",
            "Epoch: 3, Training Loss: 1.4615405797958374\n",
            "Epoch: 3, Training Loss: 1.5399353504180908\n",
            "Epoch: 3, Training Loss: 1.4408847093582153\n",
            "Epoch: 3, Training Loss: 1.6352791786193848\n",
            "Epoch: 3, Training Loss: 1.5636430978775024\n",
            "Epoch: 3, Training Loss: 1.5425363779067993\n",
            "Epoch: 3, Training Loss: 1.713384747505188\n",
            "Epoch: 3, Training Loss: 1.5028491020202637\n",
            "Epoch: 3, Training Loss: 1.7252519130706787\n",
            "Epoch: 3, Training Loss: 1.4421358108520508\n",
            "Epoch: 3, Training Loss: 1.7976312637329102\n",
            "Epoch: 3, Training Loss: 1.6486940383911133\n",
            "Epoch: 3, Training Loss: 1.4916248321533203\n",
            "Epoch: 3, Training Loss: 1.5016841888427734\n",
            "Epoch: 3, Training Loss: 1.5568146705627441\n",
            "Epoch: 3, Training Loss: 1.4454656839370728\n",
            "Epoch: 3, Training Loss: 1.8146764039993286\n",
            "Epoch: 3, Training Loss: 1.5492267608642578\n",
            "Epoch: 3, Training Loss: 1.5532885789871216\n",
            "Epoch: 3, Training Loss: 1.6332870721817017\n",
            "Epoch: 3, Training Loss: 1.8551442623138428\n",
            "Epoch: 3, Training Loss: 1.5577857494354248\n",
            "Epoch: 3, Training Loss: 1.4381933212280273\n",
            "Epoch: 3, Training Loss: 1.541163682937622\n",
            "Epoch: 3, Training Loss: 1.4216500520706177\n",
            "Epoch: 3, Training Loss: 1.721645712852478\n",
            "Epoch: 3, Training Loss: 1.717140793800354\n",
            "Epoch: 3, Training Loss: 1.6778593063354492\n",
            "Epoch: 3, Training Loss: 1.2655339241027832\n",
            "Epoch: 3, Training Loss: 1.5603053569793701\n",
            "Epoch: 3, Training Loss: 1.221009612083435\n",
            "Epoch: 3, Training Loss: 1.4718493223190308\n",
            "Epoch: 3, Training Loss: 1.6602838039398193\n",
            "Epoch: 3, Training Loss: 1.5793039798736572\n",
            "Epoch: 3, Training Loss: 1.465967059135437\n",
            "Epoch: 3, Training Loss: 1.4334107637405396\n",
            "Epoch: 3, Training Loss: 1.4778203964233398\n",
            "Epoch: 3, Training Loss: 1.4530973434448242\n",
            "Epoch: 3, Training Loss: 1.5737686157226562\n",
            "Epoch: 3, Training Loss: 1.6132687330245972\n",
            "Epoch: 3, Training Loss: 1.6943622827529907\n",
            "Epoch: 3, Training Loss: 1.5103211402893066\n",
            "Epoch: 3, Training Loss: 1.6320010423660278\n",
            "Epoch: 3, Training Loss: 1.5717352628707886\n",
            "Epoch: 3, Training Loss: 1.2135276794433594\n",
            "Epoch: 3, Training Loss: 1.5450884103775024\n",
            "Epoch: 3, Training Loss: 1.6228876113891602\n",
            "Epoch: 3, Training Loss: 1.4889382123947144\n",
            "Epoch: 3, Training Loss: 1.6576160192489624\n",
            "Epoch: 3, Training Loss: 1.4527645111083984\n",
            "Epoch: 3, Training Loss: 1.2683988809585571\n",
            "Epoch: 3, Training Loss: 1.6396414041519165\n",
            "Epoch: 3, Training Loss: 1.6193833351135254\n",
            "Epoch: 3, Training Loss: 1.4724318981170654\n",
            "Epoch: 3, Training Loss: 1.6969951391220093\n",
            "Epoch: 3, Training Loss: 1.5812093019485474\n",
            "Epoch: 3, Training Loss: 1.7801263332366943\n",
            "Epoch: 3, Training Loss: 1.6022381782531738\n",
            "Epoch: 3, Training Loss: 1.7238714694976807\n",
            "Epoch: 3, Training Loss: 1.4601958990097046\n",
            "Epoch: 3, Training Loss: 1.665832281112671\n",
            "Epoch: 3, Training Loss: 1.3793954849243164\n",
            "Epoch: 3, Training Loss: 1.4650555849075317\n",
            "Epoch: 3, Training Loss: 1.6951854228973389\n",
            "Epoch: 3, Training Loss: 1.366898775100708\n",
            "Epoch: 3, Training Loss: 1.670960783958435\n",
            "Epoch: 3, Training Loss: 1.5468626022338867\n",
            "Epoch: 3, Training Loss: 1.3355765342712402\n",
            "Epoch: 3, Training Loss: 1.5911495685577393\n",
            "Epoch: 3, Training Loss: 1.7127243280410767\n",
            "Epoch: 3, Training Loss: 1.5671968460083008\n",
            "Epoch: 3, Training Loss: 1.7301547527313232\n",
            "Epoch: 3, Training Loss: 1.5121610164642334\n",
            "Epoch: 3, Training Loss: 1.4089465141296387\n",
            "Epoch: 3, Training Loss: 1.4755911827087402\n",
            "Epoch: 3, Training Loss: 1.3908010721206665\n",
            "Epoch: 3, Training Loss: 1.424795389175415\n",
            "Epoch: 3, Training Loss: 1.5555673837661743\n",
            "Epoch: 3, Training Loss: 1.5768449306488037\n",
            "Epoch: 3, Training Loss: 1.5217252969741821\n",
            "Epoch: 3, Training Loss: 1.4474071264266968\n",
            "Epoch: 3, Training Loss: 1.6072925329208374\n",
            "Epoch: 3, Training Loss: 1.6871769428253174\n",
            "Epoch: 3, Training Loss: 1.578281044960022\n",
            "Epoch: 3, Training Loss: 1.6615225076675415\n",
            "Epoch: 3, Training Loss: 1.5036323070526123\n",
            "Epoch: 3, Training Loss: 1.4363046884536743\n",
            "Epoch: 3, Training Loss: 1.4460400342941284\n",
            "Epoch: 3, Training Loss: 1.8350218534469604\n",
            "Epoch: 3, Training Loss: 1.472154140472412\n",
            "Epoch: 3, Training Loss: 1.6455392837524414\n",
            "Epoch: 3, Training Loss: 1.3725950717926025\n",
            "Epoch: 3, Training Loss: 1.5737859010696411\n",
            "Epoch: 3, Training Loss: 1.4007751941680908\n",
            "Epoch: 3, Training Loss: 1.6209226846694946\n",
            "Epoch: 3, Training Loss: 1.3997817039489746\n",
            "Epoch: 3, Training Loss: 1.40163254737854\n",
            "Epoch: 3, Training Loss: 1.4092367887496948\n",
            "Epoch: 3, Training Loss: 1.3030685186386108\n",
            "Epoch: 3, Training Loss: 1.4112380743026733\n",
            "Epoch: 3, Training Loss: 1.6879602670669556\n",
            "Epoch: 3, Training Loss: 1.7595463991165161\n",
            "Epoch: 3, Training Loss: 1.8489161729812622\n",
            "Epoch: 3, Training Loss: 1.3308557271957397\n",
            "Epoch: 3, Training Loss: 1.6031442880630493\n",
            "Epoch: 3, Training Loss: 1.4591662883758545\n",
            "Epoch: 3, Training Loss: 1.5333507061004639\n",
            "Epoch: 3, Training Loss: 1.6892691850662231\n",
            "Epoch: 3, Training Loss: 1.791076898574829\n",
            "Epoch: 3, Training Loss: 1.665808081626892\n",
            "Epoch: 3, Training Loss: 1.504384994506836\n",
            "Epoch: 3, Training Loss: 1.5725609064102173\n",
            "Epoch: 3, Training Loss: 1.5255568027496338\n",
            "Epoch: 3, Training Loss: 1.54514479637146\n",
            "Epoch: 3, Training Loss: 1.5993584394454956\n",
            "Epoch: 3, Training Loss: 1.5536361932754517\n",
            "Epoch: 3, Training Loss: 1.4871680736541748\n",
            "Epoch: 3, Training Loss: 1.6665321588516235\n",
            "Epoch: 3, Training Loss: 1.4796582460403442\n",
            "Epoch: 3, Training Loss: 1.5285333395004272\n",
            "Epoch: 3, Training Loss: 1.4505276679992676\n",
            "Epoch: 3, Training Loss: 1.6364753246307373\n",
            "Epoch: 3, Training Loss: 1.6916135549545288\n",
            "Epoch: 3, Training Loss: 1.5657482147216797\n",
            "Epoch: 3, Training Loss: 1.3999476432800293\n",
            "Epoch: 3, Training Loss: 1.556073784828186\n",
            "Epoch: 3, Training Loss: 1.3838123083114624\n",
            "Epoch: 3, Training Loss: 1.5109738111495972\n",
            "Epoch: 3, Training Loss: 1.6408087015151978\n",
            "Epoch: 3, Training Loss: 1.7130213975906372\n",
            "Epoch: 3, Training Loss: 1.5162533521652222\n",
            "Epoch: 3, Training Loss: 1.6249175071716309\n",
            "Epoch: 3, Training Loss: 1.7175774574279785\n",
            "Epoch: 3, Training Loss: 1.382168173789978\n",
            "Epoch: 3, Training Loss: 1.3562555313110352\n",
            "Epoch: 3, Training Loss: 1.5855475664138794\n",
            "Epoch: 3, Training Loss: 1.5343294143676758\n",
            "Epoch: 3, Training Loss: 1.525968313217163\n",
            "Epoch: 3, Training Loss: 1.453898310661316\n",
            "Epoch: 3, Training Loss: 1.249585509300232\n",
            "Epoch: 3, Training Loss: 1.2218174934387207\n",
            "Epoch: 3, Training Loss: 1.4565669298171997\n",
            "Epoch: 3, Training Loss: 1.3797954320907593\n",
            "Epoch: 3, Training Loss: 1.3825228214263916\n",
            "Epoch: 3, Training Loss: 1.6147292852401733\n",
            "Epoch: 3, Training Loss: 1.5777426958084106\n",
            "Epoch: 3, Training Loss: 1.6801869869232178\n",
            "Epoch: 3, Training Loss: 1.6291799545288086\n",
            "Epoch: 3, Training Loss: 1.6230502128601074\n",
            "Epoch: 3, Training Loss: 1.222118854522705\n",
            "Epoch: 3, Training Loss: 1.6179468631744385\n",
            "Epoch: 3, Training Loss: 1.4697390794754028\n",
            "Epoch: 3, Training Loss: 1.7947760820388794\n",
            "Epoch: 3, Training Loss: 1.5420194864273071\n",
            "Epoch: 3, Training Loss: 1.4777804613113403\n",
            "Epoch: 3, Training Loss: 1.6373966932296753\n",
            "Epoch: 3, Training Loss: 1.8190886974334717\n",
            "Epoch: 3, Training Loss: 1.420655369758606\n",
            "Epoch: 3, Training Loss: 1.4154326915740967\n",
            "Epoch: 3, Training Loss: 1.6716630458831787\n",
            "Epoch: 3, Training Loss: 1.4700027704238892\n",
            "Epoch: 3, Training Loss: 1.3024040460586548\n",
            "Epoch: 3, Training Loss: 1.8346682786941528\n",
            "Epoch: 3, Training Loss: 1.3342885971069336\n",
            "Epoch: 3, Training Loss: 1.5053844451904297\n",
            "Epoch: 3, Training Loss: 1.5189261436462402\n",
            "Epoch: 3, Training Loss: 1.574601650238037\n",
            "Epoch: 3, Training Loss: 1.4451394081115723\n",
            "Epoch: 3, Training Loss: 1.5176568031311035\n",
            "Epoch: 3, Training Loss: 1.3717007637023926\n",
            "Epoch: 3, Training Loss: 1.276825189590454\n",
            "Epoch: 3, Training Loss: 1.6212941408157349\n",
            "Epoch: 3, Training Loss: 1.4747986793518066\n",
            "Epoch: 3, Training Loss: 1.3060355186462402\n",
            "Epoch: 3, Training Loss: 1.4046818017959595\n",
            "Epoch: 3, Training Loss: 1.3263459205627441\n",
            "Epoch: 3, Training Loss: 1.4073116779327393\n",
            "Epoch: 3, Training Loss: 1.6073358058929443\n",
            "Epoch: 3, Training Loss: 1.5558440685272217\n",
            "Epoch: 3, Training Loss: 1.5178771018981934\n",
            "Epoch: 3, Training Loss: 1.71700119972229\n",
            "Epoch: 3, Training Loss: 1.7410829067230225\n",
            "Epoch: 3, Training Loss: 1.308335781097412\n",
            "Epoch: 3, Training Loss: 1.4492686986923218\n",
            "Epoch: 3, Training Loss: 1.4336358308792114\n",
            "Epoch: 3, Training Loss: 1.297985553741455\n",
            "Epoch: 3, Training Loss: 1.3366492986679077\n",
            "Epoch: 3, Training Loss: 1.4803928136825562\n",
            "Epoch: 3, Training Loss: 1.421188235282898\n",
            "Epoch: 3, Training Loss: 1.3097914457321167\n",
            "Epoch: 3, Training Loss: 1.618857502937317\n",
            "Epoch: 3, Training Loss: 1.404697299003601\n",
            "Epoch: 3, Training Loss: 1.8243873119354248\n",
            "Epoch: 3, Training Loss: 1.4853312969207764\n",
            "Epoch: 3, Training Loss: 1.5412043333053589\n",
            "Epoch: 3, Training Loss: 1.372696042060852\n",
            "Epoch: 3, Training Loss: 1.221238613128662\n",
            "Epoch: 3, Training Loss: 2.0057153701782227\n",
            "Epoch: 3, Training Loss: 1.4434280395507812\n",
            "Epoch: 3, Training Loss: 1.6674219369888306\n",
            "Epoch: 3, Training Loss: 1.3515632152557373\n",
            "Epoch: 3, Training Loss: 1.4676977396011353\n",
            "Epoch: 3, Training Loss: 1.5705095529556274\n",
            "Epoch: 3, Training Loss: 1.8114057779312134\n",
            "Epoch: 3, Training Loss: 1.6915333271026611\n",
            "Epoch: 3, Training Loss: 1.6576608419418335\n",
            "Epoch: 3, Training Loss: 1.4847649335861206\n",
            "Epoch: 3, Training Loss: 1.6176239252090454\n",
            "Epoch: 3, Training Loss: 1.7399017810821533\n",
            "Epoch: 3, Training Loss: 1.7319945096969604\n",
            "Epoch: 3, Training Loss: 1.6164380311965942\n",
            "Epoch: 3, Training Loss: 1.6271182298660278\n",
            "Epoch: 3, Training Loss: 1.5199024677276611\n",
            "Epoch: 3, Training Loss: 1.6077220439910889\n",
            "Epoch: 3, Training Loss: 1.5216624736785889\n",
            "Epoch: 3, Training Loss: 1.470421314239502\n",
            "Epoch: 3, Training Loss: 1.6898950338363647\n",
            "Epoch: 3, Training Loss: 1.6740782260894775\n",
            "Epoch: 3, Training Loss: 1.4834370613098145\n",
            "Epoch: 3, Training Loss: 1.7472994327545166\n",
            "Epoch: 3, Training Loss: 1.6209242343902588\n",
            "Epoch: 3, Training Loss: 1.6631609201431274\n",
            "Epoch: 3, Training Loss: 1.5700534582138062\n",
            "Epoch: 3, Training Loss: 1.5815718173980713\n",
            "Epoch: 3, Training Loss: 1.3149648904800415\n",
            "Epoch: 3, Training Loss: 1.7633874416351318\n",
            "Epoch: 3, Training Loss: 1.7370294332504272\n",
            "Epoch: 3, Training Loss: 1.4556877613067627\n",
            "Epoch: 3, Training Loss: 1.5988572835922241\n",
            "Epoch: 3, Training Loss: 1.689529299736023\n",
            "Epoch: 3, Training Loss: 1.3216030597686768\n",
            "Epoch: 3, Training Loss: 1.7077094316482544\n",
            "Epoch: 3, Training Loss: 1.6462891101837158\n",
            "Epoch: 3, Training Loss: 1.5941256284713745\n",
            "Epoch: 3, Training Loss: 1.3615626096725464\n",
            "Epoch: 3, Training Loss: 1.4163599014282227\n",
            "Epoch: 3, Training Loss: 1.4967082738876343\n",
            "Epoch: 3, Training Loss: 1.5069082975387573\n",
            "Epoch: 3, Training Loss: 1.5824729204177856\n",
            "Epoch: 3, Training Loss: 1.4695446491241455\n",
            "Epoch: 3, Training Loss: 1.521822214126587\n",
            "Epoch: 3, Training Loss: 1.503532886505127\n",
            "Epoch: 3, Training Loss: 1.591715693473816\n",
            "Epoch: 3, Training Loss: 1.8002411127090454\n",
            "Epoch: 3, Training Loss: 1.6211237907409668\n",
            "Epoch: 3, Training Loss: 1.4085848331451416\n",
            "Epoch: 3, Training Loss: 1.3307327032089233\n",
            "Epoch: 3, Training Loss: 1.6132248640060425\n",
            "Epoch: 3, Training Loss: 1.8159561157226562\n",
            "Epoch: 3, Training Loss: 1.4566211700439453\n",
            "Epoch: 3, Training Loss: 1.3560432195663452\n",
            "Epoch: 3, Training Loss: 1.3754509687423706\n",
            "Epoch: 3, Training Loss: 1.3476920127868652\n",
            "Epoch: 3, Training Loss: 1.3695935010910034\n",
            "Epoch: 3, Training Loss: 1.4782085418701172\n",
            "Epoch: 3, Training Loss: 1.5206987857818604\n",
            "Epoch: 3, Training Loss: 1.5194904804229736\n",
            "Epoch: 3, Training Loss: 1.54158616065979\n",
            "Epoch: 3, Training Loss: 1.6984494924545288\n",
            "Epoch: 3, Training Loss: 1.6082475185394287\n",
            "Epoch: 3, Training Loss: 1.5732786655426025\n",
            "Epoch: 3, Training Loss: 1.545863151550293\n",
            "Epoch: 3, Training Loss: 1.5656492710113525\n",
            "Epoch: 3, Training Loss: 1.4619170427322388\n",
            "Epoch: 3, Training Loss: 1.5525527000427246\n",
            "Epoch: 3, Training Loss: 1.492314338684082\n",
            "Epoch: 3, Training Loss: 1.5902169942855835\n",
            "Epoch: 3, Training Loss: 1.9102554321289062\n",
            "Epoch: 3, Training Loss: 1.3807049989700317\n",
            "Epoch: 3, Training Loss: 1.5004174709320068\n",
            "Epoch: 3, Training Loss: 1.5512216091156006\n",
            "Epoch: 3, Training Loss: 1.75129234790802\n",
            "Epoch: 3, Training Loss: 1.5336850881576538\n",
            "Epoch: 3, Training Loss: 1.1842756271362305\n",
            "Epoch: 3, Training Loss: 1.7647136449813843\n",
            "Epoch: 3, Training Loss: 1.4399023056030273\n",
            "Epoch: 3, Training Loss: 1.3621529340744019\n",
            "Epoch: 3, Training Loss: 1.3214836120605469\n",
            "Epoch: 3, Training Loss: 1.5375452041625977\n",
            "Epoch: 3, Training Loss: 1.5862282514572144\n",
            "Epoch: 3, Training Loss: 1.4945675134658813\n",
            "Epoch: 3, Training Loss: 1.3666224479675293\n",
            "Epoch: 3, Training Loss: 1.3677562475204468\n",
            "Epoch: 3, Training Loss: 1.6190075874328613\n",
            "Epoch: 3, Training Loss: 1.3219486474990845\n",
            "Epoch: 3, Training Loss: 1.5449488162994385\n",
            "Epoch: 3, Training Loss: 1.5982158184051514\n",
            "Epoch: 3, Training Loss: 1.4775123596191406\n",
            "Epoch: 3, Training Loss: 1.5747480392456055\n",
            "Epoch: 3, Training Loss: 1.435725450515747\n",
            "Epoch: 3, Training Loss: 1.4134334325790405\n",
            "Epoch: 3, Training Loss: 1.4995981454849243\n",
            "Epoch: 3, Training Loss: 1.4424647092819214\n",
            "Epoch: 3, Training Loss: 1.400284767150879\n",
            "Epoch: 3, Training Loss: 1.5700637102127075\n",
            "Epoch: 3, Training Loss: 1.362443208694458\n",
            "Epoch: 3, Training Loss: 1.597080111503601\n",
            "Epoch: 3, Training Loss: 1.2741668224334717\n",
            "Epoch: 3, Training Loss: 1.5762072801589966\n",
            "Epoch: 3, Training Loss: 1.6077885627746582\n",
            "Epoch: 3, Training Loss: 1.6518609523773193\n",
            "Epoch: 3, Training Loss: 1.392466425895691\n",
            "Epoch: 3, Training Loss: 1.372144103050232\n",
            "Epoch: 3, Training Loss: 1.5208126306533813\n",
            "Epoch: 3, Training Loss: 1.3282572031021118\n",
            "Epoch: 3, Training Loss: 1.741631269454956\n",
            "Epoch: 3, Training Loss: 1.4448586702346802\n",
            "Epoch: 3, Training Loss: 1.3619431257247925\n",
            "Epoch: 3, Training Loss: 1.5647433996200562\n",
            "Epoch: 3, Training Loss: 1.4648805856704712\n",
            "Epoch: 3, Training Loss: 1.4427732229232788\n",
            "Epoch: 3, Training Loss: 1.5659738779067993\n",
            "Epoch: 3, Training Loss: 1.7298507690429688\n",
            "Epoch: 3, Training Loss: 1.5923322439193726\n",
            "Epoch: 3, Training Loss: 1.4974138736724854\n",
            "Epoch: 3, Training Loss: 1.4670995473861694\n",
            "Epoch: 3, Training Loss: 1.5007834434509277\n",
            "Epoch: 3, Training Loss: 1.6362131834030151\n",
            "Epoch: 3, Training Loss: 1.5554567575454712\n",
            "Epoch: 3, Training Loss: 1.4511040449142456\n",
            "Epoch: 3, Training Loss: 1.591949462890625\n",
            "Epoch: 3, Training Loss: 1.3042365312576294\n",
            "Epoch: 3, Training Loss: 1.1817598342895508\n",
            "Epoch: 3, Training Loss: 1.7556936740875244\n",
            "Epoch: 3, Training Loss: 1.4302783012390137\n",
            "Epoch: 3, Training Loss: 1.3342645168304443\n",
            "Epoch: 3, Training Loss: 1.4220393896102905\n",
            "Epoch: 3, Training Loss: 1.54922616481781\n",
            "Epoch: 3, Training Loss: 1.3298497200012207\n",
            "Epoch: 3, Training Loss: 1.4677330255508423\n",
            "Epoch: 3, Training Loss: 1.6080337762832642\n",
            "Epoch: 3, Training Loss: 1.3865243196487427\n",
            "Epoch: 3, Training Loss: 1.421286702156067\n",
            "Epoch: 3, Training Loss: 1.2936218976974487\n",
            "Epoch: 3, Training Loss: 1.3425499200820923\n",
            "Epoch: 3, Training Loss: 1.5458251237869263\n",
            "Epoch: 3, Training Loss: 1.5946638584136963\n",
            "Epoch: 3, Training Loss: 1.6109944581985474\n",
            "Epoch: 3, Training Loss: 1.737241506576538\n",
            "Epoch: 3, Training Loss: 1.5832023620605469\n",
            "Epoch: 3, Training Loss: 1.5834342241287231\n",
            "Epoch: 3, Training Loss: 1.5271685123443604\n",
            "Epoch: 3, Training Loss: 1.41287100315094\n",
            "Epoch: 3, Training Loss: 1.861824870109558\n",
            "Epoch: 3, Training Loss: 1.7802977561950684\n",
            "Epoch: 3, Training Loss: 1.590692162513733\n",
            "Epoch: 3, Training Loss: 1.2593910694122314\n",
            "Epoch: 3, Training Loss: 1.7929388284683228\n",
            "Epoch: 3, Training Loss: 1.4586626291275024\n",
            "Epoch: 3, Training Loss: 1.616389274597168\n",
            "Epoch: 3, Training Loss: 1.509463906288147\n",
            "Epoch: 3, Training Loss: 1.694416880607605\n",
            "Epoch: 3, Training Loss: 1.475292444229126\n",
            "Epoch: 3, Training Loss: 1.621804118156433\n",
            "Epoch: 3, Training Loss: 1.2889114618301392\n",
            "Epoch: 3, Training Loss: 1.5840967893600464\n",
            "Epoch: 3, Training Loss: 1.261794924736023\n",
            "Epoch: 3, Training Loss: 1.5866172313690186\n",
            "Epoch: 3, Training Loss: 1.702096939086914\n",
            "Epoch: 3, Training Loss: 1.3213951587677002\n",
            "Epoch: 3, Training Loss: 1.4766291379928589\n",
            "Epoch: 3, Training Loss: 1.6931618452072144\n",
            "Epoch: 3, Training Loss: 1.6723257303237915\n",
            "Epoch: 3, Training Loss: 1.5683757066726685\n",
            "Epoch: 3, Training Loss: 1.617761254310608\n",
            "Epoch: 3, Training Loss: 1.416203260421753\n",
            "Epoch: 3, Training Loss: 1.4929982423782349\n",
            "Epoch: 3, Training Loss: 1.4459298849105835\n",
            "Epoch: 3, Training Loss: 1.6219924688339233\n",
            "Epoch: 3, Training Loss: 1.5426483154296875\n",
            "Epoch: 3, Training Loss: 1.402240514755249\n",
            "Epoch: 3, Training Loss: 1.706276535987854\n",
            "Epoch: 3, Training Loss: 1.5740352869033813\n",
            "Epoch: 3, Training Loss: 1.506094217300415\n",
            "Epoch: 3, Training Loss: 1.6550652980804443\n",
            "Epoch: 3, Training Loss: 1.203890085220337\n",
            "Epoch: 3, Training Loss: 1.5520881414413452\n",
            "Epoch: 3, Training Loss: 1.4227503538131714\n",
            "Epoch: 3, Training Loss: 1.483367919921875\n",
            "Epoch: 3, Training Loss: 1.3016548156738281\n",
            "Epoch: 3, Training Loss: 1.180090308189392\n",
            "Epoch: 3, Training Loss: 1.5470770597457886\n",
            "Epoch: 3, Training Loss: 1.8104853630065918\n",
            "Epoch: 3, Training Loss: 1.6042543649673462\n",
            "Epoch: 3, Training Loss: 1.4755464792251587\n",
            "Epoch: 3, Training Loss: 1.2183904647827148\n",
            "Epoch: 3, Training Loss: 1.5106785297393799\n",
            "Epoch: 3, Training Loss: 1.600933313369751\n",
            "Epoch: 3, Training Loss: 1.208068609237671\n",
            "Epoch: 3, Training Loss: 1.3979748487472534\n",
            "Epoch: 3, Training Loss: 1.5676734447479248\n",
            "Epoch: 3, Training Loss: 1.492220163345337\n",
            "Epoch: 3, Training Loss: 1.2410376071929932\n",
            "Epoch: 3, Training Loss: 1.6609736680984497\n",
            "Epoch: 3, Training Loss: 1.551543951034546\n",
            "Epoch: 3, Training Loss: 1.6975618600845337\n",
            "Epoch: 3, Training Loss: 1.3211437463760376\n",
            "Epoch: 3, Training Loss: 1.4389058351516724\n",
            "Epoch: 3, Training Loss: 1.5689585208892822\n",
            "Epoch: 3, Training Loss: 1.5132333040237427\n",
            "Epoch: 3, Training Loss: 1.6488844156265259\n",
            "Epoch: 3, Training Loss: 1.3632073402404785\n",
            "Epoch: 3, Training Loss: 1.5997716188430786\n",
            "Epoch: 3, Training Loss: 1.5235120058059692\n",
            "Epoch: 3, Training Loss: 1.5996617078781128\n",
            "Epoch: 3, Training Loss: 1.5749624967575073\n",
            "Epoch: 3, Training Loss: 1.5154509544372559\n",
            "Epoch: 3, Training Loss: 1.4933006763458252\n",
            "Epoch: 3, Training Loss: 1.3677552938461304\n",
            "Epoch: 3, Training Loss: 1.3666878938674927\n",
            "Epoch: 3, Training Loss: 1.4053975343704224\n",
            "Epoch: 3, Training Loss: 1.5596282482147217\n",
            "Epoch: 3, Training Loss: 1.4865875244140625\n",
            "Epoch: 3, Training Loss: 1.4847849607467651\n",
            "Epoch: 3, Training Loss: 1.5082253217697144\n",
            "Epoch: 3, Training Loss: 1.4218961000442505\n",
            "Epoch: 3, Training Loss: 1.4843488931655884\n",
            "Epoch: 3, Training Loss: 1.566890835762024\n",
            "Epoch: 3, Training Loss: 1.6435627937316895\n",
            "Epoch: 3, Training Loss: 1.3712788820266724\n",
            "Epoch: 3, Training Loss: 1.3298842906951904\n",
            "Epoch: 3, Training Loss: 1.5653891563415527\n",
            "Epoch: 3, Training Loss: 1.3511077165603638\n",
            "Epoch: 3, Training Loss: 1.3410483598709106\n",
            "Epoch: 3, Training Loss: 1.4237229824066162\n",
            "Epoch: 3, Training Loss: 1.3843309879302979\n",
            "Epoch: 3, Training Loss: 1.6831698417663574\n",
            "Epoch: 3, Training Loss: 1.5776923894882202\n",
            "Epoch: 3, Training Loss: 1.5904295444488525\n",
            "Epoch: 3, Training Loss: 1.6471062898635864\n",
            "Epoch: 3, Training Loss: 1.4406789541244507\n",
            "Epoch: 3, Training Loss: 1.2461615800857544\n",
            "Epoch: 3, Training Loss: 1.6162453889846802\n",
            "Epoch: 3, Training Loss: 1.5772217512130737\n",
            "Epoch: 3, Training Loss: 1.4525009393692017\n",
            "Epoch: 3, Training Loss: 1.4009850025177002\n",
            "Epoch: 3, Training Loss: 1.4179104566574097\n",
            "Epoch: 3, Training Loss: 1.4395110607147217\n",
            "Epoch: 3, Training Loss: 1.4316468238830566\n",
            "Epoch: 3, Training Loss: 1.3718268871307373\n",
            "Epoch: 3, Training Loss: 1.2538765668869019\n",
            "Epoch: 3, Training Loss: 1.53037428855896\n",
            "Epoch: 3, Training Loss: 1.4882866144180298\n",
            "Epoch: 3, Training Loss: 1.743064045906067\n",
            "Epoch: 3, Training Loss: 1.4557743072509766\n",
            "Epoch: 3, Training Loss: 1.2175548076629639\n",
            "Epoch: 3, Training Loss: 1.7265880107879639\n",
            "Epoch: 3, Training Loss: 1.4210940599441528\n",
            "Epoch: 3, Training Loss: 1.2407708168029785\n",
            "Epoch: 3, Training Loss: 1.5634201765060425\n",
            "Epoch: 3, Training Loss: 1.346286416053772\n",
            "Epoch: 3, Training Loss: 1.5544947385787964\n",
            "Epoch: 3, Training Loss: 1.4638187885284424\n",
            "Epoch: 3, Training Loss: 1.6528432369232178\n",
            "Epoch: 3, Training Loss: 1.5521100759506226\n",
            "Epoch: 3, Training Loss: 1.544978380203247\n",
            "Epoch: 3, Training Loss: 1.3582032918930054\n",
            "Epoch: 3, Training Loss: 1.4721200466156006\n",
            "Epoch: 3, Training Loss: 1.147598385810852\n",
            "Epoch: 3, Training Loss: 1.5503536462783813\n",
            "Epoch: 3, Training Loss: 1.4660117626190186\n",
            "Epoch: 3, Training Loss: 1.7384389638900757\n",
            "Epoch: 3, Training Loss: 1.4632459878921509\n",
            "Epoch: 3, Training Loss: 1.624312400817871\n",
            "Epoch: 3, Training Loss: 1.6215976476669312\n",
            "Epoch: 3, Training Loss: 1.1636998653411865\n",
            "Epoch: 3, Training Loss: 1.588789701461792\n",
            "Epoch: 3, Training Loss: 1.3603050708770752\n",
            "Epoch: 3, Training Loss: 1.3303613662719727\n",
            "Epoch: 3, Training Loss: 1.4152538776397705\n",
            "Epoch: 3, Training Loss: 1.0831758975982666\n",
            "Epoch: 3, Training Loss: 1.5040513277053833\n",
            "Epoch: 3, Training Loss: 1.8581271171569824\n",
            "Epoch: 3, Training Loss: 1.3548771142959595\n",
            "Epoch: 3, Training Loss: 1.613440990447998\n",
            "Epoch: 3, Training Loss: 1.420701265335083\n",
            "Epoch: 3, Training Loss: 1.3205935955047607\n",
            "Epoch: 3, Training Loss: 1.4789499044418335\n",
            "Epoch: 3, Training Loss: 1.4770394563674927\n",
            "Epoch: 3, Training Loss: 1.2845829725265503\n",
            "Epoch: 3, Training Loss: 1.7448654174804688\n",
            "Epoch: 3, Training Loss: 1.307870864868164\n",
            "Epoch: 3, Training Loss: 1.576393961906433\n",
            "Epoch: 3, Training Loss: 1.5132462978363037\n",
            "Epoch: 3, Training Loss: 1.6092097759246826\n",
            "Epoch: 3, Training Loss: 1.4926073551177979\n",
            "Epoch: 3, Training Loss: 1.6287964582443237\n",
            "Epoch: 3, Training Loss: 1.530422329902649\n",
            "Epoch: 3, Training Loss: 1.4103240966796875\n",
            "Epoch: 3, Training Loss: 1.510975956916809\n",
            "Epoch: 3, Training Loss: 1.6389422416687012\n",
            "Epoch: 3, Training Loss: 1.561531901359558\n",
            "Epoch: 3, Training Loss: 1.4720170497894287\n",
            "Epoch: 3, Training Loss: 1.716425895690918\n",
            "Epoch: 3, Training Loss: 1.5152512788772583\n",
            "Epoch: 3, Training Loss: 1.661172866821289\n",
            "Epoch: 3, Training Loss: 1.4674841165542603\n",
            "Epoch: 3, Training Loss: 1.9320999383926392\n",
            "Epoch: 3, Training Loss: 1.3770689964294434\n",
            "Epoch: 3, Training Loss: 1.4528684616088867\n",
            "Epoch: 3, Training Loss: 1.4944264888763428\n",
            "Epoch: 3, Training Loss: 1.6345428228378296\n",
            "Epoch: 3, Training Loss: 1.567379355430603\n",
            "Epoch: 3, Training Loss: 1.531884789466858\n",
            "Epoch: 3, Training Loss: 1.3973519802093506\n",
            "Epoch: 3, Training Loss: 1.5823276042938232\n",
            "Epoch: 3, Training Loss: 1.4860278367996216\n",
            "Epoch: 3, Training Loss: 1.7415456771850586\n",
            "Epoch: 3, Training Loss: 1.395510196685791\n",
            "Epoch: 3, Training Loss: 1.6476242542266846\n",
            "Epoch: 3, Training Loss: 1.6736328601837158\n",
            "Epoch: 3, Training Loss: 1.29165518283844\n",
            "Epoch: 3, Training Loss: 1.5926196575164795\n",
            "Epoch: 3, Training Loss: 1.6836575269699097\n",
            "Epoch: 3, Training Loss: 1.429641604423523\n",
            "Epoch: 3, Training Loss: 1.4946092367172241\n",
            "Epoch: 3, Training Loss: 1.4522815942764282\n",
            "Epoch: 3, Training Loss: 1.2254385948181152\n",
            "Epoch: 3, Training Loss: 1.365768551826477\n",
            "Epoch: 3, Training Loss: 1.4373313188552856\n",
            "Epoch: 3, Training Loss: 1.7360107898712158\n",
            "Epoch: 3, Training Loss: 1.6272672414779663\n",
            "Epoch: 3, Training Loss: 1.6373592615127563\n",
            "Epoch: 3, Training Loss: 1.5231934785842896\n",
            "Epoch: 3, Training Loss: 1.5726265907287598\n",
            "Epoch: 3, Training Loss: 1.5526903867721558\n",
            "Epoch: 3, Training Loss: 1.7091392278671265\n",
            "Epoch: 3, Training Loss: 1.7904528379440308\n",
            "Epoch: 3, Training Loss: 1.403604507446289\n",
            "Epoch: 3, Training Loss: 1.6432703733444214\n",
            "Epoch: 3, Training Loss: 1.3540699481964111\n",
            "Epoch: 3, Training Loss: 1.3725152015686035\n",
            "Epoch: 3, Training Loss: 1.3758251667022705\n",
            "Epoch: 3, Training Loss: 1.4018150568008423\n",
            "Epoch: 3, Training Loss: 1.3781951665878296\n",
            "Epoch: 3, Training Loss: 1.4066296815872192\n",
            "Epoch: 3, Training Loss: 1.725614070892334\n",
            "Epoch: 3, Training Loss: 1.5245277881622314\n",
            "Epoch: 3, Training Loss: 1.3963879346847534\n",
            "Epoch: 3, Training Loss: 1.3509191274642944\n",
            "Epoch: 3, Training Loss: 1.4811758995056152\n",
            "Epoch: 3, Training Loss: 1.746975302696228\n",
            "Epoch: 3, Training Loss: 1.5549123287200928\n",
            "Epoch: 3, Training Loss: 1.587733268737793\n",
            "Epoch: 3, Training Loss: 1.644372820854187\n",
            "Epoch: 3, Training Loss: 1.5762983560562134\n",
            "Epoch: 3, Training Loss: 1.7132240533828735\n",
            "Epoch: 3, Training Loss: 1.5500104427337646\n",
            "Epoch: 3, Training Loss: 1.6092761754989624\n",
            "Epoch: 3, Training Loss: 1.699180006980896\n",
            "Epoch: 3, Training Loss: 1.6661852598190308\n",
            "Epoch: 3, Training Loss: 1.5391572713851929\n",
            "Epoch: 3, Training Loss: 1.4328597784042358\n",
            "Epoch: 3, Training Loss: 1.5884850025177002\n",
            "Epoch: 3, Training Loss: 1.805798888206482\n",
            "Epoch: 3, Training Loss: 1.408983588218689\n",
            "Epoch: 3, Training Loss: 1.4263920783996582\n",
            "Epoch: 3, Training Loss: 1.3607925176620483\n",
            "Epoch: 3, Training Loss: 1.5161041021347046\n",
            "Epoch: 3, Training Loss: 1.2946090698242188\n",
            "Epoch: 3, Training Loss: 1.4768857955932617\n",
            "Epoch: 3, Training Loss: 1.3886719942092896\n",
            "Epoch: 3, Training Loss: 1.4090553522109985\n",
            "Epoch: 3, Training Loss: 1.547404170036316\n",
            "Epoch: 3, Training Loss: 1.3967829942703247\n",
            "Epoch: 3, Training Loss: 1.6962050199508667\n",
            "Epoch: 3, Training Loss: 1.7785389423370361\n",
            "Epoch: 3, Training Loss: 1.7337830066680908\n",
            "Epoch: 3, Training Loss: 1.5704278945922852\n",
            "Epoch: 3, Training Loss: 1.3388673067092896\n",
            "Epoch: 3, Training Loss: 1.4573534727096558\n",
            "Epoch: 3, Training Loss: 1.8551061153411865\n",
            "Epoch: 3, Training Loss: 1.4047913551330566\n",
            "Epoch: 3, Training Loss: 1.752246379852295\n",
            "Epoch: 3, Training Loss: 1.4630213975906372\n",
            "Epoch: 3, Training Loss: 1.4941951036453247\n",
            "Epoch: 3, Training Loss: 1.1868222951889038\n",
            "Epoch: 3, Training Loss: 1.3162204027175903\n",
            "Epoch: 3, Training Loss: 1.36362886428833\n",
            "Epoch: 3, Training Loss: 1.421287178993225\n",
            "Epoch: 3, Training Loss: 1.3440711498260498\n",
            "Epoch: 3, Training Loss: 1.8291703462600708\n",
            "Epoch: 3, Training Loss: 1.45937180519104\n",
            "Epoch: 3, Training Loss: 1.7856597900390625\n",
            "Epoch: 3, Training Loss: 1.4936128854751587\n",
            "Epoch: 3, Training Loss: 1.5198163986206055\n",
            "Epoch: 3, Training Loss: 1.451431155204773\n",
            "Epoch: 3, Training Loss: 1.334591269493103\n",
            "Epoch: 3, Training Loss: 1.57941472530365\n",
            "Epoch: 3, Training Loss: 1.4879207611083984\n",
            "Epoch: 3, Training Loss: 1.502943992614746\n",
            "Epoch: 3, Training Loss: 1.3546897172927856\n",
            "Epoch: 3, Training Loss: 1.4160593748092651\n",
            "Epoch: 3, Training Loss: 1.258052945137024\n",
            "Epoch: 3, Training Loss: 1.590104579925537\n",
            "Epoch: 3, Training Loss: 1.4119257926940918\n",
            "Epoch: 3, Training Loss: 1.359870195388794\n",
            "Epoch: 3, Training Loss: 1.242416501045227\n",
            "Epoch: 3, Training Loss: 1.7975233793258667\n",
            "Epoch: 3, Training Loss: 1.5075812339782715\n",
            "Epoch: 3, Training Loss: 1.5857857465744019\n",
            "Epoch: 3, Training Loss: 1.4124987125396729\n",
            "Epoch: 3, Training Loss: 1.5452635288238525\n",
            "Epoch: 3, Training Loss: 1.5613785982131958\n",
            "Epoch: 3, Training Loss: 1.5798574686050415\n",
            "Epoch: 3, Training Loss: 1.4562499523162842\n",
            "Epoch: 3, Training Loss: 1.5342973470687866\n",
            "Epoch: 3, Training Loss: 1.347977876663208\n",
            "Epoch: 3, Training Loss: 1.4948084354400635\n",
            "Epoch: 3, Training Loss: 1.3951389789581299\n",
            "Epoch: 3, Training Loss: 1.695688247680664\n",
            "Epoch: 3, Training Loss: 1.5449334383010864\n",
            "Epoch: 3, Training Loss: 1.5273452997207642\n",
            "Epoch: 3, Training Loss: 1.4848557710647583\n",
            "Epoch: 3, Training Loss: 1.1838653087615967\n",
            "Epoch: 3, Training Loss: 1.3543306589126587\n",
            "Epoch: 3, Training Loss: 1.5545079708099365\n",
            "Epoch: 3, Training Loss: 1.4524418115615845\n",
            "Epoch: 3, Training Loss: 1.4496551752090454\n",
            "Epoch: 3, Training Loss: 1.5541987419128418\n",
            "Epoch: 3, Training Loss: 1.5859222412109375\n",
            "Epoch: 3, Training Loss: 1.5192089080810547\n",
            "Epoch: 3, Training Loss: 1.493045687675476\n",
            "Epoch: 3, Training Loss: 1.6685792207717896\n",
            "Epoch: 3, Training Loss: 1.1513447761535645\n",
            "Epoch: 3, Training Loss: 1.5025131702423096\n",
            "Epoch: 3, Training Loss: 1.3478612899780273\n",
            "Epoch: 3, Training Loss: 1.3590067625045776\n",
            "Epoch: 3, Training Loss: 1.526749849319458\n",
            "Epoch: 3, Training Loss: 1.2685496807098389\n",
            "Epoch: 3, Training Loss: 1.2496222257614136\n",
            "Epoch: 3, Training Loss: 1.467207908630371\n",
            "Epoch: 3, Training Loss: 1.5719778537750244\n",
            "Epoch: 3, Training Loss: 1.4852242469787598\n",
            "Epoch: 3, Training Loss: 1.4687362909317017\n",
            "Epoch: 3, Training Loss: 1.3978596925735474\n",
            "Epoch: 3, Training Loss: 1.622454285621643\n",
            "Epoch: 3, Training Loss: 1.2796355485916138\n",
            "Epoch: 3, Training Loss: 1.5294737815856934\n",
            "Epoch: 3, Training Loss: 1.4670034646987915\n",
            "Epoch: 3, Training Loss: 1.3225045204162598\n",
            "Epoch: 3, Training Loss: 1.592970371246338\n",
            "Epoch: 3, Training Loss: 1.4531495571136475\n",
            "Epoch: 3, Training Loss: 1.2618703842163086\n",
            "Epoch: 3, Training Loss: 1.404482126235962\n",
            "Epoch: 3, Training Loss: 1.372430443763733\n",
            "Epoch: 3, Training Loss: 1.309645652770996\n",
            "Epoch: 3, Training Loss: 1.278835415840149\n",
            "Epoch: 3, Training Loss: 1.6421886682510376\n",
            "Epoch: 3, Training Loss: 1.4888173341751099\n",
            "Epoch: 3, Training Loss: 1.1792501211166382\n",
            "Epoch: 3, Training Loss: 1.6996147632598877\n",
            "Epoch: 3, Training Loss: 1.3038195371627808\n",
            "Epoch: 3, Training Loss: 1.4308764934539795\n",
            "Epoch: 3, Training Loss: 1.578277349472046\n",
            "Epoch: 3, Training Loss: 1.5371057987213135\n",
            "Epoch: 3, Training Loss: 1.4552356004714966\n",
            "Epoch: 3, Training Loss: 1.433205485343933\n",
            "Epoch: 3, Training Loss: 1.411408543586731\n",
            "Epoch: 3, Training Loss: 1.5329375267028809\n",
            "Epoch: 3, Training Loss: 1.2356781959533691\n",
            "Epoch: 3, Training Loss: 1.71007239818573\n",
            "Epoch: 3, Training Loss: 1.6313790082931519\n",
            "Epoch: 3, Training Loss: 1.4545776844024658\n",
            "Epoch: 3, Training Loss: 1.393784999847412\n",
            "Epoch: 3, Training Loss: 1.78188157081604\n",
            "Epoch: 3, Training Loss: 1.4023405313491821\n",
            "Epoch: 3, Training Loss: 1.498865008354187\n",
            "Epoch: 3, Training Loss: 1.5821630954742432\n",
            "Epoch: 3, Training Loss: 1.434578776359558\n",
            "Epoch: 3, Training Loss: 1.521951675415039\n",
            "Epoch: 3, Training Loss: 1.4769132137298584\n",
            "Epoch: 3, Training Loss: 1.319433331489563\n",
            "Epoch: 3, Training Loss: 1.6063226461410522\n",
            "Epoch: 3, Training Loss: 1.6440590620040894\n",
            "Epoch: 3, Training Loss: 1.4119511842727661\n",
            "Epoch: 3, Training Loss: 1.3621701002120972\n",
            "Epoch: 3, Training Loss: 1.3407169580459595\n",
            "Epoch: 3, Training Loss: 1.3268088102340698\n",
            "Epoch: 3, Training Loss: 1.4719178676605225\n",
            "Epoch: 3, Training Loss: 1.4772417545318604\n",
            "Epoch: 3, Training Loss: 0.9707995057106018\n",
            "Epoch: 3, Training Loss: 1.4558897018432617\n",
            "Epoch: 3, Training Loss: 1.4739573001861572\n",
            "Epoch: 3, Training Loss: 1.5890730619430542\n",
            "Epoch: 3, Training Loss: 1.480777621269226\n",
            "Epoch: 3, Training Loss: 1.5351313352584839\n",
            "Epoch: 3, Training Loss: 1.4819059371948242\n",
            "Epoch: 3, Training Loss: 1.4549319744110107\n",
            "Epoch: 3, Training Loss: 1.4249298572540283\n",
            "Epoch: 3, Training Loss: 1.4498467445373535\n",
            "Epoch: 3, Training Loss: 1.3852548599243164\n",
            "Epoch: 3, Training Loss: 1.587525486946106\n",
            "Epoch: 3, Training Loss: 1.3429182767868042\n",
            "Epoch: 3, Training Loss: 1.5546754598617554\n",
            "Epoch: 3, Training Loss: 1.509404182434082\n",
            "Epoch: 3, Training Loss: 1.7014169692993164\n",
            "Epoch: 3, Training Loss: 1.508091926574707\n",
            "Epoch: 3, Training Loss: 1.3228672742843628\n",
            "Epoch: 3, Training Loss: 1.397635817527771\n",
            "Epoch: 3, Training Loss: 1.592703938484192\n",
            "Epoch: 3, Training Loss: 1.6012705564498901\n",
            "Epoch: 3, Training Loss: 1.5394407510757446\n",
            "Epoch: 3, Training Loss: 1.5675026178359985\n",
            "Epoch: 3, Training Loss: 1.5393673181533813\n",
            "Epoch: 3, Training Loss: 1.4528083801269531\n",
            "Epoch: 3, Training Loss: 1.7194548845291138\n",
            "Epoch: 3, Training Loss: 1.627440333366394\n",
            "Epoch: 3, Training Loss: 1.406593680381775\n",
            "Epoch: 3, Training Loss: 1.4168719053268433\n",
            "Epoch: 3, Training Loss: 1.5079749822616577\n",
            "Epoch: 3, Training Loss: 1.5546897649765015\n",
            "Epoch: 3, Training Loss: 1.3941534757614136\n",
            "Epoch: 3, Training Loss: 1.385039210319519\n",
            "Epoch: 3, Training Loss: 1.4980049133300781\n",
            "Epoch: 3, Training Loss: 1.1798690557479858\n",
            "Epoch: 3, Training Loss: 1.4572978019714355\n",
            "Epoch: 3, Training Loss: 1.4401662349700928\n",
            "Epoch: 3, Training Loss: 1.5939927101135254\n",
            "Epoch: 3, Training Loss: 1.5644673109054565\n",
            "Epoch: 3, Training Loss: 1.3505758047103882\n",
            "Epoch: 3, Training Loss: 1.3272556066513062\n",
            "Epoch: 3, Training Loss: 1.521700143814087\n",
            "Epoch: 3, Training Loss: 1.3889275789260864\n",
            "Epoch: 3, Training Loss: 1.4179577827453613\n",
            "Epoch: 3, Training Loss: 1.4478896856307983\n",
            "Epoch: 3, Training Loss: 1.3683230876922607\n",
            "Epoch: 3, Training Loss: 1.3332678079605103\n",
            "Epoch: 3, Training Loss: 1.5808192491531372\n",
            "Epoch: 3, Training Loss: 1.486672043800354\n",
            "Epoch: 3, Training Loss: 1.3635903596878052\n",
            "Epoch: 3, Training Loss: 1.7802643775939941\n",
            "Epoch: 3, Training Loss: 1.5693508386611938\n",
            "Epoch: 3, Training Loss: 1.5334008932113647\n",
            "Epoch: 3, Training Loss: 1.4996848106384277\n",
            "Epoch: 3, Training Loss: 1.332389235496521\n",
            "Epoch: 3, Training Loss: 1.523267388343811\n",
            "Epoch: 3, Training Loss: 1.4400523900985718\n",
            "Epoch: 3, Training Loss: 1.5975679159164429\n",
            "Epoch: 3, Training Loss: 1.2724831104278564\n",
            "Epoch: 3, Training Loss: 1.6076234579086304\n",
            "Epoch: 3, Training Loss: 1.6431561708450317\n",
            "Epoch: 3, Training Loss: 1.658310890197754\n",
            "Epoch: 3, Training Loss: 1.4663501977920532\n",
            "Epoch: 3, Training Loss: 1.536624550819397\n",
            "Epoch: 3, Training Loss: 1.4421790838241577\n",
            "Epoch: 3, Training Loss: 1.5276968479156494\n",
            "Epoch: 3, Training Loss: 1.5476479530334473\n",
            "Epoch: 3, Training Loss: 1.7490462064743042\n",
            "Epoch: 3, Training Loss: 1.5732383728027344\n",
            "Epoch: 3, Training Loss: 1.5542720556259155\n",
            "Epoch: 3, Training Loss: 1.4042532444000244\n",
            "Epoch: 3, Training Loss: 1.3793383836746216\n",
            "Epoch: 3, Training Loss: 1.509791374206543\n",
            "Epoch: 3, Training Loss: 1.579637050628662\n",
            "Epoch: 3, Training Loss: 1.2512096166610718\n",
            "Epoch: 3, Training Loss: 1.637515902519226\n",
            "Epoch: 3, Training Loss: 1.630173921585083\n",
            "Epoch: 3, Training Loss: 1.4395062923431396\n",
            "Epoch: 3, Training Loss: 1.359878659248352\n",
            "Epoch: 3, Training Loss: 1.2517660856246948\n",
            "Epoch: 3, Training Loss: 1.666599988937378\n",
            "Epoch: 3, Training Loss: 1.4817934036254883\n",
            "Epoch: 3, Training Loss: 1.4886480569839478\n",
            "Epoch: 3, Training Loss: 1.500474452972412\n",
            "Epoch: 3, Training Loss: 1.3961564302444458\n",
            "Epoch: 3, Training Loss: 1.2809367179870605\n",
            "Epoch: 3, Training Loss: 1.476513147354126\n",
            "Epoch: 3, Training Loss: 1.525542140007019\n",
            "Epoch: 3, Training Loss: 1.4689797163009644\n",
            "Epoch: 3, Training Loss: 1.4550055265426636\n",
            "Epoch: 3, Training Loss: 1.4604732990264893\n",
            "Epoch: 3, Training Loss: 1.5271143913269043\n",
            "Epoch: 3, Training Loss: 1.4151140451431274\n",
            "Epoch: 3, Training Loss: 1.3258635997772217\n",
            "Epoch: 3, Training Loss: 1.4790838956832886\n",
            "Epoch: 3, Training Loss: 1.3734970092773438\n",
            "Epoch: 3, Training Loss: 1.6144886016845703\n",
            "Epoch: 3, Training Loss: 1.4040074348449707\n",
            "Epoch: 3, Training Loss: 1.7546676397323608\n",
            "Epoch: 3, Training Loss: 1.5355316400527954\n",
            "Epoch: 3, Training Loss: 1.5702458620071411\n",
            "Epoch: 3, Training Loss: 1.8862214088439941\n",
            "Epoch: 3, Training Loss: 1.6918636560440063\n",
            "Epoch: 3, Training Loss: 1.4221664667129517\n",
            "Epoch: 3, Training Loss: 1.6724973917007446\n",
            "Epoch: 3, Training Loss: 1.4258419275283813\n",
            "Epoch: 3, Training Loss: 1.4179250001907349\n",
            "Epoch: 3, Training Loss: 1.4186066389083862\n",
            "Epoch: 3, Training Loss: 1.4954675436019897\n",
            "Epoch: 3, Training Loss: 1.6388897895812988\n",
            "Epoch: 3, Training Loss: 1.2696800231933594\n",
            "Epoch: 3, Training Loss: 1.450753927230835\n",
            "Epoch: 3, Training Loss: 1.2055976390838623\n",
            "Epoch: 3, Training Loss: 1.6171523332595825\n",
            "Epoch: 3, Training Loss: 1.535470724105835\n",
            "Epoch: 3, Training Loss: 1.4995160102844238\n",
            "Epoch: 3, Training Loss: 1.453520655632019\n",
            "Epoch: 3, Training Loss: 1.4312474727630615\n",
            "Epoch: 3, Training Loss: 1.285927414894104\n",
            "Epoch: 3, Training Loss: 1.3187410831451416\n",
            "Epoch: 3, Training Loss: 1.5680322647094727\n",
            "Epoch: 3, Training Loss: 1.675424575805664\n",
            "Epoch: 3, Training Loss: 1.4245150089263916\n",
            "Epoch: 3, Training Loss: 1.3480054140090942\n",
            "Epoch: 3, Training Loss: 1.5468931198120117\n",
            "Epoch: 3, Training Loss: 1.597210168838501\n",
            "Epoch: 3, Training Loss: 1.4358059167861938\n",
            "Epoch: 3, Training Loss: 1.422668695449829\n",
            "Epoch: 3, Training Loss: 1.4159092903137207\n",
            "Epoch: 3, Training Loss: 1.6099482774734497\n",
            "Epoch: 3, Training Loss: 1.495091199874878\n",
            "Epoch: 3, Training Loss: 1.4052785634994507\n",
            "Epoch: 3, Training Loss: 1.466652750968933\n",
            "Epoch: 3, Training Loss: 1.6027215719223022\n",
            "Epoch: 3, Training Loss: 1.4621009826660156\n",
            "Epoch: 3, Training Loss: 1.4570846557617188\n",
            "Epoch: 3, Training Loss: 1.5499601364135742\n",
            "Epoch: 3, Training Loss: 1.685397982597351\n",
            "Epoch: 3, Training Loss: 1.5332163572311401\n",
            "Epoch: 3, Training Loss: 1.3513541221618652\n",
            "Epoch: 3, Training Loss: 1.3011846542358398\n",
            "Epoch: 3, Training Loss: 1.5450408458709717\n",
            "Epoch: 3, Training Loss: 1.675352692604065\n",
            "Epoch: 3, Training Loss: 1.320670247077942\n",
            "Epoch: 3, Training Loss: 1.658375859260559\n",
            "Epoch: 3, Training Loss: 1.4749611616134644\n",
            "Epoch: 3, Training Loss: 1.462823510169983\n",
            "Epoch: 3, Training Loss: 1.3459675312042236\n",
            "Epoch: 3, Training Loss: 1.2869627475738525\n",
            "Epoch: 3, Training Loss: 1.3475499153137207\n",
            "Epoch: 3, Training Loss: 1.4171102046966553\n",
            "Epoch: 3, Training Loss: 1.2145304679870605\n",
            "Epoch: 3, Training Loss: 1.683491826057434\n",
            "Epoch: 3, Training Loss: 1.483672022819519\n",
            "Epoch: 3, Training Loss: 1.295283317565918\n",
            "Epoch: 3, Training Loss: 1.540860652923584\n",
            "Epoch: 3, Training Loss: 1.3874268531799316\n",
            "Epoch: 3, Training Loss: 1.597403883934021\n",
            "Epoch: 3, Training Loss: 1.4588792324066162\n",
            "Epoch: 3, Training Loss: 1.4311596155166626\n",
            "Epoch: 3, Training Loss: 1.310627818107605\n",
            "Epoch: 3, Training Loss: 1.5690202713012695\n",
            "Epoch: 3, Training Loss: 1.3535743951797485\n",
            "Epoch: 3, Training Loss: 1.315529704093933\n",
            "Epoch: 3, Training Loss: 1.5703128576278687\n",
            "Epoch: 3, Training Loss: 1.3917673826217651\n",
            "Epoch: 3, Training Loss: 1.7698135375976562\n",
            "Epoch: 3, Training Loss: 1.4857534170150757\n",
            "Epoch: 3, Training Loss: 1.473874807357788\n",
            "Epoch: 3, Training Loss: 1.214574933052063\n",
            "Epoch: 3, Training Loss: 1.5525048971176147\n",
            "Epoch: 3, Training Loss: 1.3843315839767456\n",
            "Epoch: 3, Training Loss: 1.46610426902771\n",
            "Epoch: 3, Training Loss: 1.4206740856170654\n",
            "Epoch: 3, Training Loss: 1.8869423866271973\n",
            "Epoch: 3, Training Loss: 1.2488486766815186\n",
            "Epoch: 3, Training Loss: 1.4746249914169312\n",
            "Epoch: 3, Training Loss: 1.4789855480194092\n",
            "Epoch: 3, Training Loss: 1.3601703643798828\n",
            "Epoch: 3, Training Loss: 1.3900068998336792\n",
            "Epoch: 3, Training Loss: 1.3910624980926514\n",
            "Epoch: 3, Training Loss: 1.3680686950683594\n",
            "Epoch: 3, Training Loss: 1.3206477165222168\n",
            "Epoch: 3, Training Loss: 1.5074489116668701\n",
            "Epoch: 3, Training Loss: 1.5255571603775024\n",
            "Epoch: 3, Training Loss: 1.5347342491149902\n",
            "Epoch: 3, Training Loss: 1.5341931581497192\n",
            "Epoch: 3, Training Loss: 1.3506715297698975\n",
            "Epoch: 3, Training Loss: 1.4940602779388428\n",
            "Epoch: 3, Training Loss: 1.5618407726287842\n",
            "Epoch: 3, Training Loss: 1.5106267929077148\n",
            "Epoch: 3, Training Loss: 1.4039220809936523\n",
            "Epoch: 3, Training Loss: 1.5465240478515625\n",
            "Epoch: 3, Training Loss: 1.4948869943618774\n",
            "Epoch: 3, Training Loss: 1.3372082710266113\n",
            "Epoch: 3, Training Loss: 1.3823204040527344\n",
            "Epoch: 3, Training Loss: 1.2832180261611938\n",
            "Epoch: 3, Training Loss: 1.3066837787628174\n",
            "Epoch: 3, Training Loss: 1.5021036863327026\n",
            "Epoch: 3, Training Loss: 1.3410332202911377\n",
            "Epoch: 3, Training Loss: 1.242685079574585\n",
            "Epoch: 3, Training Loss: 1.3321112394332886\n",
            "Epoch: 3, Training Loss: 1.6567461490631104\n",
            "Epoch: 3, Training Loss: 1.7420847415924072\n",
            "Epoch: 3, Training Loss: 1.500282645225525\n",
            "Epoch: 3, Training Loss: 1.435259222984314\n",
            "Epoch: 3, Training Loss: 1.5623359680175781\n",
            "Epoch: 3, Training Loss: 1.5471221208572388\n",
            "Epoch: 3, Training Loss: 1.3051460981369019\n",
            "Epoch: 3, Training Loss: 1.4032419919967651\n",
            "Epoch: 3, Training Loss: 1.3223967552185059\n",
            "Epoch: 3, Training Loss: 1.513741374015808\n",
            "Epoch: 3, Training Loss: 1.547721266746521\n",
            "Epoch: 3, Training Loss: 1.415897011756897\n",
            "Epoch: 3, Training Loss: 1.2534055709838867\n",
            "Epoch: 3, Training Loss: 1.6067370176315308\n",
            "Epoch: 3, Training Loss: 1.537913203239441\n",
            "Epoch: 3, Training Loss: 1.52583909034729\n",
            "Epoch: 3, Training Loss: 1.4432623386383057\n",
            "Epoch: 3, Training Loss: 1.526596188545227\n",
            "Epoch: 3, Training Loss: 1.376380443572998\n",
            "Epoch: 3, Training Loss: 1.1467218399047852\n",
            "Epoch: 3, Training Loss: 1.408530354499817\n",
            "Epoch: 3, Training Loss: 1.3490325212478638\n",
            "Epoch: 3, Training Loss: 1.3413993120193481\n",
            "Epoch: 3, Training Loss: 1.5153625011444092\n",
            "Epoch: 3, Training Loss: 1.4403834342956543\n",
            "Epoch: 3, Training Loss: 1.3832484483718872\n",
            "Epoch: 3, Training Loss: 1.3985662460327148\n",
            "Epoch: 3, Training Loss: 1.5523806810379028\n",
            "Epoch: 3, Training Loss: 1.5518994331359863\n",
            "Epoch: 3, Training Loss: 1.4123574495315552\n",
            "Epoch: 3, Training Loss: 1.4661834239959717\n",
            "Epoch: 3, Training Loss: 1.3759135007858276\n",
            "Epoch: 3, Training Loss: 1.2637702226638794\n",
            "Epoch: 3, Training Loss: 1.626097321510315\n",
            "Epoch: 3, Training Loss: 1.3752228021621704\n",
            "Epoch: 3, Training Loss: 1.484536051750183\n",
            "Epoch: 3, Training Loss: 1.632688045501709\n",
            "Epoch: 3, Training Loss: 1.292770504951477\n",
            "Epoch: 3, Training Loss: 1.3484090566635132\n",
            "Epoch: 3, Training Loss: 1.675473690032959\n",
            "Epoch: 3, Training Loss: 1.2627226114273071\n",
            "Epoch: 3, Training Loss: 1.4452382326126099\n",
            "Epoch: 3, Training Loss: 1.496387243270874\n",
            "Epoch: 3, Training Loss: 1.405707597732544\n",
            "Epoch: 3, Training Loss: 1.3276021480560303\n",
            "Epoch: 3, Training Loss: 1.6771676540374756\n",
            "Epoch: 3, Training Loss: 1.4984902143478394\n",
            "Epoch: 3, Training Loss: 1.3602019548416138\n",
            "Epoch: 3, Training Loss: 1.3680713176727295\n",
            "Epoch: 3, Training Loss: 1.397023320198059\n",
            "Epoch: 3, Training Loss: 1.1218551397323608\n",
            "Epoch: 3, Training Loss: 1.395268440246582\n",
            "Epoch: 3, Training Loss: 1.416812539100647\n",
            "Epoch: 3, Training Loss: 1.430289626121521\n",
            "Epoch: 3, Training Loss: 1.2805447578430176\n",
            "Epoch: 3, Training Loss: 1.3810051679611206\n",
            "Epoch: 3, Training Loss: 1.3668889999389648\n",
            "Epoch: 3, Training Loss: 1.3375132083892822\n",
            "Epoch: 3, Training Loss: 1.478292465209961\n",
            "Epoch: 3, Training Loss: 1.5042136907577515\n",
            "Epoch: 3, Training Loss: 1.7619980573654175\n",
            "Epoch: 3, Training Loss: 1.5286279916763306\n",
            "Epoch: 3, Training Loss: 1.471790075302124\n",
            "Epoch: 3, Training Loss: 1.3484447002410889\n",
            "Epoch: 3, Training Loss: 1.6445361375808716\n",
            "Epoch: 3, Training Loss: 1.5379389524459839\n",
            "Epoch: 3, Training Loss: 1.5922054052352905\n",
            "Epoch: 3, Training Loss: 1.3784055709838867\n",
            "Epoch: 3, Training Loss: 1.719127893447876\n",
            "Epoch: 3, Training Loss: 1.3424224853515625\n",
            "Epoch: 3, Training Loss: 1.4922789335250854\n",
            "Epoch: 3, Training Loss: 1.3724726438522339\n",
            "Epoch: 3, Training Loss: 1.6649190187454224\n",
            "Epoch: 3, Training Loss: 1.4181376695632935\n",
            "Epoch: 3, Training Loss: 1.1303648948669434\n",
            "Epoch: 3, Training Loss: 1.205811858177185\n",
            "Epoch: 3, Training Loss: 1.4884759187698364\n",
            "Epoch: 3, Training Loss: 1.5479744672775269\n",
            "Epoch: 3, Training Loss: 1.2385534048080444\n",
            "Epoch: 3, Training Loss: 1.2163045406341553\n",
            "Epoch: 3, Training Loss: 1.4866265058517456\n",
            "Epoch: 3, Training Loss: 1.537719488143921\n",
            "Epoch: 3, Training Loss: 1.5588411092758179\n",
            "Epoch: 3, Training Loss: 1.275750756263733\n",
            "Epoch: 3, Training Loss: 1.5376874208450317\n",
            "Epoch: 3, Training Loss: 1.340669870376587\n",
            "Epoch: 3, Training Loss: 1.4698872566223145\n",
            "Epoch: 3, Training Loss: 1.3013724088668823\n",
            "Epoch: 3, Training Loss: 1.4914807081222534\n",
            "Epoch: 3, Training Loss: 1.5990574359893799\n",
            "Epoch: 3, Training Loss: 1.222468376159668\n",
            "Epoch: 3, Training Loss: 1.3254737854003906\n",
            "Epoch: 3, Training Loss: 1.578005313873291\n",
            "Epoch: 3, Training Loss: 1.3811997175216675\n",
            "Epoch: 3, Training Loss: 1.5302919149398804\n",
            "Epoch: 3, Training Loss: 1.59359872341156\n",
            "Epoch: 3, Training Loss: 1.1910700798034668\n",
            "Epoch: 3, Training Loss: 1.6957486867904663\n",
            "Epoch: 3, Training Loss: 1.0574464797973633\n",
            "Epoch: 3, Training Loss: 1.483435034751892\n",
            "Epoch: 3, Training Loss: 1.5771268606185913\n",
            "Epoch: 3, Training Loss: 1.4238154888153076\n",
            "Epoch: 3, Training Loss: 1.4318894147872925\n",
            "Epoch: 3, Training Loss: 1.561832308769226\n",
            "Epoch: 3, Training Loss: 1.5333284139633179\n",
            "Epoch: 3, Training Loss: 1.5702446699142456\n",
            "Epoch: 3, Training Loss: 1.4693576097488403\n",
            "Epoch: 3, Training Loss: 1.639683723449707\n",
            "Epoch: 3, Training Loss: 1.543114185333252\n",
            "Epoch: 3, Training Loss: 1.3400940895080566\n",
            "Epoch: 3, Training Loss: 1.3393148183822632\n",
            "Epoch: 3, Training Loss: 1.233365535736084\n",
            "Epoch: 3, Training Loss: 1.4281327724456787\n",
            "Epoch: 3, Training Loss: 1.3988640308380127\n",
            "Epoch: 3, Training Loss: 1.560907244682312\n",
            "Epoch: 3, Training Loss: 1.561190128326416\n",
            "Epoch: 3, Training Loss: 1.393251895904541\n",
            "Epoch: 3, Training Loss: 1.4741076231002808\n",
            "Epoch: 3, Training Loss: 1.48000967502594\n",
            "Epoch: 3, Training Loss: 1.5806692838668823\n",
            "Epoch: 3, Training Loss: 1.2229517698287964\n",
            "Epoch: 3, Training Loss: 1.1868208646774292\n",
            "Epoch: 3, Training Loss: 1.540542721748352\n",
            "Epoch: 3, Training Loss: 1.5689334869384766\n",
            "Epoch: 3, Training Loss: 1.4100253582000732\n",
            "Epoch: 3, Training Loss: 1.400914192199707\n",
            "Epoch: 3, Training Loss: 1.639219880104065\n",
            "Epoch: 3, Training Loss: 1.3675416707992554\n",
            "Epoch: 3, Training Loss: 1.3708282709121704\n",
            "Epoch: 3, Training Loss: 1.5909736156463623\n",
            "Epoch: 3, Training Loss: 1.4084550142288208\n",
            "Epoch: 3, Training Loss: 1.3781087398529053\n",
            "Epoch: 3, Training Loss: 1.6022151708602905\n",
            "Epoch: 3, Training Loss: 1.4780566692352295\n",
            "Epoch: 3, Training Loss: 1.7674123048782349\n",
            "Epoch: 3, Training Loss: 1.6928825378417969\n",
            "Epoch: 3, Training Loss: 1.307616949081421\n",
            "Epoch: 3, Training Loss: 1.4564080238342285\n",
            "Epoch: 3, Training Loss: 1.1732984781265259\n",
            "Epoch: 3, Training Loss: 1.3589072227478027\n",
            "Epoch: 3, Training Loss: 1.3837803602218628\n",
            "Epoch: 3, Training Loss: 1.4162976741790771\n",
            "Epoch: 3, Training Loss: 1.3374937772750854\n",
            "Epoch: 3, Training Loss: 1.321326494216919\n",
            "Epoch: 3, Training Loss: 1.2395015954971313\n",
            "Epoch: 3, Training Loss: 1.665984869003296\n",
            "Epoch: 3, Training Loss: 1.3878949880599976\n",
            "Epoch: 3, Training Loss: 1.5183643102645874\n",
            "Epoch: 3, Training Loss: 1.3455941677093506\n",
            "Epoch: 3, Training Loss: 1.648383378982544\n",
            "Epoch: 3, Training Loss: 1.5135427713394165\n",
            "Epoch: 3, Training Loss: 1.4879820346832275\n",
            "Epoch: 3, Training Loss: 1.5551081895828247\n",
            "Epoch: 3, Training Loss: 1.382948875427246\n",
            "Epoch: 3, Training Loss: 1.6588674783706665\n",
            "Epoch: 3, Training Loss: 1.3313661813735962\n",
            "Epoch: 3, Training Loss: 1.3643364906311035\n",
            "Epoch: 3, Training Loss: 1.6184221506118774\n",
            "Epoch: 3, Training Loss: 1.6963025331497192\n",
            "Epoch: 3, Training Loss: 1.355132818222046\n",
            "Epoch: 3, Training Loss: 1.4440704584121704\n",
            "Epoch: 3, Training Loss: 1.6706664562225342\n",
            "Epoch: 3, Training Loss: 1.4310449361801147\n",
            "Epoch: 3, Training Loss: 1.5371198654174805\n",
            "Epoch: 3, Training Loss: 1.3025106191635132\n",
            "Epoch: 3, Training Loss: 1.634661078453064\n",
            "Epoch: 3, Training Loss: 1.33157217502594\n",
            "Epoch: 3, Training Loss: 1.1861847639083862\n",
            "Epoch: 3, Training Loss: 1.557023525238037\n",
            "Epoch: 3, Training Loss: 1.5091549158096313\n",
            "Epoch: 3, Training Loss: 1.1908435821533203\n",
            "Epoch: 3, Training Loss: 1.6169825792312622\n",
            "Epoch: 3, Training Loss: 1.222530484199524\n",
            "Epoch: 3, Training Loss: 1.5999919176101685\n",
            "Epoch: 3, Training Loss: 1.2919375896453857\n",
            "Epoch: 3, Training Loss: 1.481381893157959\n",
            "Epoch: 3, Training Loss: 1.453282117843628\n",
            "Epoch: 3, Training Loss: 1.5218453407287598\n",
            "Epoch: 3, Training Loss: 1.2814213037490845\n",
            "Epoch: 3, Training Loss: 1.346447229385376\n",
            "Epoch: 3, Training Loss: 1.5721371173858643\n",
            "Epoch: 3, Training Loss: 1.429816722869873\n",
            "Epoch: 3, Training Loss: 1.6348261833190918\n",
            "Epoch: 3, Training Loss: 1.4852875471115112\n",
            "Epoch: 3, Training Loss: 1.4685739278793335\n",
            "Epoch: 3, Training Loss: 1.3612955808639526\n",
            "Epoch: 3, Training Loss: 1.5780068635940552\n",
            "Epoch: 3, Training Loss: 1.4472492933273315\n",
            "Epoch: 3, Training Loss: 1.4862151145935059\n",
            "Epoch: 3, Training Loss: 1.451006531715393\n",
            "Epoch: 3, Training Loss: 1.4157103300094604\n",
            "Epoch: 3, Training Loss: 1.635170817375183\n",
            "Epoch: 3, Training Loss: 1.6459656953811646\n",
            "Epoch: 3, Training Loss: 1.5763851404190063\n",
            "Epoch: 3, Training Loss: 1.2792669534683228\n",
            "Epoch: 3, Training Loss: 1.3868098258972168\n",
            "Epoch: 3, Training Loss: 1.2525676488876343\n",
            "Epoch: 3, Training Loss: 1.4889328479766846\n",
            "Epoch: 3, Training Loss: 1.3426553010940552\n",
            "Epoch: 3, Training Loss: 1.4536421298980713\n",
            "Epoch: 3, Training Loss: 1.39795982837677\n",
            "Epoch: 3, Training Loss: 1.2132049798965454\n",
            "Epoch: 3, Training Loss: 1.3920185565948486\n",
            "Epoch: 3, Training Loss: 1.5054409503936768\n",
            "Epoch: 3, Training Loss: 1.4186878204345703\n",
            "Epoch: 3, Training Loss: 1.3287190198898315\n",
            "Epoch: 3, Training Loss: 1.2626327276229858\n",
            "Epoch: 3, Training Loss: 1.7199008464813232\n",
            "Epoch: 3, Training Loss: 1.331676959991455\n",
            "Epoch: 3, Training Loss: 1.699674367904663\n",
            "Epoch: 3, Training Loss: 1.5530250072479248\n",
            "Epoch: 3, Training Loss: 1.4344233274459839\n",
            "Epoch: 3, Training Loss: 1.2946230173110962\n",
            "Epoch: 3, Training Loss: 1.3123334646224976\n",
            "Epoch: 3, Training Loss: 1.2229058742523193\n",
            "Epoch: 3, Training Loss: 1.7135063409805298\n",
            "Epoch: 3, Training Loss: 1.4616237878799438\n",
            "Epoch: 3, Training Loss: 1.3189001083374023\n",
            "Epoch: 3, Training Loss: 1.527165412902832\n",
            "Epoch: 3, Training Loss: 1.234142780303955\n",
            "Epoch: 3, Training Loss: 1.5060335397720337\n",
            "Epoch: 3, Training Loss: 1.247977614402771\n",
            "Epoch: 3, Training Loss: 1.767060399055481\n",
            "Epoch: 3, Training Loss: 1.552699089050293\n",
            "Epoch: 3, Training Loss: 1.3089544773101807\n",
            "Epoch: 3, Training Loss: 1.4032471179962158\n",
            "Epoch: 3, Training Loss: 1.4201306104660034\n",
            "Epoch: 3, Training Loss: 1.8749338388442993\n",
            "Epoch: 3, Training Loss: 1.6641229391098022\n",
            "Epoch: 3, Training Loss: 1.5702364444732666\n",
            "Epoch: 3, Training Loss: 1.4600110054016113\n",
            "Epoch: 3, Training Loss: 1.4918380975723267\n",
            "Epoch: 3, Training Loss: 1.5032862424850464\n",
            "Epoch: 3, Training Loss: 1.368254542350769\n",
            "Epoch: 3, Training Loss: 1.371191382408142\n",
            "Epoch: 3, Training Loss: 1.6586337089538574\n",
            "Epoch: 3, Training Loss: 1.5224125385284424\n",
            "Epoch: 3, Training Loss: 1.4834322929382324\n",
            "Epoch: 3, Training Loss: 1.5266320705413818\n",
            "Epoch: 3, Training Loss: 1.5231175422668457\n",
            "Epoch: 3, Training Loss: 1.4144691228866577\n",
            "Epoch: 3, Training Loss: 1.4285963773727417\n",
            "Epoch: 3, Training Loss: 1.4698530435562134\n",
            "Epoch: 3, Training Loss: 1.2562776803970337\n",
            "Epoch: 3, Training Loss: 1.2925647497177124\n",
            "Epoch: 3, Training Loss: 1.4927822351455688\n",
            "Epoch: 3, Training Loss: 1.3374279737472534\n",
            "Epoch: 3, Training Loss: 1.5791934728622437\n",
            "Epoch: 3, Training Loss: 1.407631278038025\n",
            "Epoch: 3, Training Loss: 1.6990107297897339\n",
            "Epoch: 3, Training Loss: 1.4295275211334229\n",
            "Epoch: 3, Training Loss: 1.2668706178665161\n",
            "Epoch: 3, Training Loss: 1.4520680904388428\n",
            "Epoch: 3, Training Loss: 1.45793616771698\n",
            "Epoch: 3, Training Loss: 1.327162504196167\n",
            "Epoch: 3, Training Loss: 1.2573210000991821\n",
            "Epoch: 3, Training Loss: 1.447529673576355\n",
            "Epoch: 3, Training Loss: 1.6337130069732666\n",
            "Epoch: 3, Training Loss: 1.417137861251831\n",
            "Epoch: 3, Training Loss: 1.2105404138565063\n",
            "Epoch: 3, Training Loss: 1.343243956565857\n",
            "Epoch: 3, Training Loss: 1.4796302318572998\n",
            "Epoch: 3, Training Loss: 1.4812202453613281\n",
            "Epoch: 3, Training Loss: 1.1846421957015991\n",
            "Epoch: 3, Training Loss: 1.251592993736267\n",
            "Epoch: 3, Training Loss: 1.5201793909072876\n",
            "Epoch: 3, Training Loss: 1.3187521696090698\n",
            "Epoch: 3, Training Loss: 1.4379111528396606\n",
            "Epoch: 3, Training Loss: 1.3168843984603882\n",
            "Epoch: 3, Training Loss: 1.3712676763534546\n",
            "Epoch: 3, Training Loss: 1.4790239334106445\n",
            "Epoch: 3, Training Loss: 1.648324728012085\n",
            "Epoch: 3, Training Loss: 1.3292981386184692\n",
            "Epoch: 3, Training Loss: 1.510245680809021\n",
            "Epoch: 3, Training Loss: 1.3873320817947388\n",
            "Epoch: 3, Training Loss: 1.5065701007843018\n",
            "Epoch: 3, Training Loss: 1.6557499170303345\n",
            "Epoch: 3, Training Loss: 1.5280890464782715\n",
            "Epoch: 3, Training Loss: 1.3322490453720093\n",
            "Epoch: 3, Training Loss: 1.404470682144165\n",
            "Epoch: 3, Training Loss: 1.3418606519699097\n",
            "Epoch: 3, Training Loss: 1.6648098230361938\n",
            "Epoch: 3, Training Loss: 1.4447340965270996\n",
            "Epoch: 3, Training Loss: 1.469204306602478\n",
            "Epoch: 3, Training Loss: 1.5855919122695923\n",
            "Epoch: 3, Training Loss: 1.496201992034912\n",
            "Epoch: 3, Training Loss: 1.6302696466445923\n",
            "Epoch: 3, Training Loss: 1.270216703414917\n",
            "Epoch: 3, Training Loss: 1.2714647054672241\n",
            "Epoch: 3, Training Loss: 1.2919694185256958\n",
            "Epoch: 3, Training Loss: 1.5121043920516968\n",
            "Epoch: 3, Training Loss: 1.5961720943450928\n",
            "Epoch: 3, Training Loss: 1.2295489311218262\n",
            "Epoch: 3, Training Loss: 1.3318047523498535\n",
            "Epoch: 3, Training Loss: 1.3952038288116455\n",
            "Epoch: 3, Training Loss: 1.4684255123138428\n",
            "Epoch: 3, Training Loss: 1.5209646224975586\n",
            "Epoch: 3, Training Loss: 1.3961766958236694\n",
            "Epoch: 3, Training Loss: 1.3312406539916992\n",
            "Epoch: 3, Training Loss: 1.3235359191894531\n",
            "Epoch: 3, Training Loss: 1.6578956842422485\n",
            "Epoch: 3, Training Loss: 1.5723190307617188\n",
            "Epoch: 3, Training Loss: 1.528045654296875\n",
            "Epoch: 3, Training Loss: 1.4045192003250122\n",
            "Epoch: 3, Training Loss: 1.4267678260803223\n",
            "Epoch: 3, Training Loss: 1.3873649835586548\n",
            "Epoch: 3, Training Loss: 1.7204419374465942\n",
            "Epoch: 3, Training Loss: 1.3895624876022339\n",
            "Epoch: 3, Training Loss: 1.6284654140472412\n",
            "Epoch: 3, Training Loss: 1.7743175029754639\n",
            "Epoch: 3, Training Loss: 1.4142999649047852\n",
            "Epoch: 3, Training Loss: 1.4042552709579468\n",
            "Epoch: 3, Training Loss: 1.4734573364257812\n",
            "Epoch: 3, Training Loss: 1.472798228263855\n",
            "Epoch: 3, Training Loss: 1.2474535703659058\n",
            "Epoch: 3, Training Loss: 1.3451695442199707\n",
            "Epoch: 3, Training Loss: 1.2633695602416992\n",
            "Epoch: 3, Training Loss: 1.2980337142944336\n",
            "Epoch: 3, Training Loss: 1.4965561628341675\n",
            "Epoch: 3, Training Loss: 1.4604629278182983\n",
            "Epoch: 3, Training Loss: 1.4305135011672974\n",
            "Epoch: 3, Training Loss: 1.4155640602111816\n",
            "Epoch: 3, Training Loss: 1.2247793674468994\n",
            "Epoch: 3, Training Loss: 1.496364712715149\n",
            "Epoch: 3, Training Loss: 1.4092098474502563\n",
            "Epoch: 3, Training Loss: 1.4868080615997314\n",
            "Epoch: 3, Training Loss: 1.4876266717910767\n",
            "Epoch: 3, Training Loss: 1.3649907112121582\n",
            "Epoch: 3, Training Loss: 1.4021577835083008\n",
            "Epoch: 3, Training Loss: 1.323643445968628\n",
            "Epoch: 3, Training Loss: 1.3947172164916992\n",
            "Epoch: 3, Training Loss: 1.3331025838851929\n",
            "Epoch: 3, Training Loss: 1.7203195095062256\n",
            "Epoch: 3, Training Loss: 1.5545988082885742\n",
            "Epoch: 3, Training Loss: 1.1949137449264526\n",
            "Epoch: 3, Training Loss: 1.6472514867782593\n",
            "Epoch: 3, Training Loss: 1.4295564889907837\n",
            "Epoch: 3, Training Loss: 1.5027233362197876\n",
            "Epoch: 3, Training Loss: 1.1989508867263794\n",
            "Epoch: 3, Training Loss: 1.4686275720596313\n",
            "Epoch: 3, Training Loss: 1.4808616638183594\n",
            "Epoch: 3, Training Loss: 1.2895854711532593\n",
            "Epoch: 3, Training Loss: 1.7294116020202637\n",
            "Epoch: 3, Training Loss: 1.4927253723144531\n",
            "Epoch: 3, Training Loss: 1.6463475227355957\n",
            "Epoch: 3, Training Loss: 1.354071855545044\n",
            "Epoch: 3, Training Loss: 1.4088104963302612\n",
            "Epoch: 3, Training Loss: 1.2269513607025146\n",
            "Epoch: 3, Training Loss: 1.6963526010513306\n",
            "Epoch: 3, Training Loss: 1.5688115358352661\n",
            "Epoch: 3, Training Loss: 1.2101342678070068\n",
            "Epoch: 3, Training Loss: 1.3509361743927002\n",
            "Epoch: 3, Training Loss: 1.1390249729156494\n",
            "Epoch: 3, Training Loss: 1.4254217147827148\n",
            "Epoch: 3, Training Loss: 1.447167992591858\n",
            "Epoch: 3, Training Loss: 1.403859257698059\n",
            "Epoch: 3, Training Loss: 1.2608832120895386\n",
            "Epoch: 3, Validation Loss: 1.4348936080932617\n",
            "Epoch: 3, Validation Loss: 1.3235398530960083\n",
            "Epoch: 3, Validation Loss: 1.5384329557418823\n",
            "Epoch: 3, Validation Loss: 1.2607207298278809\n",
            "Epoch: 3, Validation Loss: 1.3840185403823853\n",
            "Epoch: 3, Validation Loss: 1.2304414510726929\n",
            "Epoch: 3, Validation Loss: 1.5371013879776\n",
            "Epoch: 3, Validation Loss: 1.2900315523147583\n",
            "Epoch: 3, Validation Loss: 1.4107569456100464\n",
            "Epoch: 3, Validation Loss: 1.6448044776916504\n",
            "Epoch: 3, Validation Loss: 1.2049901485443115\n",
            "Epoch: 3, Validation Loss: 1.2836090326309204\n",
            "Epoch: 3, Validation Loss: 1.352948784828186\n",
            "Epoch: 3, Validation Loss: 1.6654049158096313\n",
            "Epoch: 3, Validation Loss: 1.3648158311843872\n",
            "Epoch: 3, Validation Loss: 1.3123931884765625\n",
            "Epoch: 3, Validation Loss: 1.3177573680877686\n",
            "Epoch: 3, Validation Loss: 1.2059791088104248\n",
            "Epoch: 3, Validation Loss: 1.5854804515838623\n",
            "Epoch: 3, Validation Loss: 1.4086754322052002\n",
            "Epoch: 3, Validation Loss: 1.4546990394592285\n",
            "Epoch: 3, Validation Loss: 1.3713098764419556\n",
            "Epoch: 3, Validation Loss: 1.70866858959198\n",
            "Epoch: 3, Validation Loss: 1.4255093336105347\n",
            "Epoch: 3, Validation Loss: 1.3331092596054077\n",
            "Epoch: 3, Validation Loss: 1.209065556526184\n",
            "Epoch: 3, Validation Loss: 1.311728835105896\n",
            "Epoch: 3, Validation Loss: 1.334934949874878\n",
            "Epoch: 3, Validation Loss: 1.2608380317687988\n",
            "Epoch: 3, Validation Loss: 1.4960951805114746\n",
            "Epoch: 3, Validation Loss: 1.4977999925613403\n",
            "Epoch: 3, Validation Loss: 1.4821656942367554\n",
            "Epoch: 3, Validation Loss: 1.6221572160720825\n",
            "Epoch: 3, Validation Loss: 1.1880406141281128\n",
            "Epoch: 3, Validation Loss: 1.305079698562622\n",
            "Epoch: 3, Validation Loss: 1.3648549318313599\n",
            "Epoch: 3, Validation Loss: 1.598616600036621\n",
            "Epoch: 3, Validation Loss: 1.212537169456482\n",
            "Epoch: 3, Validation Loss: 1.4704831838607788\n",
            "Epoch: 3, Validation Loss: 1.5806876420974731\n",
            "Epoch: 3, Validation Loss: 1.3861171007156372\n",
            "Epoch: 3, Validation Loss: 1.4200879335403442\n",
            "Epoch: 3, Validation Loss: 1.3986475467681885\n",
            "Epoch: 3, Validation Loss: 1.4560370445251465\n",
            "Epoch: 3, Validation Loss: 1.6069660186767578\n",
            "Epoch: 3, Validation Loss: 1.1616284847259521\n",
            "Epoch: 3, Validation Loss: 1.2774310111999512\n",
            "Epoch: 3, Validation Loss: 1.3531076908111572\n",
            "Epoch: 3, Validation Loss: 1.5072193145751953\n",
            "Epoch: 3, Validation Loss: 1.6161763668060303\n",
            "Epoch: 3, Validation Loss: 1.6566874980926514\n",
            "Epoch: 3, Validation Loss: 1.4990358352661133\n",
            "Epoch: 3, Validation Loss: 1.3313912153244019\n",
            "Epoch: 3, Validation Loss: 1.6640872955322266\n",
            "Epoch: 3, Validation Loss: 1.5719717741012573\n",
            "Epoch: 3, Validation Loss: 1.606751561164856\n",
            "Epoch: 3, Validation Loss: 1.5443288087844849\n",
            "Epoch: 3, Validation Loss: 1.3408294916152954\n",
            "Epoch: 3, Validation Loss: 1.40634024143219\n",
            "Epoch: 3, Validation Loss: 1.2784981727600098\n",
            "Epoch: 3, Validation Loss: 1.1884596347808838\n",
            "Epoch: 3, Validation Loss: 1.4315978288650513\n",
            "Epoch: 3, Validation Loss: 1.6560001373291016\n",
            "Epoch: 3, Validation Loss: 1.469481348991394\n",
            "Epoch: 3, Validation Loss: 1.4318435192108154\n",
            "Epoch: 3, Validation Loss: 1.4361255168914795\n",
            "Epoch: 3, Validation Loss: 1.3684649467468262\n",
            "Epoch: 3, Validation Loss: 1.5991472005844116\n",
            "Epoch: 3, Validation Loss: 1.3248292207717896\n",
            "Epoch: 3, Validation Loss: 1.4315181970596313\n",
            "Epoch: 3, Validation Loss: 1.4479635953903198\n",
            "Epoch: 3, Validation Loss: 1.5042171478271484\n",
            "Epoch: 3, Validation Loss: 1.565733790397644\n",
            "Epoch: 3, Validation Loss: 1.3257640600204468\n",
            "Epoch: 3, Validation Loss: 1.1495802402496338\n",
            "Epoch: 3, Validation Loss: 1.4343565702438354\n",
            "Epoch: 3, Validation Loss: 1.348588466644287\n",
            "Epoch: 3, Validation Loss: 1.176791787147522\n",
            "Epoch: 3, Validation Loss: 1.4881216287612915\n",
            "Epoch: 3, Validation Loss: 1.5274380445480347\n",
            "Epoch: 3, Validation Loss: 1.2167201042175293\n",
            "Epoch: 3, Validation Loss: 1.3814940452575684\n",
            "Epoch: 3, Validation Loss: 1.5282429456710815\n",
            "Epoch: 3, Validation Loss: 1.4113620519638062\n",
            "Epoch: 3, Validation Loss: 1.6842632293701172\n",
            "Epoch: 3, Validation Loss: 1.5191925764083862\n",
            "Epoch: 3, Validation Loss: 1.615702509880066\n",
            "Epoch: 3, Validation Loss: 1.259208083152771\n",
            "Epoch: 3, Validation Loss: 1.4996501207351685\n",
            "Epoch: 3, Validation Loss: 1.1663259267807007\n",
            "Epoch: 3, Validation Loss: 1.2720872163772583\n",
            "Epoch: 3, Validation Loss: 1.244587779045105\n",
            "Epoch: 3, Validation Loss: 1.5421674251556396\n",
            "Epoch: 3, Validation Loss: 1.3966684341430664\n",
            "Epoch: 3, Validation Loss: 1.4324778318405151\n",
            "Epoch: 3, Validation Loss: 1.2995644807815552\n",
            "Epoch: 3, Validation Loss: 1.2396907806396484\n",
            "Epoch: 3, Validation Loss: 1.5733779668807983\n",
            "Epoch: 3, Validation Loss: 1.2668967247009277\n",
            "Epoch: 3, Validation Loss: 1.2260489463806152\n",
            "Epoch: 3, Validation Loss: 1.364182710647583\n",
            "Epoch: 3, Validation Loss: 1.5720608234405518\n",
            "Epoch: 3, Validation Loss: 1.6107759475708008\n",
            "Epoch: 3, Validation Loss: 1.5989936590194702\n",
            "Epoch: 3, Validation Loss: 1.0662553310394287\n",
            "Epoch: 3, Validation Loss: 1.4935327768325806\n",
            "Epoch: 3, Validation Loss: 1.151740550994873\n",
            "Epoch: 3, Validation Loss: 1.4030044078826904\n",
            "Epoch: 3, Validation Loss: 1.366063117980957\n",
            "Epoch: 3, Validation Loss: 1.7344231605529785\n",
            "Epoch: 3, Validation Loss: 1.503927230834961\n",
            "Epoch: 3, Validation Loss: 1.4355548620224\n",
            "Epoch: 3, Validation Loss: 1.5854268074035645\n",
            "Epoch: 3, Validation Loss: 1.4158153533935547\n",
            "Epoch: 3, Validation Loss: 1.339554786682129\n",
            "Epoch: 3, Validation Loss: 1.5212593078613281\n",
            "Epoch: 3, Validation Loss: 1.456649661064148\n",
            "Epoch: 3, Validation Loss: 1.3785942792892456\n",
            "Epoch: 3, Validation Loss: 1.3485037088394165\n",
            "Epoch: 3, Validation Loss: 1.3812599182128906\n",
            "Epoch: 3, Validation Loss: 1.231928825378418\n",
            "Epoch: 3, Validation Loss: 1.454357624053955\n",
            "Epoch: 3, Validation Loss: 1.7052538394927979\n",
            "Epoch: 3, Validation Loss: 1.14606511592865\n",
            "Epoch: 3, Validation Loss: 1.5772331953048706\n",
            "Epoch: 3, Validation Loss: 1.503143310546875\n",
            "Epoch: 3, Validation Loss: 1.682150959968567\n",
            "Epoch: 3, Validation Loss: 1.553183913230896\n",
            "Epoch: 3, Validation Loss: 1.2731670141220093\n",
            "Epoch: 3, Validation Loss: 1.3636971712112427\n",
            "Epoch: 3, Validation Loss: 1.3710967302322388\n",
            "Epoch: 3, Validation Loss: 1.4749974012374878\n",
            "Epoch: 3, Validation Loss: 1.3166167736053467\n",
            "Epoch: 3, Validation Loss: 1.4028615951538086\n",
            "Epoch: 3, Validation Loss: 1.4581029415130615\n",
            "Epoch: 3, Validation Loss: 1.3337675333023071\n",
            "Epoch: 3, Validation Loss: 1.549269199371338\n",
            "Epoch: 3, Validation Loss: 1.2354319095611572\n",
            "Epoch: 3, Validation Loss: 1.3310506343841553\n",
            "Epoch: 3, Validation Loss: 1.4447252750396729\n",
            "Epoch: 3, Validation Loss: 1.5184602737426758\n",
            "Epoch: 3, Validation Loss: 1.5109381675720215\n",
            "Epoch: 3, Validation Loss: 1.2453465461730957\n",
            "Epoch: 3, Validation Loss: 1.6267184019088745\n",
            "Epoch: 3, Validation Loss: 1.5301895141601562\n",
            "Epoch: 3, Validation Loss: 1.5438836812973022\n",
            "Epoch: 3, Validation Loss: 1.2911144495010376\n",
            "Epoch: 3, Validation Loss: 1.3757823705673218\n",
            "Epoch: 3, Validation Loss: 1.3902827501296997\n",
            "Epoch: 3, Validation Loss: 1.3988069295883179\n",
            "Epoch: 3, Validation Loss: 1.2249252796173096\n",
            "Epoch: 3, Validation Loss: 1.2365883588790894\n",
            "Epoch: 3, Validation Loss: 1.235331416130066\n",
            "Epoch: 3, Validation Loss: 1.5058064460754395\n",
            "Epoch: 3, Validation Loss: 1.5047895908355713\n",
            "Epoch: 3, Validation Loss: 1.4305450916290283\n",
            "Epoch: 3, Validation Loss: 1.3376154899597168\n",
            "Epoch: 3, Validation Loss: 1.4853650331497192\n",
            "Epoch: 3, Validation Loss: 1.3697713613510132\n",
            "Epoch: 3, Validation Loss: 1.3724218606948853\n",
            "Epoch: 3, Validation Loss: 1.762494683265686\n",
            "Epoch: 3, Validation Loss: 1.4253758192062378\n",
            "Epoch: 3, Validation Loss: 1.2914292812347412\n",
            "Epoch: 3, Validation Loss: 1.540500521659851\n",
            "Epoch: 3, Validation Loss: 1.2575392723083496\n",
            "Epoch: 3, Validation Loss: 1.74448561668396\n",
            "Epoch: 3, Validation Loss: 1.5468101501464844\n",
            "Epoch: 3, Validation Loss: 1.824215292930603\n",
            "Epoch: 3, Validation Loss: 1.4428253173828125\n",
            "Epoch: 3, Validation Loss: 1.4068740606307983\n",
            "Epoch: 3, Validation Loss: 1.2512127161026\n",
            "Epoch: 3, Validation Loss: 1.4786109924316406\n",
            "Epoch: 3, Validation Loss: 1.5455683469772339\n",
            "Epoch: 3, Validation Loss: 1.618988037109375\n",
            "Epoch: 3, Validation Loss: 1.3816940784454346\n",
            "Epoch: 3, Validation Loss: 1.4011167287826538\n",
            "Epoch: 3, Validation Loss: 1.3993184566497803\n",
            "Epoch: 3, Validation Loss: 1.2758272886276245\n",
            "Epoch: 3, Validation Loss: 1.1179425716400146\n",
            "Epoch: 3, Validation Loss: 1.7010070085525513\n",
            "Epoch: 3, Validation Loss: 1.240919828414917\n",
            "Epoch: 3, Validation Loss: 1.3792165517807007\n",
            "Epoch: 3, Validation Loss: 1.5644886493682861\n",
            "Epoch: 3, Validation Loss: 1.702402949333191\n",
            "Epoch: 3, Validation Loss: 1.4117071628570557\n",
            "Epoch: 3, Validation Loss: 1.4183671474456787\n",
            "Epoch: 3, Validation Loss: 1.4903905391693115\n",
            "Epoch: 3, Validation Loss: 1.589287519454956\n",
            "Epoch: 3, Validation Loss: 1.3194599151611328\n",
            "Epoch: 3, Validation Loss: 1.1825028657913208\n",
            "Epoch: 3, Validation Loss: 1.3361910581588745\n",
            "Epoch: 3, Validation Loss: 1.280397653579712\n",
            "Epoch: 3, Validation Loss: 1.558408260345459\n",
            "Epoch: 3, Validation Loss: 1.5143846273422241\n",
            "Epoch: 3, Validation Loss: 1.3846253156661987\n",
            "Epoch: 3, Validation Loss: 1.2513267993927002\n",
            "Epoch: 3, Validation Loss: 1.4765435457229614\n",
            "Epoch: 3, Validation Loss: 1.4437453746795654\n",
            "Epoch: 3, Validation Loss: 1.689767837524414\n",
            "Epoch: 3, Validation Loss: 1.5027828216552734\n",
            "Epoch: 3, Validation Loss: 1.299957513809204\n",
            "Epoch: 3, Validation Loss: 1.3893983364105225\n",
            "Epoch: 3, Validation Loss: 1.3032795190811157\n",
            "Epoch: 3, Validation Loss: 1.7015869617462158\n",
            "Epoch: 3, Validation Loss: 1.4086008071899414\n",
            "Epoch: 3, Validation Loss: 1.478698492050171\n",
            "Epoch: 3, Validation Loss: 1.267043113708496\n",
            "Epoch: 3, Validation Loss: 1.540010690689087\n",
            "Epoch: 3, Validation Loss: 1.2901651859283447\n",
            "Epoch: 3, Validation Loss: 1.4205577373504639\n",
            "Epoch: 3, Validation Loss: 1.396638035774231\n",
            "Epoch: 3, Validation Loss: 1.5146793127059937\n",
            "Epoch: 3, Validation Loss: 1.1755599975585938\n",
            "Epoch: 3, Validation Loss: 1.3230048418045044\n",
            "Epoch: 3, Validation Loss: 1.4310580492019653\n",
            "Epoch: 3, Validation Loss: 1.298466444015503\n",
            "Epoch: 3, Validation Loss: 1.4291990995407104\n",
            "Epoch: 3, Validation Loss: 1.391412615776062\n",
            "Epoch: 3, Validation Loss: 1.2669624090194702\n",
            "Epoch: 3, Validation Loss: 1.661893367767334\n",
            "Epoch: 3, Validation Loss: 1.750091791152954\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.eval()\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        src_data, tgt_data = data\n",
        "        output = transformer(src_data, tgt_data[:, :-1])\n",
        "        loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
        "        print(f\"Test Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLQfPbuV_MW4",
        "outputId": "afb704c1-4e7e-4786-bfb0-165f70ddd8f6"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.2654552459716797\n",
            "Test Loss: 1.6323927640914917\n",
            "Test Loss: 1.4729979038238525\n",
            "Test Loss: 1.394705057144165\n",
            "Test Loss: 1.4396368265151978\n",
            "Test Loss: 1.2058790922164917\n",
            "Test Loss: 1.2001762390136719\n",
            "Test Loss: 1.4212464094161987\n",
            "Test Loss: 1.4447784423828125\n",
            "Test Loss: 1.1544042825698853\n",
            "Test Loss: 1.1797411441802979\n",
            "Test Loss: 1.5986568927764893\n",
            "Test Loss: 1.2565982341766357\n",
            "Test Loss: 1.4874598979949951\n",
            "Test Loss: 1.7046656608581543\n",
            "Test Loss: 1.390325665473938\n",
            "Test Loss: 1.4131053686141968\n",
            "Test Loss: 1.4077860116958618\n",
            "Test Loss: 1.2143187522888184\n",
            "Test Loss: 1.209336757659912\n",
            "Test Loss: 1.5325771570205688\n",
            "Test Loss: 1.5186089277267456\n",
            "Test Loss: 1.3887872695922852\n",
            "Test Loss: 1.3155425786972046\n",
            "Test Loss: 1.28395676612854\n",
            "Test Loss: 1.1784963607788086\n",
            "Test Loss: 1.2991493940353394\n",
            "Test Loss: 1.3160443305969238\n",
            "Test Loss: 1.2958601713180542\n",
            "Test Loss: 1.9144601821899414\n",
            "Test Loss: 1.3374994993209839\n",
            "Test Loss: 1.3671674728393555\n",
            "Test Loss: 1.356597900390625\n",
            "Test Loss: 1.3817470073699951\n",
            "Test Loss: 1.3118970394134521\n",
            "Test Loss: 1.3210259675979614\n",
            "Test Loss: 1.3815141916275024\n",
            "Test Loss: 1.681010127067566\n",
            "Test Loss: 1.4313241243362427\n",
            "Test Loss: 1.4512544870376587\n",
            "Test Loss: 1.4272366762161255\n",
            "Test Loss: 1.5542041063308716\n",
            "Test Loss: 1.249460220336914\n",
            "Test Loss: 1.4281805753707886\n",
            "Test Loss: 1.5825353860855103\n",
            "Test Loss: 1.4398951530456543\n",
            "Test Loss: 1.3840091228485107\n",
            "Test Loss: 1.2946960926055908\n",
            "Test Loss: 1.1973814964294434\n",
            "Test Loss: 1.340623140335083\n",
            "Test Loss: 1.4921419620513916\n",
            "Test Loss: 1.2931474447250366\n",
            "Test Loss: 1.3145124912261963\n",
            "Test Loss: 1.3259087800979614\n",
            "Test Loss: 1.4801052808761597\n",
            "Test Loss: 1.5085294246673584\n",
            "Test Loss: 1.5182676315307617\n",
            "Test Loss: 1.2689682245254517\n",
            "Test Loss: 1.6856223344802856\n",
            "Test Loss: 1.6081442832946777\n",
            "Test Loss: 1.5967708826065063\n",
            "Test Loss: 1.4674031734466553\n",
            "Test Loss: 1.2304404973983765\n",
            "Test Loss: 1.4604395627975464\n",
            "Test Loss: 1.2560495138168335\n",
            "Test Loss: 1.3360105752944946\n",
            "Test Loss: 1.1136196851730347\n",
            "Test Loss: 1.2582350969314575\n",
            "Test Loss: 1.2796738147735596\n",
            "Test Loss: 1.4609860181808472\n",
            "Test Loss: 1.391474723815918\n",
            "Test Loss: 1.4606959819793701\n",
            "Test Loss: 1.3369089365005493\n",
            "Test Loss: 1.8678187131881714\n",
            "Test Loss: 1.4278464317321777\n",
            "Test Loss: 1.4223623275756836\n",
            "Test Loss: 1.4987860918045044\n",
            "Test Loss: 1.1783846616744995\n",
            "Test Loss: 1.2581067085266113\n",
            "Test Loss: 1.227057933807373\n",
            "Test Loss: 1.4432252645492554\n",
            "Test Loss: 1.5638916492462158\n",
            "Test Loss: 1.4143437147140503\n",
            "Test Loss: 1.5412720441818237\n",
            "Test Loss: 1.4460132122039795\n",
            "Test Loss: 1.3682655096054077\n",
            "Test Loss: 1.3736507892608643\n",
            "Test Loss: 1.3034558296203613\n",
            "Test Loss: 1.3114798069000244\n",
            "Test Loss: 1.3873783349990845\n",
            "Test Loss: 1.531314730644226\n",
            "Test Loss: 1.4132455587387085\n",
            "Test Loss: 1.3410197496414185\n",
            "Test Loss: 1.4582622051239014\n",
            "Test Loss: 1.410543441772461\n",
            "Test Loss: 1.3577970266342163\n",
            "Test Loss: 1.41403067111969\n",
            "Test Loss: 1.5342315435409546\n",
            "Test Loss: 1.3103094100952148\n",
            "Test Loss: 1.5949829816818237\n",
            "Test Loss: 1.4309065341949463\n",
            "Test Loss: 1.3025779724121094\n",
            "Test Loss: 1.5171319246292114\n",
            "Test Loss: 1.4508051872253418\n",
            "Test Loss: 1.304521918296814\n",
            "Test Loss: 1.4904416799545288\n",
            "Test Loss: 1.5890649557113647\n",
            "Test Loss: 1.3444664478302002\n",
            "Test Loss: 1.2236260175704956\n",
            "Test Loss: 1.2018122673034668\n",
            "Test Loss: 1.5910030603408813\n",
            "Test Loss: 1.3351575136184692\n",
            "Test Loss: 1.240665078163147\n",
            "Test Loss: 1.521790623664856\n",
            "Test Loss: 1.1017531156539917\n",
            "Test Loss: 1.5075840950012207\n",
            "Test Loss: 1.6528245210647583\n",
            "Test Loss: 1.3390376567840576\n",
            "Test Loss: 1.5784258842468262\n",
            "Test Loss: 1.495614767074585\n",
            "Test Loss: 1.4600703716278076\n",
            "Test Loss: 1.3280526399612427\n",
            "Test Loss: 1.3843780755996704\n",
            "Test Loss: 1.4860918521881104\n",
            "Test Loss: 1.2263374328613281\n",
            "Test Loss: 1.1888247728347778\n",
            "Test Loss: 1.3365670442581177\n",
            "Test Loss: 1.1306029558181763\n",
            "Test Loss: 1.3779689073562622\n",
            "Test Loss: 1.4456418752670288\n",
            "Test Loss: 1.618129014968872\n",
            "Test Loss: 1.5575915575027466\n",
            "Test Loss: 1.5326517820358276\n",
            "Test Loss: 1.3828002214431763\n",
            "Test Loss: 1.2071170806884766\n",
            "Test Loss: 1.2353744506835938\n",
            "Test Loss: 1.6352214813232422\n",
            "Test Loss: 1.4915356636047363\n",
            "Test Loss: 1.3636683225631714\n",
            "Test Loss: 1.5448005199432373\n",
            "Test Loss: 1.5013278722763062\n",
            "Test Loss: 1.1132075786590576\n",
            "Test Loss: 1.8475792407989502\n",
            "Test Loss: 1.4833463430404663\n",
            "Test Loss: 1.3018823862075806\n",
            "Test Loss: 1.1798129081726074\n",
            "Test Loss: 1.277764081954956\n",
            "Test Loss: 1.4250988960266113\n",
            "Test Loss: 1.4274673461914062\n",
            "Test Loss: 1.4472507238388062\n",
            "Test Loss: 1.4652338027954102\n",
            "Test Loss: 1.4716236591339111\n",
            "Test Loss: 1.5424374341964722\n",
            "Test Loss: 1.5313198566436768\n",
            "Test Loss: 1.4141666889190674\n",
            "Test Loss: 1.3763927221298218\n",
            "Test Loss: 1.4898490905761719\n",
            "Test Loss: 1.5178956985473633\n",
            "Test Loss: 1.7085939645767212\n",
            "Test Loss: 1.490660548210144\n",
            "Test Loss: 1.5627268552780151\n",
            "Test Loss: 1.34358811378479\n",
            "Test Loss: 1.2678958177566528\n",
            "Test Loss: 1.667109727859497\n",
            "Test Loss: 1.3007863759994507\n",
            "Test Loss: 1.2946044206619263\n",
            "Test Loss: 1.3298064470291138\n",
            "Test Loss: 1.7313624620437622\n",
            "Test Loss: 1.6781642436981201\n",
            "Test Loss: 1.2910993099212646\n",
            "Test Loss: 1.4605509042739868\n",
            "Test Loss: 1.2726415395736694\n",
            "Test Loss: 1.4888721704483032\n",
            "Test Loss: 1.3784785270690918\n",
            "Test Loss: 1.4667856693267822\n",
            "Test Loss: 1.5713647603988647\n",
            "Test Loss: 1.4793124198913574\n",
            "Test Loss: 1.2597311735153198\n",
            "Test Loss: 1.206373929977417\n",
            "Test Loss: 1.1382560729980469\n",
            "Test Loss: 1.492475986480713\n",
            "Test Loss: 1.2533001899719238\n",
            "Test Loss: 1.4560368061065674\n",
            "Test Loss: 1.5261050462722778\n",
            "Test Loss: 1.5170774459838867\n",
            "Test Loss: 1.3372929096221924\n",
            "Test Loss: 1.3796274662017822\n",
            "Test Loss: 1.6956325769424438\n",
            "Test Loss: 1.47239351272583\n",
            "Test Loss: 1.4823949337005615\n",
            "Test Loss: 1.6704825162887573\n",
            "Test Loss: 1.2929773330688477\n",
            "Test Loss: 1.5047448873519897\n",
            "Test Loss: 1.4453035593032837\n",
            "Test Loss: 1.4715136289596558\n",
            "Test Loss: 1.2147587537765503\n",
            "Test Loss: 1.5755701065063477\n",
            "Test Loss: 1.2686858177185059\n",
            "Test Loss: 1.678766131401062\n",
            "Test Loss: 1.4037421941757202\n",
            "Test Loss: 1.37807297706604\n",
            "Test Loss: 1.2708133459091187\n",
            "Test Loss: 1.33809232711792\n",
            "Test Loss: 1.5316035747528076\n",
            "Test Loss: 1.2194088697433472\n",
            "Test Loss: 1.5290566682815552\n",
            "Test Loss: 1.4154592752456665\n",
            "Test Loss: 1.4403668642044067\n",
            "Test Loss: 1.3000621795654297\n",
            "Test Loss: 1.4514551162719727\n",
            "Test Loss: 1.5469574928283691\n",
            "Test Loss: 1.432971477508545\n",
            "Test Loss: 1.3234983682632446\n",
            "Test Loss: 1.5752708911895752\n",
            "Test Loss: 1.5146327018737793\n",
            "Test Loss: 1.7557979822158813\n",
            "Test Loss: 1.3042477369308472\n",
            "Test Loss: 1.4419848918914795\n",
            "Test Loss: 1.4782246351242065\n",
            "Test Loss: 1.4525178670883179\n",
            "Test Loss: 1.285535454750061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "ofl363KBgiyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./transformer_epoch_3\"\n",
        "state_dict = torch.load(model_path)\n",
        "\n",
        "src_vocab_size = 10_000\n",
        "tgt_vocab_size = 10_000\n",
        "d_model = 512\n",
        "num_heads = 4\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length = 100\n",
        "dropout = 0.1\n",
        "num_epochs = 3\n",
        "\n",
        "transformer_loaded = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout).to(device)\n",
        "transformer_loaded.load_state_dict(state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsbGPbE_hFPD",
        "outputId": "7bfbe4d9-52f2-43f0-918b-7c5346aae9cf"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(src):\n",
        "    src_tokens = tokenizer[SRC_LANGUAGE](src)\n",
        "    tgt_tokens = [\"<BOS>\"]\n",
        "\n",
        "    src_vectors = torch.tensor(([BOS_IDX] + vocab[SRC_LANGUAGE](src_tokens) + [EOS_IDX] + [0] * (max_seq_len - len(src_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    for i in range(max_seq_len):\n",
        "        tgt_vectors = torch.tensor((vocab[TGT_LANGUAGE](tgt_tokens) + [0] * (max_seq_len - len(tgt_tokens)))[:max_seq_len], dtype=torch.long, device=device).unsqueeze(0)\n",
        "        output = transformer(src_vectors, tgt_vectors)\n",
        "        idx = torch.argmax(nn.functional.softmax(output, dim=2)[0][i]).item()\n",
        "        tgt_tokens.append(vocab[TGT_LANGUAGE].lookup_token(idx))\n",
        "\n",
        "        if idx == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return \" \".join(tgt_tokens).replace(\"<BOS>\", \"\").replace(\"<EOS>\", \"\").replace(\"<PAD>\", \"\").strip()"
      ],
      "metadata": {
        "id": "x0NYt9BPMufm"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"Hello, I am a teacher.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "d9xThngkeK6W",
        "outputId": "c39a69d3-9ebe-4353-a7e9-0cf6b17195a3"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hola , soy maestro .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"My name is John.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lE7m-fQefb0V",
        "outputId": "c952c1ec-ce63-49f8-d03f-9228596adfd0"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mi nombre es John .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I am learning Spanish.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "QyHIl2fhfp2O",
        "outputId": "247f0f94-5807-458f-92d3-979f922935f3"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Estoy aprendiendo español .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I eat apples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KahapCeCfwMr",
        "outputId": "69c4a3e7-c97d-4a88-9ad0-fad3ea4da9e7"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Como manzanas .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"I have three books and two pens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q_7aRX5Hf0A7",
        "outputId": "60b2db64-f72b-49b3-de2b-870496c632d6"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tengo tres libros y dos bolígrafos .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"Do you work in an office?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OQkoUnAmgMw1",
        "outputId": "8a91b56d-f187-4f57-bd5c-eb7a59d01724"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¿ Te trabajo en una oficina ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translate(\"How are you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "eQ3u-gnngu_l",
        "outputId": "108177f8-be87-4c0f-d2a3-5fa0c2353874"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¿ Cómo estás ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng, spa = test_lines[0].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "_p_dI_QOiWRJ",
        "outputId": "9820902f-7f01-48a6-aeb4-a9caebddb2c4"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you think we should import rice from the U.S.?\n",
            "¿Crees que deberíamos importar arroz de los Estados Unidos?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¿ Crees que deberíamos <UNK> arroz de la <UNK> ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng, spa = test_lines[500].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "T0VbizfWil7D",
        "outputId": "239fdf9c-a5eb-4466-d355-53d31285a9e4"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aren't you going to finish your supper?\n",
            "¿No te vas a terminar tu cena?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'¿ No vas a terminar tu cena ?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng, spa = train_lines[1000].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vp-71fggiuM5",
        "outputId": "354adf16-99c6-42b6-8ec8-a43f10536df5"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom got into a fight with his brother.\n",
            "Tom se metió en una pelea con su hermano.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tom se metió en una pelea con su hermano .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eng, spa = train_lines[10000].split('\\t')\n",
        "print(eng)\n",
        "print(spa)\n",
        "translate(eng)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "fGxzFl4Ui0lY",
        "outputId": "49683592-a50d-4930-b16a-c487372ab486"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tom would like some coffee.\n",
            "Tom quisiera un poco de café.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tom quisiera un café .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export model and vocabulary"
      ],
      "metadata": {
        "id": "3HIFB166pJMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(vocab[SRC_LANGUAGE], \"./vocab-english\")\n",
        "torch.save(vocab[TGT_LANGUAGE], \"./vocab-spanish\")"
      ],
      "metadata": {
        "id": "NRQyHCffi7-s"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(tokenizer[SRC_LANGUAGE], \"./tokenizer-english\")\n",
        "torch.save(tokenizer[TGT_LANGUAGE], \"./tokenizer-spanish\")"
      ],
      "metadata": {
        "id": "DFaLmjFWqB3e"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(transformer, \"./transformer_model\")"
      ],
      "metadata": {
        "id": "v5leNOqGqmT3"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ndAILIdq58F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}